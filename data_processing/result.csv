Section,Sub Section,Content,Content Token Count
1 - Overview,default,"Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. This page is an overview of Kubernetes. Kubernetes is a portable, extensible, open source platform for managing containerizedworkloads and services, that facilitates both declarative configuration and automation.It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. The name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviationresults from counting the eight letters between the ""K"" and the ""s"". Google open-sourced theKubernetes project in 2014. Kubernetes combinesover 15 years of Google's experience runningproduction workloads at scale with best-of-breed ideas and practices from the community.",210
1 - Overview,Going back in time,"Going back in time Let's take a look at why Kubernetes is so useful by going back in time.  Traditional deployment era:Early on, organizations ran applications on physical servers. There was no way to defineresource boundaries for applications in a physical server, and this caused resourceallocation issues. For example, if multiple applications run on a physical server, therecan be instances where one application would take up most of the resources, and as a result,the other applications would underperform. A solution for this would be to run each applicationon a different physical server. But this did not scale as resources were underutilized, and itwas expensive for organizations to maintain many physical servers. Virtualized deployment era: As a solution, virtualization was introduced. It allows youto run multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualizationallows applications to be isolated between VMs and provides a level of security as theinformation of one application cannot be freely accessed by another application. Virtualization allows better utilization of resources in a physical server and allowsbetter scalability because an application can be added or updated easily, reduceshardware costs, and much more. With virtualization you can present a set of physicalresources as a cluster of disposable virtual machines. Each VM is a full machine running all the components, including its own operatingsystem, on top of the virtualized hardware. Container deployment era: Containers are similar to VMs, but they have relaxedisolation properties to share the Operating System (OS) among the applications.Therefore, containers are considered lightweight. Similar to a VM, a containerhas its own filesystem, share of CPU, memory, process space, and more. As theyare decoupled from the underlying infrastructure, they are portable across cloudsand OS distributions. Containers have become popular because they provide extra benefits, such as: Agile application creation and deployment: increased ease and efficiency ofcontainer image creation compared to VM image use.Continuous development, integration, and deployment: provides for reliableand frequent container image build and deployment with quick and efficientrollbacks (due to image immutability).Dev and Ops separation of concerns: create application container images atbuild/release time rather than deployment time, thereby decouplingapplications from infrastructure.Observability: not only surfaces OS-level information and metrics, but alsoapplication health and other signals.Environmental consistency across development, testing, and production: runsthe same on a laptop as it does in the cloud.Cloud and OS distribution portability: runs on Ubuntu, RHEL, CoreOS, on-premises,on major public clouds, and anywhere else.Application-centric management: raises the level of abstraction from running anOS on virtual hardware to running an application on an OS using logical resources.Loosely coupled, distributed, elastic, liberated micro-services: applications arebroken into smaller, independent pieces and can be deployed and managed dynamically –not a monolithic stack running on one big single-purpose machine.Resource isolation: predictable application performance.Resource utilization: high efficiency and density.",632
1 - Overview,Why you need Kubernetes and what it can do,"Why you need Kubernetes and what it can do Containers are a good way to bundle and run your applications. In a productionenvironment, you need to manage the containers that run the applications andensure that there is no downtime. For example, if a container goes down, anothercontainer needs to start. Wouldn't it be easier if this behavior was handled by a system? That's how Kubernetes comes to the rescue! Kubernetes provides you with a frameworkto run distributed systems resiliently. It takes care of scaling and failover foryour application, provides deployment patterns, and more. For example: Kubernetescan easily manage a canary deployment for your system. Kubernetes provides you with: Service discovery and load balancingKubernetes can expose a container using the DNS name or using their own IP address.If traffic to a container is high, Kubernetes is able to load balance and distributethe network traffic so that the deployment is stable.Storage orchestrationKubernetes allows you to automatically mount a storage system of your choice, such aslocal storages, public cloud providers, and more.Automated rollouts and rollbacksYou can describe the desired state for your deployed containers using Kubernetes,and it can change the actual state to the desired state at a controlled rate.For example, you can automate Kubernetes to create new containers for yourdeployment, remove existing containers and adopt all their resources to the new container.Automatic bin packingYou provide Kubernetes with a cluster of nodes that it can use to run containerized tasks.You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fitcontainers onto your nodes to make the best use of your resources.Self-healingKubernetes restarts containers that fail, replaces containers, kills containers that don'trespond to your user-defined health check, and doesn't advertise them to clients until theyare ready to serve.Secret and configuration managementKubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens,and SSH keys. You can deploy and update secrets and application configuration withoutrebuilding your container images, and without exposing secrets in your stack configuration.",470
1 - Overview,What Kubernetes is not,"What Kubernetes is not Kubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system.Since Kubernetes operates at the container level rather than at the hardware level,it provides some generally applicable features common to PaaS offerings, such asdeployment, scaling, load balancing, and lets users integrate their logging, monitoring,and alerting solutions. However, Kubernetes is not monolithic, and these default solutionsare optional and pluggable. Kubernetes provides the building blocks for building developerplatforms, but preserves user choice and flexibility where it is important. Kubernetes: Does not limit the types of applications supported. Kubernetes aims to support anextremely diverse variety of workloads, including stateless, stateful, and data-processingworkloads. If an application can run in a container, it should run great on Kubernetes.Does not deploy source code and does not build your application. Continuous Integration,Delivery, and Deployment (CI/CD) workflows are determined by organization cultures andpreferences as well as technical requirements.Does not provide application-level services, such as middleware (for example, message buses),data-processing frameworks (for example, Spark), databases (for example, MySQL), caches, norcluster storage systems (for example, Ceph) as built-in services. Such components can run onKubernetes, and/or can be accessed by applications running on Kubernetes through portablemechanisms, such as the Open Service Broker.Does not dictate logging, monitoring, or alerting solutions. It provides some integrationsas proof of concept, and mechanisms to collect and export metrics.Does not provide nor mandate a configuration language/system (for example, Jsonnet). It providesa declarative API that may be targeted by arbitrary forms of declarative specifications.Does not provide nor adopt any comprehensive machine configuration, maintenance, management,or self-healing systems.Additionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the needfor orchestration. The technical definition of orchestration is execution of a defined workflow:first do A, then B, then C. In contrast, Kubernetes comprises a set of independent, composablecontrol processes that continuously drive the current state towards the provided desired state.It shouldn't matter how you get from A to C. Centralized control is also not required. Thisresults in a system that is easier to use and more powerful, robust, resilient, and extensible. Take a look at the Kubernetes ComponentsTake a look at the The Kubernetes APITake a look at the Cluster ArchitectureReady to Get Started?",573
1.1 - Kubernetes Components,default,"A Kubernetes cluster consists of the components that are a part of the control plane and a set of machines called nodes. When you deploy Kubernetes, you get a cluster.A Kubernetes cluster consists of a set of worker machines, called nodes,that run containerized applications. Every cluster has at least one worker node.The worker node(s) host the Pods that arethe components of the application workload. Thecontrol plane manages the workernodes and the Pods in the cluster. In production environments, the control plane usuallyruns across multiple computers and a cluster usually runs multiple nodes, providingfault-tolerance and high availability. This document outlines the various components you need to have fora complete and working Kubernetes cluster. The components of a Kubernetes cluster",166
1.1 - Kubernetes Components,Control Plane Components,"Control Plane Components The control plane's components make global decisions about the cluster (for example, scheduling), as well as detecting and responding to cluster events (for example, starting up a new pod when a deployment's replicas field is unsatisfied). Control plane components can be run on any machine in the cluster. However,for simplicity, set up scripts typically start all control plane components onthe same machine, and do not run user containers on this machine. SeeCreating Highly Available clusters with kubeadmfor an example control plane setup that runs across multiple machines.",113
1.1 - Kubernetes Components,kube-apiserver,"kube-apiserver The API server is a component of the Kubernetescontrol plane that exposes the Kubernetes API.The API server is the front end for the Kubernetes control plane. The main implementation of a Kubernetes API server is kube-apiserver.kube-apiserver is designed to scale horizontally—that is, it scales by deploying more instances.You can run several instances of kube-apiserver and balance traffic between those instances.",105
1.1 - Kubernetes Components,etcd,"etcd Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data. If your Kubernetes cluster uses etcd as its backing store, make sure you have aback up planfor those data. You can find in-depth information about etcd in the official documentation.",66
1.1 - Kubernetes Components,kube-scheduler,"kube-scheduler Control plane component that watches for newly createdPods with no assignednode, and selects a node for themto run on. Factors taken into account for scheduling decisions include:individual and collective resource requirements, hardware/software/policyconstraints, affinity and anti-affinity specifications, data locality,inter-workload interference, and deadlines.",75
1.1 - Kubernetes Components,kube-controller-manager,"kube-controller-manager Control plane component that runs controller processes. Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process. Some types of these controllers are: Node controller: Responsible for noticing and responding when nodes go down.Job controller: Watches for Job objects that represent one-off tasks, then createsPods to run those tasks to completion.EndpointSlice controller: Populates EndpointSlice objects (to provide a link between Services and Pods).ServiceAccount controller: Create default ServiceAccounts for new namespaces.",129
1.1 - Kubernetes Components,cloud-controller-manager,"cloud-controller-manager control plane The cloud-controller-manager only runs controllers that are specific to your cloud provider.If you are running Kubernetes on your own premises, or in a learning environment inside yourown PC, the cluster does not have a cloud controller manager. As with the kube-controller-manager, the cloud-controller-manager combines several logicallyindependent control loops into a single binary that you run as a single process. You canscale horizontally (run more than one copy) to improve performance or to help tolerate failures. The following controllers can have cloud provider dependencies: Node controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops respondingRoute controller: For setting up routes in the underlying cloud infrastructureService controller: For creating, updating and deleting cloud provider load balancers",169
1.1 - Kubernetes Components,kubelet,kubelet An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes.,76
1.1 - Kubernetes Components,kube-proxy,"kube-proxy kube-proxy is a network proxy that runs on eachnode in your cluster,implementing part of the KubernetesService concept. kube-proxymaintains network rules on nodes. These network rules allow networkcommunication to your Pods from network sessions inside or outside ofyour cluster. kube-proxy uses the operating system packet filtering layer if there is oneand it's available. Otherwise, kube-proxy forwards the traffic itself.",98
1.1 - Kubernetes Components,Container runtime,"Container runtime The container runtime is the software that is responsible for running containers. Kubernetes supports container runtimes such ascontainerd, CRI-O,and any other implementation of the Kubernetes CRI (Container RuntimeInterface).",50
1.1 - Kubernetes Components,Addons,"Addons Addons use Kubernetes resources (DaemonSet,Deployment, etc)to implement cluster features. Because these are providing cluster-level features, namespaced resourcesfor addons belong within the kube-system namespace. Selected addons are described below; for an extended list of available addons, pleasesee Addons.",71
1.1 - Kubernetes Components,DNS,"DNS While the other addons are not strictly required, all Kubernetes clusters should have cluster DNS, as many examples rely on it. Cluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which serves DNS records for Kubernetes services. Containers started by Kubernetes automatically include this DNS server in their DNS searches.",80
1.1 - Kubernetes Components,Web UI (Dashboard),"Web UI (Dashboard) Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage and troubleshoot applications running in the cluster, as well as the cluster itself.",46
1.1 - Kubernetes Components,Cluster-level Logging,Cluster-level Logging A cluster-level logging mechanism is responsible forsaving container logs to a central log store with search/browsing interface. Learn about NodesLearn about ControllersLearn about kube-schedulerRead etcd's official documentation,53
1.2 - The Kubernetes API,default,"The Kubernetes API lets you query and manipulate the state of objects in Kubernetes. The core of Kubernetes' control plane is the API server and the HTTP API that it exposes. Users, the different parts of your cluster, and external components all communicate with one another through the API server. The core of Kubernetes' control planeis the API server. The API serverexposes an HTTP API that lets end users, different parts of your cluster, andexternal components communicate with one another. The Kubernetes API lets you query and manipulate the state of API objects in Kubernetes(for example: Pods, Namespaces, ConfigMaps, and Events). Most operations can be performed through thekubectl command-line interface or othercommand-line tools, such askubeadm, which in turn use theAPI. However, you can also access the API directly using REST calls. Consider using one of the client librariesif you are writing an application using the Kubernetes API.",214
1.2 - The Kubernetes API,OpenAPI V2,"OpenAPI V2 The Kubernetes API server serves an aggregated OpenAPI v2 spec via the/openapi/v2 endpoint. You can request the response format usingrequest headers as follows: Valid request header values for OpenAPI v2 queriesHeaderPossible valuesNotesAccept-Encodinggzipnot supplying this header is also acceptableAcceptapplication/com.github.proto-openapi.spec.v2@v1.0+protobufmainly for intra-cluster useapplication/jsondefault*serves application/json Kubernetes implements an alternative Protobuf based serialization format thatis primarily intended for intra-cluster communication. For more informationabout this format, see the Kubernetes Protobuf serialization design proposal and theInterface Definition Language (IDL) files for each schema located in the Gopackages that define the API objects.",187
1.2 - The Kubernetes API,OpenAPI V3,"OpenAPI V3 FEATURE STATE: Kubernetes v1.24 [beta] Kubernetes v1.26 offers beta support for publishing its APIs as OpenAPI v3; this is abeta feature that is enabled by default.You can disable the beta feature by turning off thefeature gate named OpenAPIV3for the kube-apiserver component. A discovery endpoint /openapi/v3 is provided to see a list of allgroup/versions available. This endpoint only returns JSON. These group/versionsare provided in the following format: {    ""paths"": {        ...,        ""api/v1"": {            ""serverRelativeURL"": ""/openapi/v3/api/v1?hash=CC0E9BFD992D8C59AEC98A1E2336F899E8318D3CF4C68944C3DEC640AF5AB52D864AC50DAA8D145B3494F75FA3CFF939FCBDDA431DAD3CA79738B297795818CF""        },        ""apis/admissionregistration.k8s.io/v1"": {            ""serverRelativeURL"": ""/openapi/v3/apis/admissionregistration.k8s.io/v1?hash=E19CC93A116982CE5422FC42B590A8AFAD92CDE9AE4D59B5CAAD568F083AD07946E6CB5817531680BCE6E215C16973CD39003B0425F3477CFD854E89A9DB6597""        },        ....    }} The relative URLs are pointing to immutable OpenAPI descriptions, inorder to improve client-side caching. The proper HTTP caching headersare also set by the API server for that purpose (Expires to 1 year inthe future, and Cache-Control to immutable). When an obsolete URL isused, the API server returns a redirect to the newest URL. The Kubernetes API server publishes an OpenAPI v3 spec per Kubernetesgroup version at the /openapi/v3/apis/<group>/<version>?hash=<hash>endpoint. Refer to the table below for accepted request headers. Valid request header values for OpenAPI v3 queriesHeaderPossible valuesNotesAccept-Encodinggzipnot supplying this header is also acceptableAcceptapplication/com.github.proto-openapi.spec.v3@v1.0+protobufmainly for intra-cluster useapplication/jsondefault*serves application/json",571
1.2 - The Kubernetes API,API groups and versioning,"API groups and versioning To make it easier to eliminate fields or restructure resource representations,Kubernetes supports multiple API versions, each at a different API path, suchas /api/v1 or /apis/rbac.authorization.k8s.io/v1alpha1. Versioning is done at the API level rather than at the resource or field levelto ensure that the API presents a clear, consistent view of system resourcesand behavior, and to enable controlling access to end-of-life and/orexperimental APIs. To make it easier to evolve and to extend its API, Kubernetes implementsAPI groups that can beenabled or disabled. API resources are distinguished by their API group, resource type, namespace(for namespaced resources), and name. The API server handles the conversion betweenAPI versions transparently: all the different versions are actually representationsof the same persisted data. The API server may serve the same underlying datathrough multiple API versions. For example, suppose there are two API versions, v1 and v1beta1, for the sameresource. If you originally created an object using the v1beta1 version of itsAPI, you can later read, update, or delete that object using either the v1beta1or the v1 API version, until the v1beta1 version is deprecated and removed.At that point you can continue accessing and modifying the object using the v1 API.",298
1.2 - The Kubernetes API,API changes,"API changes Any system that is successful needs to grow and change as new use cases emerge or existing ones change.Therefore, Kubernetes has designed the Kubernetes API to continuously change and grow.The Kubernetes project aims to not break compatibility with existing clients, and to maintain thatcompatibility for a length of time so that other projects have an opportunity to adapt. In general, new API resources and new resource fields can be added often and frequently.Elimination of resources or fields requires following theAPI deprecation policy. Kubernetes makes a strong commitment to maintain compatibility for official Kubernetes APIsonce they reach general availability (GA), typically at API version v1. Additionally,Kubernetes maintains compatibility with data persisted via beta API versions of official Kubernetes APIs,and ensures that data can be converted and accessed via GA API versions when the feature goes stable. If you adopt a beta API version, you will need to transition to a subsequent beta or stable API versiononce the API graduates. The best time to do this is while the beta API is in its deprecation period,since objects are simultaneously accessible via both API versions. Once the beta API completes itsdeprecation period and is no longer served, the replacement API version must be used. Note: Although Kubernetes also aims to maintain compatibility for alpha APIs versions, in somecircumstances this is not possible. If you use any alpha API versions, check the release notesfor Kubernetes when upgrading your cluster, in case the API did change in incompatibleways that require deleting all existing alpha objects prior to upgrade. Refer to API versions referencefor more details on the API version level definitions.",351
1.2 - The Kubernetes API,API Extension,"API Extension The Kubernetes API can be extended in one of two ways: Custom resourceslet you declaratively define how the API server should provide your chosen resource API.You can also extend the Kubernetes API by implementing anaggregation layer. Learn how to extend the Kubernetes API by adding your ownCustomResourceDefinition.Controlling Access To The Kubernetes API describeshow the cluster manages authentication and authorization for API access.Learn about API endpoints, resource types and samples by readingAPI Reference.Learn about what constitutes a compatible change, and how to change the API, fromAPI changes.",127
1.3 - Working with Kubernetes Objects,default,Kubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Learn about the Kubernetes object model and how to work with these objects.,48
1.3.1 - Understanding Kubernetes Objects,Understanding Kubernetes objects,"Understanding Kubernetes objects Kubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses theseentities to represent the state of your cluster. Specifically, they can describe: What containerized applications are running (and on which nodes)The resources available to those applicationsThe policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance A Kubernetes object is a ""record of intent""--once you create the object, the Kubernetes systemwill constantly work to ensure that object exists. By creating an object, you're effectivelytelling the Kubernetes system what you want your cluster's workload to look like; this is yourcluster's desired state. To work with Kubernetes objects--whether to create, modify, or delete them--you'll need to use theKubernetes API. When you use the kubectl command-lineinterface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also usethe Kubernetes API directly in your own programs using one of theClient Libraries.",235
1.3.1 - Understanding Kubernetes Objects,Object spec and status,"Object spec and status Almost every Kubernetes object includes two nested object fields that governthe object's configuration: the object spec and the object status.For objects that have a spec, you have to set this when you create the object,providing a description of the characteristics you want the resource to have:its desired state. The status describes the current state of the object, supplied and updatedby the Kubernetes system and its components. The Kubernetescontrol plane continuallyand actively manages every object's actual state to match the desired state yousupplied. For example: in Kubernetes, a Deployment is an object that can represent anapplication running on your cluster. When you create the Deployment, youmight set the Deployment spec to specify that you want three replicas ofthe application to be running. The Kubernetes system reads the Deploymentspec and starts three instances of your desired application--updatingthe status to match your spec. If any of those instances should fail(a status change), the Kubernetes system responds to the differencebetween spec and status by making a correction--in this case, startinga replacement instance. For more information on the object spec, status, and metadata, see theKubernetes API Conventions.",264
1.3.1 - Understanding Kubernetes Objects,Describing a Kubernetes object,"Describing a Kubernetes object When you create an object in Kubernetes, you must provide the object spec that describes itsdesired state, as well as some basic information about the object (such as a name). When you usethe Kubernetes API to create the object (either directly or via kubectl), that API request mustinclude that information as JSON in the request body. Most often, you provide the information tokubectl in a .yaml file. kubectl converts the information to JSON when making the APIrequest. Here's an example .yaml file that shows the required fields and object spec for a Kubernetes Deployment: application/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deploymentspec:  selector:    matchLabels:      app: nginx  replicas: 2 # tells deployment to run 2 pods matching the template  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:1.14.2        ports:        - containerPort: 80 One way to create a Deployment using a .yaml file like the one above is to use thekubectl apply commandin the kubectl command-line interface, passing the .yaml file as an argument. Here's an example: kubectl apply -f https://k8s.io/examples/application/deployment.yaml The output is similar to this: deployment.apps/nginx-deployment created",346
1.3.1 - Understanding Kubernetes Objects,Required fields,"Required fields In the .yaml file for the Kubernetes object you want to create, you'll need to set values for the following fields: apiVersion - Which version of the Kubernetes API you're using to create this objectkind - What kind of object you want to createmetadata - Data that helps uniquely identify the object, including a name string, UID, and optional namespacespec - What state you desire for the object The precise format of the object spec is different for every Kubernetes object, and containsnested fields specific to that object. The Kubernetes API Referencecan help you find the spec format for all of the objects you can create using Kubernetes. For example, see the spec fieldfor the Pod API reference.For each Pod, the .spec field specifies the pod and its desired state (such as the container image name foreach container within that pod).Another example of an object specification is thespec fieldfor the StatefulSet API. For StatefulSet, the .spec field specifies the StatefulSet andits desired state.Within the .spec of a StatefulSet is a templatefor Pod objects. That template describes Pods that the StatefulSet controller will create in order tosatisfy the StatefulSet specification.Different kinds of object can also have different .status; again, the API reference pagesdetail the structure of that .status field, and its content for each different type of object. Learn more about the following: Pods which are the most important basic Kubernetes objects.Deployment objects.Controllers in Kubernetes.Kubernetes API overview which explains some more API concepts.kubectl and kubectl commands.",357
1.3.2 - Kubernetes Object Management,default,The kubectl command-line tool supports several different ways to create and manageKubernetes objects. This document provides an overview of the differentapproaches. Read the Kubectl book fordetails of managing objects by Kubectl.,50
1.3.2 - Kubernetes Object Management,Management techniques,Management techniques Warning: A Kubernetes object should be managed using only one technique. Mixingand matching techniques for the same object results in undefined behavior. Management techniqueOperates onRecommended environmentSupported writersLearning curveImperative commandsLive objectsDevelopment projects1+LowestImperative object configurationIndividual filesProduction projects1ModerateDeclarative object configurationDirectories of filesProduction projects1+Highest,82
1.3.2 - Kubernetes Object Management,Imperative commands,"Imperative commands When using imperative commands, a user operates directly on live objectsin a cluster. The user provides operations tothe kubectl command as arguments or flags. This is the recommended way to get started or to run a one-off task ina cluster. Because this technique operates directly on liveobjects, it provides no history of previous configurations.",73
1.3.2 - Kubernetes Object Management,Examples,"Examples Run an instance of the nginx container by creating a Deployment object: kubectl create deployment nginx --image nginx Examples Create the objects defined in a configuration file: kubectl create -f nginx.yaml Delete the objects defined in two configuration files: kubectl delete -f nginx.yaml -f redis.yaml Update the objects defined in a configuration file by overwritingthe live configuration: kubectl replace -f nginx.yaml Examples Process all object configuration files in the configs directory, and create orpatch the live objects. You can first diff to see what changes are going to bemade, and then apply: kubectl diff -f configs/kubectl apply -f configs/ Recursively process directories: kubectl diff -R -f configs/kubectl apply -R -f configs/",194
1.3.2 - Kubernetes Object Management,Trade-offs,"Trade-offs Advantages compared to object configuration: Commands are expressed as a single action word.Commands require only a single step to make changes to the cluster. Disadvantages compared to object configuration: Commands do not integrate with change review processes.Commands do not provide an audit trail associated with changes.Commands do not provide a source of records except for what is live.Commands do not provide a template for creating new objects. Trade-offs Advantages compared to imperative commands: Object configuration can be stored in a source control system such as Git.Object configuration can integrate with processes such as reviewing changes before push and audit trails.Object configuration provides a template for creating new objects. Disadvantages compared to imperative commands: Object configuration requires basic understanding of the object schema.Object configuration requires the additional step of writing a YAML file. Advantages compared to declarative object configuration: Imperative object configuration behavior is simpler and easier to understand.As of Kubernetes version 1.5, imperative object configuration is more mature. Disadvantages compared to declarative object configuration: Imperative object configuration works best on files, not directories.Updates to live objects must be reflected in configuration files, or they will be lost during the next replacement. Trade-offs Advantages compared to imperative object configuration: Changes made directly to live objects are retained, even if they are not merged back into the configuration files.Declarative object configuration has better support for operating on directories and automatically detecting operation types (create, patch, delete) per-object. Disadvantages compared to imperative object configuration: Declarative object configuration is harder to debug and understand results when they are unexpected.Partial updates using diffs create complex merge and patch operations. Managing Kubernetes Objects Using Imperative CommandsImperative Management of Kubernetes Objects Using Configuration FilesDeclarative Management of Kubernetes Objects Using Configuration FilesDeclarative Management of Kubernetes Objects Using KustomizeKubectl Command ReferenceKubectl BookKubernetes API Reference",426
1.3.2 - Kubernetes Object Management,Imperative object configuration,"Imperative object configuration In imperative object configuration, the kubectl command specifies theoperation (create, replace, etc.), optional flags and at least one filename. The file specified must contain a full definition of the objectin YAML or JSON format. See the API referencefor more details on object definitions. Warning: The imperative replace command replaces the existingspec with the newly provided one, dropping all changes to the object missing fromthe configuration file. This approach should not be used with resourcetypes whose specs are updated independently of the configuration file.Services of type LoadBalancer, for example, have their externalIPs field updatedindependently from the configuration by the cluster.",142
1.3.2 - Kubernetes Object Management,Declarative object configuration,"Declarative object configuration When using declarative object configuration, a user operates on objectconfiguration files stored locally, however the user does not define theoperations to be taken on the files. Create, update, and delete operationsare automatically detected per-object by kubectl. This enables working ondirectories, where different operations might be needed for different objects. Note: Declarative object configuration retains changes made by otherwriters, even if the changes are not merged back to the object configuration file.This is possible by using the patch API operation to write onlyobserved differences, instead of using the replaceAPI operation to replace the entire object configuration.",134
1.3.3 - Object Names and IDs,default,"Each object in your cluster has a Name that is unique for that type of resource.Every Kubernetes object also has a UID that is unique across your whole cluster. For example, you can only have one Pod named myapp-1234 within the same namespace, but you can have one Pod and one Deployment that are each named myapp-1234. For non-unique user-provided attributes, Kubernetes provides labels and annotations.",93
1.3.3 - Object Names and IDs,Names,"Names A client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-name. Only one object of a given kind can have a given name at a time. However, if you delete the object, you can make a new object with the same name. Note: In cases when objects represent a physical entity, like a Node representing a physical host, when the host is re-created under the same name without deleting and re-creating the Node, Kubernetes treats the new host as the old one, which may lead to inconsistencies. Below are four types of commonly used name constraints for resources.",137
1.3.3 - Object Names and IDs,DNS Subdomain Names,"DNS Subdomain Names Most resource types require a name that can be used as a DNS subdomain nameas defined in RFC 1123.This means the name must: contain no more than 253 characterscontain only lowercase alphanumeric characters, '-' or '.'start with an alphanumeric characterend with an alphanumeric character",70
1.3.3 - Object Names and IDs,RFC 1123 Label Names,RFC 1123 Label Names Some resource types require their names to follow the DNSlabel standard as defined in RFC 1123.This means the name must: contain at most 63 characterscontain only lowercase alphanumeric characters or '-'start with an alphanumeric characterend with an alphanumeric character,62
1.3.3 - Object Names and IDs,RFC 1035 Label Names,RFC 1035 Label Names Some resource types require their names to follow the DNSlabel standard as defined in RFC 1035.This means the name must: contain at most 63 characterscontain only lowercase alphanumeric characters or '-'start with an alphabetic characterend with an alphanumeric character,62
1.3.3 - Object Names and IDs,Path Segment Names,"Path Segment Names Some resource types require their names to be able to be safely encoded as apath segment. In other words, the name may not be ""."" or "".."" and the name maynot contain ""/"" or ""%"". Here's an example manifest for a Pod named nginx-demo. apiVersion: v1kind: Podmetadata:  name: nginx-demospec:  containers:  - name: nginx    image: nginx:1.14.2    ports:    - containerPort: 80 Note: Some resource types have additional restrictions on their names.",124
1.3.3 - Object Names and IDs,UIDs,UIDs A Kubernetes systems-generated string to uniquely identify objects. Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities. Kubernetes UIDs are universally unique identifiers (also known as UUIDs).UUIDs are standardized as ISO/IEC 9834-8 and as ITU-T X.667. Read about labels and annotations in Kubernetes.See the Identifiers and Names in Kubernetes design document.,114
1.3.4 - Labels and Selectors,default,"Labels are key/value pairs that are attached to objects, such as pods.Labels are intended to be used to specify identifying attributes of objectsthat are meaningful and relevant to users, but do not directly imply semanticsto the core system. Labels can be used to organize and to select subsets ofobjects. Labels can be attached to objects at creation time and subsequentlyadded and modified at any time. Each object can have a set of key/value labelsdefined. Each Key must be unique for a given object. ""metadata"": {  ""labels"": {    ""key1"" : ""value1"",    ""key2"" : ""value2""  }} Labels allow for efficient queries and watches and are ideal for use in UIsand CLIs. Non-identifying information should be recorded usingannotations.",172
1.3.4 - Labels and Selectors,Motivation,"Motivation Labels enable users to map their own organizational structures onto system objectsin a loosely coupled fashion, without requiring clients to store these mappings. Service deployments and batch processing pipelines are often multi-dimensional entities(e.g., multiple partitions or deployments, multiple release tracks, multiple tiers,multiple micro-services per tier). Management often requires cross-cutting operations,which breaks encapsulation of strictly hierarchical representations, especially rigidhierarchies determined by the infrastructure rather than by users. Example labels: ""release"" : ""stable"", ""release"" : ""canary""""environment"" : ""dev"", ""environment"" : ""qa"", ""environment"" : ""production""""tier"" : ""frontend"", ""tier"" : ""backend"", ""tier"" : ""cache""""partition"" : ""customerA"", ""partition"" : ""customerB""""track"" : ""daily"", ""track"" : ""weekly"" These are examples ofcommonly used labels;you are free to develop your own conventions.Keep in mind that label Key must be unique for a given object.",222
1.3.4 - Labels and Selectors,Syntax and character set,"Syntax and character set Labels are key/value pairs. Valid label keys have two segments: an optionalprefix and name, separated by a slash (/). The name segment is required andmust be 63 characters or less, beginning and ending with an alphanumericcharacter ([a-z0-9A-Z]) with dashes (-), underscores (_), dots (.),and alphanumerics between. The prefix is optional. If specified, the prefixmust be a DNS subdomain: a series of DNS labels separated by dots (.),not longer than 253 characters in total, followed by a slash (/). If the prefix is omitted, the label Key is presumed to be private to the user.Automated system components (e.g. kube-scheduler, kube-controller-manager,kube-apiserver, kubectl, or other third-party automation) which add labelsto end-user objects must specify a prefix. The kubernetes.io/ and k8s.io/ prefixes arereserved for Kubernetes core components. Valid label value: must be 63 characters or less (can be empty),unless empty, must begin and end with an alphanumeric character ([a-z0-9A-Z]),could contain dashes (-), underscores (_), dots (.), and alphanumerics between. For example, here's the configuration file for a Pod that has two labelsenvironment: production and app: nginx: apiVersion: v1kind: Podmetadata:  name: label-demo  labels:    environment: production    app: nginxspec:  containers:  - name: nginx    image: nginx:1.14.2    ports:    - containerPort: 80",368
1.3.4 - Labels and Selectors,Label selectors,"Label selectors Unlike names and UIDs, labelsdo not provide uniqueness. In general, we expect many objects to carry the same label(s). Via a label selector, the client/user can identify a set of objects.The label selector is the core grouping primitive in Kubernetes. The API currently supports two types of selectors: equality-based and set-based.A label selector can be made of multiple requirements which are comma-separated.In the case of multiple requirements, all must be satisfied so the comma separatoracts as a logical AND (&&) operator. The semantics of empty or non-specified selectors are dependent on the context,and API types that use selectors should document the validity and meaning ofthem. Note: For some API types, such as ReplicaSets, the label selectors of two instances mustnot overlap within a namespace, or the controller can see that as conflictinginstructions and fail to determine how many replicas should be present. Caution: For both equality-based and set-based conditions there is no logical OR (||) operator.Ensure your filter statements are structured accordingly.",235
1.3.4 - Labels and Selectors,Equality-based requirement,"Equality-based requirement Equality- or inequality-based requirements allow filtering by label keys and values.Matching objects must satisfy all of the specified label constraints, though they mayhave additional labels as well. Three kinds of operators are admitted =,==,!=.The first two represent equality (and are synonyms), while the latter represents inequality.For example: environment = productiontier != frontend The former selects all resources with key equal to environment and value equal to production.The latter selects all resources with key equal to tier and value distinct from frontend,and all resources with no labels with the tier key. One could filter for resources in productionexcluding frontend using the comma operator: environment=production,tier!=frontend One usage scenario for equality-based label requirement is for Pods to specifynode selection criteria. For example, the sample Pod below selects nodes withthe label ""accelerator=nvidia-tesla-p100"". apiVersion: v1kind: Podmetadata:  name: cuda-testspec:  containers:    - name: cuda-test      image: ""registry.k8s.io/cuda-vector-add:v0.1""      resources:        limits:          nvidia.com/gpu: 1  nodeSelector:    accelerator: nvidia-tesla-p100",280
1.3.4 - Labels and Selectors,Set-based requirement,"Set-based requirement Set-based label requirements allow filtering keys according to a set of values.Three kinds of operators are supported: in,notin and exists (only the key identifier).For example: environment in (production, qa)tier notin (frontend, backend)partition!partition The first example selects all resources with key equal to environment and valueequal to production or qa.The second example selects all resources with key equal to tier and values otherthan frontend and backend, and all resources with no labels with the tier key.The third example selects all resources including a label with key partition;no values are checked.The fourth example selects all resources without a label with key partition;no values are checked. Similarly the comma separator acts as an AND operator. So filtering resourceswith a partition key (no matter the value) and with environment differentthan qa can be achieved using partition,environment notin (qa).The set-based label selector is a general form of equality sinceenvironment=production is equivalent to environment in (production);similarly for != and notin. Set-based requirements can be mixed with equality-based requirements.For example: partition in (customerA, customerB),environment!=qa.",256
1.3.4 - Labels and Selectors,LIST and WATCH filtering,"LIST and WATCH filtering LIST and WATCH operations may specify label selectors to filter the sets of objectsreturned using a query parameter. Both requirements are permitted(presented here as they would appear in a URL query string): equality-based requirements: ?labelSelector=environment%3Dproduction,tier%3Dfrontendset-based requirements: ?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29 Both label selector styles can be used to list or watch resources via a REST client.For example, targeting apiserver with kubectl and using equality-based one may write: kubectl get pods -l environment=production,tier=frontend or using set-based requirements: kubectl get pods -l 'environment in (production),tier in (frontend)' As already mentioned set-based requirements are more expressive.For instance, they can implement the OR operator on values: kubectl get pods -l 'environment in (production, qa)' or restricting negative matching via notin operator: kubectl get pods -l 'environment,environment notin (frontend)'",250
1.3.4 - Labels and Selectors,Service and ReplicationController,"Service and ReplicationController The set of pods that a service targets is defined with a label selector.Similarly, the population of pods that a replicationcontroller shouldmanage is also defined with a label selector. Labels selectors for both objects are defined in json or yaml files using maps,and only equality-based requirement selectors are supported: ""selector"": {    ""component"" : ""redis"",} or selector:    component: redis This selector (respectively in json or yaml format) is equivalent tocomponent=redis or component in (redis).",119
1.3.4 - Labels and Selectors,Resources that support set-based requirements,"Resources that support set-based requirements Newer resources, such as Job,Deployment,ReplicaSet, andDaemonSet,support set-based requirements as well. selector:  matchLabels:    component: redis  matchExpressions:    - {key: tier, operator: In, values: [cache]}    - {key: environment, operator: NotIn, values: [dev]} matchLabels is a map of {key,value} pairs. A single {key,value} in thematchLabels map is equivalent to an element of matchExpressions, whose keyfield is ""key"", the operator is ""In"", and the values array contains only ""value"".matchExpressions is a list of pod selector requirements. Valid operators includeIn, NotIn, Exists, and DoesNotExist. The values set must be non-empty in the case ofIn and NotIn. All of the requirements, from both matchLabels and matchExpressionsare ANDed together -- they must all be satisfied in order to match.",218
1.3.4 - Labels and Selectors,Selecting sets of nodes,"Selecting sets of nodes One use case for selecting over labels is to constrain the set of nodes onto whicha pod can schedule. See the documentation onnode selection for more information. Learn how to add a label to a nodeFind Well-known labels, Annotations and TaintsSee Recommended labelsEnforce Pod Security Standards with Namespace LabelsUse Labels effectively to manage deployments.Read a blog on Writing a Controller for Pod Labels",88
1.3.5 - Namespaces,default,"In Kubernetes, namespaces provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc).",89
1.3.5 - Namespaces,When to Use Multiple Namespaces,"When to Use Multiple Namespaces Namespaces are intended for use in environments with many users spread across multipleteams, or projects. For clusters with a few to tens of users, you should notneed to create or think about namespaces at all. Start using namespaces when youneed the features they provide. Namespaces provide a scope for names. Names of resources need to be unique within a namespace,but not across namespaces. Namespaces cannot be nested inside one another and each Kubernetesresource can only be in one namespace. Namespaces are a way to divide cluster resources between multiple users (via resource quota). It is not necessary to use multiple namespaces to separate slightly differentresources, such as different versions of the same software: uselabels to distinguishresources within the same namespace. Note: For a production cluster, consider not using the default namespace. Instead, make other namespaces and use those.",186
1.3.5 - Namespaces,Initial namespaces,"Initial namespaces Kubernetes starts with four initial namespaces: defaultKubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.kube-node-leaseThis namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure.kube-publicThis namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.kube-systemThe namespace for objects created by the Kubernetes system.",150
1.3.5 - Namespaces,Working with Namespaces,"Working with Namespaces Creation and deletion of namespaces are described in theAdmin Guide documentation for namespaces. Note: Avoid creating namespaces with the prefix kube-, since it is reserved for Kubernetes system namespaces.",46
1.3.5 - Namespaces,Viewing namespaces,Viewing namespaces You can list the current namespaces in a cluster using: kubectl get namespace NAME              STATUS   AGEdefault           Active   1dkube-node-lease   Active   1dkube-public       Active   1dkube-system       Active   1d,61
1.3.5 - Namespaces,Setting the namespace for a request,"Setting the namespace for a request To set the namespace for a current request, use the --namespace flag. For example: kubectl run nginx --image=nginx --namespace=<insert-namespace-name-here>kubectl get pods --namespace=<insert-namespace-name-here>",71
1.3.5 - Namespaces,Setting the namespace preference,Setting the namespace preference You can permanently save the namespace for all subsequent kubectl commands in thatcontext. kubectl config set-context --current --namespace=<insert-namespace-name-here># Validate itkubectl config view --minify | grep namespace:,63
1.3.5 - Namespaces,Namespaces and DNS,"Namespaces and DNS When you create a Service,it creates a corresponding DNS entry.This entry is of the form <service-name>.<namespace-name>.svc.cluster.local, which meansthat if a container only uses <service-name>, it will resolve to the service whichis local to a namespace. This is useful for using the same configuration acrossmultiple namespaces such as Development, Staging and Production. If you want to reachacross namespaces, you need to use the fully qualified domain name (FQDN). As a result, all namespace names must be validRFC 1123 DNS labels. Warning:By creating namespaces with the same name as public top-leveldomains, Services in thesenamespaces can have short DNS names that overlap with public DNS records.Workloads from any namespace performing a DNS lookup without a trailing dot willbe redirected to those services, taking precedence over public DNS.To mitigate this, limit privileges for creating namespaces to trusted users. Ifrequired, you could additionally configure third-party security controls, suchas admissionwebhooks,to block creating any namespace with the name of publicTLDs.",241
1.3.5 - Namespaces,Not all objects are in a namespace,"Not all objects are in a namespace Most Kubernetes resources (e.g. pods, services, replication controllers, and others) arein some namespaces. However namespace resources are not themselves in a namespace.And low-level resources, such asnodes andpersistentVolumes, are not in any namespace. To see which Kubernetes resources are and aren't in a namespace: # In a namespacekubectl api-resources --namespaced=true# Not in a namespacekubectl api-resources --namespaced=false",115
1.3.5 - Namespaces,Automatic labelling,"Automatic labelling FEATURE STATE: Kubernetes 1.22 [stable] The Kubernetes control plane sets an immutable labelkubernetes.io/metadata.name on all namespaces, provided that the NamespaceDefaultLabelNamefeature gate is enabled.The value of the label is the namespace name. Learn more about creating a new namespace.Learn more about deleting a namespace.",82
1.3.6 - Annotations,Attaching metadata to objects,"Attaching metadata to objects You can use either labels or annotations to attach metadata to Kubernetesobjects. Labels can be used to select objects and to findcollections of objects that satisfy certain conditions. In contrast, annotationsare not used to identify and select objects. The metadatain an annotation can be small or large, structured or unstructured, and caninclude characters not permitted by labels. Annotations, like labels, are key/value maps: ""metadata"": {  ""annotations"": {    ""key1"" : ""value1"",    ""key2"" : ""value2""  }} Note: The keys and the values in the map must be strings. In other words, you cannot usenumeric, boolean, list or other types for either the keys or the values. Here are some examples of information that could be recorded in annotations: Fields managed by a declarative configuration layer. Attaching these fieldsas annotations distinguishes them from default values set by clients orservers, and from auto-generated fields and fields set byauto-sizing or auto-scaling systems.Build, release, or image information like timestamps, release IDs, git branch,PR numbers, image hashes, and registry address.Pointers to logging, monitoring, analytics, or audit repositories.Client library or tool information that can be used for debugging purposes:for example, name, version, and build information.User or tool/system provenance information, such as URLs of related objectsfrom other ecosystem components.Lightweight rollout tool metadata: for example, config or checkpoints.Phone or pager numbers of persons responsible, or directory entries thatspecify where that information can be found, such as a team web site.Directives from the end-user to the implementations to modify behavior orengage non-standard features. Instead of using annotations, you could store this type of information in anexternal database or directory, but that would make it much harder to produceshared client libraries and tools for deployment, management, introspection,and the like.",420
1.3.6 - Annotations,Syntax and character set,"Syntax and character set Annotations are key/value pairs. Valid annotation keys have two segments: an optional prefix and name, separated by a slash (/). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (_), dots (.), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (.), not longer than 253 characters in total, followed by a slash (/). If the prefix is omitted, the annotation Key is presumed to be private to the user. Automated system components (e.g. kube-scheduler, kube-controller-manager, kube-apiserver, kubectl, or other third-party automation) which add annotations to end-user objects must specify a prefix. The kubernetes.io/ and k8s.io/ prefixes are reserved for Kubernetes core components. For example, here's the configuration file for a Pod that has the annotation imageregistry: https://hub.docker.com/ : apiVersion: v1kind: Podmetadata:  name: annotations-demo  annotations:    imageregistry: ""https://hub.docker.com/""spec:  containers:  - name: nginx    image: nginx:1.14.2    ports:    - containerPort: 80 Learn more about Labels and Selectors.",325
1.3.7 - Field Selectors,default,"Field selectors let you select Kubernetes resources based on the value of one or more resource fields. Here are some examples of field selector queries: metadata.name=my-servicemetadata.namespace!=defaultstatus.phase=Pending This kubectl command selects all Pods for which the value of the status.phase field is Running: kubectl get pods --field-selector status.phase=Running Note: Field selectors are essentially resource filters. By default, no selectors/filters are applied, meaning that all resources of the specified type are selected. This makes the kubectl queries kubectl get pods and kubectl get pods --field-selector """" equivalent.",154
1.3.7 - Field Selectors,Supported fields,"Supported fields Supported field selectors vary by Kubernetes resource type. All resource types support the metadata.name and metadata.namespace fields. Using unsupported field selectors produces an error. For example: kubectl get ingress --field-selector foo.bar=baz Error from server (BadRequest): Unable to find ""ingresses"" that match label selector """", field selector ""foo.bar=baz"": ""foo.bar"" is not a known field selector: only ""metadata.name"", ""metadata.namespace""",114
1.3.7 - Field Selectors,Supported operators,"Supported operators You can use the =, ==, and != operators with field selectors (= and == mean the same thing). This kubectl command, for example, selects all Kubernetes Services that aren't in the default namespace: kubectl get services  --all-namespaces --field-selector metadata.namespace!=default",74
1.3.7 - Field Selectors,Chained selectors,"Chained selectors As with label and other selectors, field selectors can be chained together as a comma-separated list. This kubectl command selects all Pods for which the status.phase does not equal Running and the spec.restartPolicy field equals Always: kubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always",84
1.3.7 - Field Selectors,Multiple resource types,"Multiple resource types You can use field selectors across multiple resource types. This kubectl command selects all Statefulsets and Services that are not in the default namespace: kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default",62
1.3.8 - Finalizers,default,"Finalizers are namespaced keys that tell Kubernetes to wait until specificconditions are met before it fully deletes resources marked for deletion.Finalizers alert controllersto clean up resources the deleted object owned. When you tell Kubernetes to delete an object that has finalizers specified forit, the Kubernetes API marks the object for deletion by populating .metadata.deletionTimestamp,and returns a 202 status code (HTTP ""Accepted""). The target object remains in a terminating state while thecontrol plane, or other components, take the actions defined by the finalizers.After these actions are complete, the controller removes the relevant finalizersfrom the target object. When the metadata.finalizers field is empty,Kubernetes considers the deletion complete and deletes the object. You can use finalizers to control garbage collectionof resources. For example, you can define a finalizer to clean up related resources orinfrastructure before the controller deletes the target resource. You can use finalizers to control garbage collectionof resources by alerting controllers to perform specific cleanup tasks beforedeleting the target resource. Finalizers don't usually specify the code to execute. Instead, they aretypically lists of keys on a specific resource similar to annotations.Kubernetes specifies some finalizers automatically, but you can also specifyyour own.",277
1.3.8 - Finalizers,How finalizers work,"How finalizers work When you create a resource using a manifest file, you can specify finalizers inthe metadata.finalizers field. When you attempt to delete the resource, theAPI server handling the delete request notices the values in the finalizers fieldand does the following: Modifies the object to add a metadata.deletionTimestamp field with thetime you started the deletion.Prevents the object from being removed until its metadata.finalizers field is empty.Returns a 202 status code (HTTP ""Accepted"") The controller managing that finalizer notices the update to the object setting themetadata.deletionTimestamp, indicating deletion of the object has been requested.The controller then attempts to satisfy the requirements of the finalizersspecified for that resource. Each time a finalizer condition is satisfied, thecontroller removes that key from the resource's finalizers field. When thefinalizers field is emptied, an object with a deletionTimestamp field setis automatically deleted. You can also use finalizers to prevent deletion of unmanaged resources. A common example of a finalizer is kubernetes.io/pv-protection, which preventsaccidental deletion of PersistentVolume objects. When a PersistentVolumeobject is in use by a Pod, Kubernetes adds the pv-protection finalizer. If youtry to delete the PersistentVolume, it enters a Terminating status, but thecontroller can't delete it because the finalizer exists. When the Pod stopsusing the PersistentVolume, Kubernetes clears the pv-protection finalizer,and the controller deletes the volume.",330
1.3.8 - Finalizers,"Owner references, labels, and finalizers","Owner references, labels, and finalizers Like labels,owner referencesdescribe the relationships between objects in Kubernetes, but are used for adifferent purpose. When acontroller manages objectslike Pods, it uses labels to track changes to groups of related objects. Forexample, when a Job creates one ormore Pods, the Job controller applies labels to those pods and tracks changes toany Pods in the cluster with the same label. The Job controller also adds owner references to those Pods, pointing at theJob that created the Pods. If you delete the Job while these Pods are running,Kubernetes uses the owner references (not labels) to determine which Pods in thecluster need cleanup. Kubernetes also processes finalizers when it identifies owner references on aresource targeted for deletion. In some situations, finalizers can block the deletion of dependent objects,which can cause the targeted owner object to remain forlonger than expected without being fully deleted. In these situations, youshould check finalizers and owner references on the target owner and dependentobjects to troubleshoot the cause. Note: In cases where objects are stuck in a deleting state, avoid manuallyremoving finalizers to allow deletion to continue. Finalizers are usually addedto resources for a reason, so forcefully removing them can lead to issues inyour cluster. This should only be done when the purpose of the finalizer isunderstood and is accomplished in another way (for example, manually cleaningup some dependent object). Read Using Finalizers to Control Deletionon the Kubernetes blog.",326
1.3.9 - Owners and Dependents,default,"In Kubernetes, some objects are owners of other objects. For example, aReplicaSet is the owner of a set of Pods. These owned objects are dependentsof their owner. Ownership is different from the labels and selectorsmechanism that some resources also use. For example, consider a Service thatcreates EndpointSlice objects. The Service uses labels to allow the control plane todetermine which EndpointSlice objects are used for that Service. In additionto the labels, each EndpointSlice that is managed on behalf of a Service hasan owner reference. Owner references help different parts of Kubernetes avoidinterfering with objects they don’t control.",147
1.3.9 - Owners and Dependents,Owner references in object specifications,"Owner references in object specifications Dependent objects have a metadata.ownerReferences field that references theirowner object. A valid owner reference consists of the object name and a UIDwithin the same namespace as the dependent object. Kubernetes sets the value ofthis field automatically for objects that are dependents of other objects likeReplicaSets, DaemonSets, Deployments, Jobs and CronJobs, and ReplicationControllers.You can also configure these relationships manually by changing the value ofthis field. However, you usually don't need to and can allow Kubernetes toautomatically manage the relationships. Dependent objects also have an ownerReferences.blockOwnerDeletion field thattakes a boolean value and controls whether specific dependents can block garbagecollection from deleting their owner object. Kubernetes automatically sets thisfield to true if a controller(for example, the Deployment controller) sets the value of themetadata.ownerReferences field. You can also set the value of theblockOwnerDeletion field manually to control which dependents block garbagecollection. A Kubernetes admission controller controls user access to change this field fordependent resources, based on the delete permissions of the owner. This controlprevents unauthorized users from delaying owner object deletion. Note:Cross-namespace owner references are disallowed by design.Namespaced dependents can specify cluster-scoped or namespaced owners.A namespaced owner must exist in the same namespace as the dependent.If it does not, the owner reference is treated as absent, and the dependentis subject to deletion once all owners are verified absent.Cluster-scoped dependents can only specify cluster-scoped owners.In v1.20+, if a cluster-scoped dependent specifies a namespaced kind as an owner,it is treated as having an unresolvable owner reference, and is not able to be garbage collected.In v1.20+, if the garbage collector detects an invalid cross-namespace ownerReference,or a cluster-scoped dependent with an ownerReference referencing a namespaced kind, a warning Eventwith a reason of OwnerRefInvalidNamespace and an involvedObject of the invalid dependent is reported.You can check for that kind of Event by runningkubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace.",483
1.3.9 - Owners and Dependents,Ownership and finalizers,"Ownership and finalizers When you tell Kubernetes to delete a resource, the API server allows themanaging controller to process any finalizer rulesfor the resource. Finalizersprevent accidental deletion of resources your cluster may still need to functioncorrectly. For example, if you try to delete a PersistentVolume that is stillin use by a Pod, the deletion does not happen immediately because thePersistentVolume has the kubernetes.io/pv-protection finalizer on it.Instead, the volume remains in the Terminating status until Kubernetes clearsthe finalizer, which only happens after the PersistentVolume is no longerbound to a Pod. Kubernetes also adds finalizers to an owner resource when you use eitherforeground or orphan cascading deletion.In foreground deletion, it adds the foreground finalizer so that thecontroller must delete dependent resources that also haveownerReferences.blockOwnerDeletion=true before it deletes the owner. If youspecify an orphan deletion policy, Kubernetes adds the orphan finalizer sothat the controller ignores dependent resources after it deletes the ownerobject. Learn more about Kubernetes finalizers.Learn about garbage collection.Read the API reference for object metadata.",259
1.3.10 - Recommended Labels,default,"You can visualize and manage Kubernetes objects with more tools than kubectl andthe dashboard. A common set of labels allows tools to work interoperably, describingobjects in a common manner that all tools can understand. In addition to supporting tooling, the recommended labels describe applicationsin a way that can be queried. The metadata is organized around the concept of an application. Kubernetes is nota platform as a service (PaaS) and doesn't have or enforce a formal notion of an application.Instead, applications are informal and described with metadata. The definition ofwhat an application contains is loose. Note: These are recommended labels. They make it easier to manage applicationsbut aren't required for any core tooling. Shared labels and annotations share a common prefix: app.kubernetes.io. Labelswithout a prefix are private to users. The shared prefix ensures that shared labelsdo not interfere with custom user labels.",194
1.3.10 - Recommended Labels,Labels,"Labels In order to take full advantage of using these labels, they should be appliedon every resource object. KeyDescriptionExampleTypeapp.kubernetes.io/nameThe name of the applicationmysqlstringapp.kubernetes.io/instanceA unique name identifying the instance of an applicationmysql-abcxzystringapp.kubernetes.io/versionThe current version of the application (e.g., a SemVer 1.0, revision hash, etc.)5.7.21stringapp.kubernetes.io/componentThe component within the architecturedatabasestringapp.kubernetes.io/part-ofThe name of a higher level application this one is part ofwordpressstringapp.kubernetes.io/managed-byThe tool being used to manage the operation of an applicationhelmstring To illustrate these labels in action, consider the following StatefulSet object: # This is an excerptapiVersion: apps/v1kind: StatefulSetmetadata:  labels:    app.kubernetes.io/name: mysql    app.kubernetes.io/instance: mysql-abcxzy    app.kubernetes.io/version: ""5.7.21""    app.kubernetes.io/component: database    app.kubernetes.io/part-of: wordpress    app.kubernetes.io/managed-by: helm",311
1.3.10 - Recommended Labels,Applications And Instances Of Applications,"Applications And Instances Of Applications An application can be installed one or more times into a Kubernetes cluster and,in some cases, the same namespace. For example, WordPress can be installed morethan once where different websites are different installations of WordPress. The name of an application and the instance name are recorded separately. Forexample, WordPress has a app.kubernetes.io/name of wordpress while it hasan instance name, represented as app.kubernetes.io/instance with a value ofwordpress-abcxzy. This enables the application and instance of the applicationto be identifiable. Every instance of an application must have a unique name.",137
1.3.10 - Recommended Labels,A Simple Stateless Service,A Simple Stateless Service Consider the case for a simple stateless service deployed using Deployment and Service objects. The following two snippets represent how the labels could be used in their simplest form. The Deployment is used to oversee the pods running the application itself. apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app.kubernetes.io/name: myservice    app.kubernetes.io/instance: myservice-abcxzy... The Service is used to expose the application. apiVersion: v1kind: Servicemetadata:  labels:    app.kubernetes.io/name: myservice    app.kubernetes.io/instance: myservice-abcxzy...,158
1.3.10 - Recommended Labels,Web Application With A Database,"Web Application With A Database Consider a slightly more complicated application: a web application (WordPress)using a database (MySQL), installed using Helm. The following snippets illustratethe start of objects used to deploy this application. The start to the following Deployment is used for WordPress: apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app.kubernetes.io/name: wordpress    app.kubernetes.io/instance: wordpress-abcxzy    app.kubernetes.io/version: ""4.9.4""    app.kubernetes.io/managed-by: helm    app.kubernetes.io/component: server    app.kubernetes.io/part-of: wordpress... The Service is used to expose WordPress: apiVersion: v1kind: Servicemetadata:  labels:    app.kubernetes.io/name: wordpress    app.kubernetes.io/instance: wordpress-abcxzy    app.kubernetes.io/version: ""4.9.4""    app.kubernetes.io/managed-by: helm    app.kubernetes.io/component: server    app.kubernetes.io/part-of: wordpress... MySQL is exposed as a StatefulSet with metadata for both it and the larger application it belongs to: apiVersion: apps/v1kind: StatefulSetmetadata:  labels:    app.kubernetes.io/name: mysql    app.kubernetes.io/instance: mysql-abcxzy    app.kubernetes.io/version: ""5.7.21""    app.kubernetes.io/managed-by: helm    app.kubernetes.io/component: database    app.kubernetes.io/part-of: wordpress... The Service is used to expose MySQL as part of WordPress: apiVersion: v1kind: Servicemetadata:  labels:    app.kubernetes.io/name: mysql    app.kubernetes.io/instance: mysql-abcxzy    app.kubernetes.io/version: ""5.7.21""    app.kubernetes.io/managed-by: helm    app.kubernetes.io/component: database    app.kubernetes.io/part-of: wordpress... With the MySQL StatefulSet and Service you'll notice information about both MySQL and WordPress, the broader application, are included.",566
2.1 - Nodes,default,"Kubernetes runs your workload by placing containers into Pods to run on Nodes.A node may be a virtual or physical machine, depending on the cluster. Each nodeis managed by thecontrol planeand contains the services necessary to runPods. Typically you have several nodes in a cluster; in a learning or resource-limitedenvironment, you might have only one node. The components on a node include thekubelet, acontainer runtime, and thekube-proxy.",100
2.1 - Nodes,Management,"Management There are two main ways to have Nodes added to the API server: The kubelet on a node self-registers to the control planeYou (or another human user) manually add a Node object After you create a Node object,or the kubelet on a node self-registers, the control plane checks whether the new Node object isvalid. For example, if you try to create a Node from the following JSON manifest: {  ""kind"": ""Node"",  ""apiVersion"": ""v1"",  ""metadata"": {    ""name"": ""10.240.79.157"",    ""labels"": {      ""name"": ""my-first-k8s-node""    }  }} Kubernetes creates a Node object internally (the representation). Kubernetes checksthat a kubelet has registered to the API server that matches the metadata.namefield of the Node. If the node is healthy (i.e. all necessary services are running),then it is eligible to run a Pod. Otherwise, that node is ignored for any cluster activityuntil it becomes healthy. Note:Kubernetes keeps the object for the invalid Node and continues checking to see whetherit becomes healthy.You, or a controller, must explicitlydelete the Node object to stop that health checking. The name of a Node object must be a validDNS subdomain name.",289
2.1 - Nodes,Node name uniqueness,"Node name uniqueness The name identifies a Node. Two Nodescannot have the same name at the same time. Kubernetes also assumes that a resource with the samename is the same object. In case of a Node, it is implicitly assumed that an instance using thesame name will have the same state (e.g. network settings, root disk contents)and attributes like node labels. This may lead toinconsistencies if an instance was modified without changing its name. If the Node needs to bereplaced or updated significantly, the existing Node object needs to be removed from API serverfirst and re-added after the update.",130
2.1 - Nodes,Self-registration of Nodes,"Self-registration of Nodes When the kubelet flag --register-node is true (the default), the kubelet will attempt toregister itself with the API server. This is the preferred pattern, used by most distros. For self-registration, the kubelet is started with the following options: --kubeconfig - Path to credentials to authenticate itself to the API server.--cloud-provider - How to talk to a cloud providerto read metadata about itself.--register-node - Automatically register with the API server.--register-with-taints - Register the node with the given list oftaints (comma separated <key>=<value>:<effect>).No-op if register-node is false.--node-ip - IP address of the node.--node-labels - Labels to add when registering the nodein the cluster (see label restrictions enforced by theNodeRestriction admission plugin).--node-status-update-frequency - Specifies how often kubelet posts its node status to the API server. When the Node authorization mode andNodeRestriction admission pluginare enabled, kubelets are only authorized to create/modify their own Node resource. Note:As mentioned in the Node name uniqueness section,when Node configuration needs to be updated, it is a good practice to re-registerthe node with the API server. For example, if the kubelet being restarted withthe new set of --node-labels, but the same Node name is used, the change willnot take an effect, as labels are being set on the Node registration.Pods already scheduled on the Node may misbehave or cause issues if the Nodeconfiguration will be changed on kubelet restart. For example, already runningPod may be tainted against the new labels assigned to the Node, while otherPods, that are incompatible with that Pod will be scheduled based on this newlabel. Node re-registration ensures all Pods will be drained and properlyre-scheduled.",424
2.1 - Nodes,Manual Node administration,"Manual Node administration You can create and modify Node objects usingkubectl. When you want to create Node objects manually, set the kubelet flag --register-node=false. You can modify Node objects regardless of the setting of --register-node.For example, you can set labels on an existing Node or mark it unschedulable. You can use labels on Nodes in conjunction with node selectors on Pods to controlscheduling. For example, you can constrain a Pod to only be eligible to run ona subset of the available nodes. Marking a node as unschedulable prevents the scheduler from placing new pods ontothat Node but does not affect existing Pods on the Node. This is useful as apreparatory step before a node reboot or other maintenance. To mark a Node unschedulable, run: kubectl cordon $NODENAME See Safely Drain a Nodefor more details. Note: Pods that are part of a DaemonSet toleratebeing run on an unschedulable Node. DaemonSets typically provide node-local servicesthat should run on the Node even if it is being drained of workload applications.",251
2.1 - Nodes,Node status,Node status A Node's status contains the following information: AddressesConditionsCapacity and AllocatableInfo You can use kubectl to view a Node's status and other details: kubectl describe node <insert-node-name-here> Each section of the output is described below.,63
2.1 - Nodes,Addresses,Addresses The usage of these fields varies depending on your cloud provider or bare metal configuration. HostName: The hostname as reported by the node's kernel. Can be overridden via the kubelet--hostname-override parameter.ExternalIP: Typically the IP address of the node that is externally routable (available fromoutside the cluster).InternalIP: Typically the IP address of the node that is routable only within the cluster.,90
2.1 - Nodes,Conditions,"Conditions The conditions field describes the status of all Running nodes. Examples of conditions include: Node conditions, and a description of when each condition applies.Node ConditionDescriptionReadyTrue if the node is healthy and ready to accept pods, False if the node is not healthy and is not accepting pods, and Unknown if the node controller has not heard from the node in the last node-monitor-grace-period (default is 40 seconds)DiskPressureTrue if pressure exists on the disk size—that is, if the disk capacity is low; otherwise FalseMemoryPressureTrue if pressure exists on the node memory—that is, if the node memory is low; otherwise FalsePIDPressureTrue if pressure exists on the processes—that is, if there are too many processes on the node; otherwise FalseNetworkUnavailableTrue if the network for the node is not correctly configured, otherwise False Note: If you use command-line tools to print details of a cordoned Node, the Condition includesSchedulingDisabled. SchedulingDisabled is not a Condition in the Kubernetes API; instead,cordoned nodes are marked Unschedulable in their spec. In the Kubernetes API, a node's condition is represented as part of the .statusof the Node resource. For example, the following JSON structure describes a healthy node: ""conditions"": [  {    ""type"": ""Ready"",    ""status"": ""True"",    ""reason"": ""KubeletReady"",    ""message"": ""kubelet is posting ready status"",    ""lastHeartbeatTime"": ""2019-06-05T18:38:35Z"",    ""lastTransitionTime"": ""2019-06-05T11:41:27Z""  }] If the status of the Ready condition remains Unknown or False for longerthan the pod-eviction-timeout (an argument passed to thekube-controller-manager), then the node controller triggersAPI-initiated evictionfor all Pods assigned to that node. The default eviction timeout duration isfive minutes.In some cases when the node is unreachable, the API server is unable to communicatewith the kubelet on the node. The decision to delete the pods cannot be communicated tothe kubelet until communication with the API server is re-established. In the meantime,the pods that are scheduled for deletion may continue to run on the partitioned node. The node controller does not force delete pods until it is confirmed that they have stoppedrunning in the cluster. You can see the pods that might be running on an unreachable node asbeing in the Terminating or Unknown state. In cases where Kubernetes cannot deduce from theunderlying infrastructure if a node has permanently left a cluster, the cluster administratormay need to delete the node object by hand. Deleting the node object from Kubernetes causesall the Pod objects running on the node to be deleted from the API server and frees up theirnames. When problems occur on nodes, the Kubernetes control plane automatically createstaints that match the conditionsaffecting the node.The scheduler takes the Node's taints into consideration when assigning a Pod to a Node.Pods can also have tolerations that letthem run on a Node even though it has a specific taint. See Taint Nodes by Conditionfor more details.",699
2.1 - Nodes,Capacity and Allocatable,"Capacity and Allocatable Describes the resources available on the node: CPU, memory, and the maximumnumber of pods that can be scheduled onto the node. The fields in the capacity block indicate the total amount of resources that aNode has. The allocatable block indicates the amount of resources on aNode that is available to be consumed by normal Pods. You may read more about capacity and allocatable resources while learning howto reserve compute resourceson a Node.",94
2.1 - Nodes,Info,"Info Describes general information about the node, such as kernel version, Kubernetesversion (kubelet and kube-proxy version), container runtime details, and whichoperating system the node uses.The kubelet gathers this information from the node and publishes it intothe Kubernetes API.",64
2.1 - Nodes,Heartbeats,"Heartbeats Heartbeats, sent by Kubernetes nodes, help your cluster determine theavailability of each node, and to take action when failures are detected. For nodes there are two forms of heartbeats: updates to the .status of a NodeLease objectswithin the kube-node-leasenamespace.Each Node has an associated Lease object. Compared to updates to .status of a Node, a Lease is a lightweight resource.Using Leases for heartbeats reduces the performance impact of these updatesfor large clusters. The kubelet is responsible for creating and updating the .status of Nodes,and for updating their related Leases. The kubelet updates the node's .status either when there is change in statusor if there has been no update for a configured interval. The default intervalfor .status updates to Nodes is 5 minutes, which is much longer than the 40second default timeout for unreachable nodes.The kubelet creates and then updates its Lease object every 10 seconds(the default update interval). Lease updates occur independently fromupdates to the Node's .status. If the Lease update fails, the kubelet retries,using exponential backoff that starts at 200 milliseconds and capped at 7 seconds.",266
2.1 - Nodes,Node controller,"Node controller The node controller is aKubernetes control plane component that manages various aspects of nodes. The node controller has multiple roles in a node's life. The first is assigning aCIDR block to the node when it is registered (if CIDR assignment is turned on). The second is keeping the node controller's internal list of nodes up to date withthe cloud provider's list of available machines. When running in a cloudenvironment and whenever a node is unhealthy, the node controller asks the cloudprovider if the VM for that node is still available. If not, the nodecontroller deletes the node from its list of nodes. The third is monitoring the nodes' health. The node controller isresponsible for: In the case that a node becomes unreachable, updating the Ready conditionin the Node's .status field. In this case the node controller sets theReady condition to Unknown.If a node remains unreachable: triggeringAPI-initiated evictionfor all of the Pods on the unreachable node. By default, the node controllerwaits 5 minutes between marking the node as Unknown and submittingthe first eviction request. By default, the node controller checks the state of each node every 5 seconds.This period can be configured using the --node-monitor-period flag on thekube-controller-manager component.",273
2.1 - Nodes,Rate limits on eviction,"Rate limits on eviction In most cases, the node controller limits the eviction rate to--node-eviction-rate (default 0.1) per second, meaning it won't evict podsfrom more than 1 node per 10 seconds. The node eviction behavior changes when a node in a given availability zonebecomes unhealthy. The node controller checks what percentage of nodes in the zoneare unhealthy (the Ready condition is Unknown or False) atthe same time: If the fraction of unhealthy nodes is at least --unhealthy-zone-threshold(default 0.55), then the eviction rate is reduced.If the cluster is small (i.e. has less than or equal to--large-cluster-size-threshold nodes - default 50), then evictions are stopped.Otherwise, the eviction rate is reduced to --secondary-node-eviction-rate(default 0.01) per second. The reason these policies are implemented per availability zone is because oneavailability zone might become partitioned from the control plane while the others remainconnected. If your cluster does not span multiple cloud provider availability zones,then the eviction mechanism does not take per-zone unavailability into account. A key reason for spreading your nodes across availability zones is so that theworkload can be shifted to healthy zones when one entire zone goes down.Therefore, if all nodes in a zone are unhealthy, then the node controller evicts atthe normal rate of --node-eviction-rate. The corner case is when all zones arecompletely unhealthy (none of the nodes in the cluster are healthy). In such acase, the node controller assumes that there is some problem with connectivitybetween the control plane and the nodes, and doesn't perform any evictions.(If there has been an outage and some nodes reappear, the node controller doesevict pods from the remaining nodes that are unhealthy or unreachable). The node controller is also responsible for evicting pods running on nodes withNoExecute taints, unless those pods tolerate that taint.The node controller also adds taintscorresponding to node problems like node unreachable or not ready. This meansthat the scheduler won't place Pods onto unhealthy nodes.",446
2.1 - Nodes,Resource capacity tracking,"Resource capacity tracking Node objects track information about the Node's resource capacity: for example, the amountof memory available and the number of CPUs.Nodes that self register report their capacity duringregistration. If you manually add a Node, thenyou need to set the node's capacity information when you add it. The Kubernetes scheduler ensures thatthere are enough resources for all the Pods on a Node. The scheduler checks that the sumof the requests of containers on the node is no greater than the node's capacity.That sum of requests includes all containers managed by the kubelet, but excludes anycontainers started directly by the container runtime, and also excludes anyprocesses running outside of the kubelet's control. Note: If you want to explicitly reserve resources for non-Pod processes, seereserve resources for system daemons.",175
2.1 - Nodes,Node topology,"Node topology FEATURE STATE: Kubernetes v1.18 [beta] If you have enabled the TopologyManagerfeature gate, thenthe kubelet can use topology hints when making resource assignment decisions.See Control Topology Management Policies on a Nodefor more information.",61
2.1 - Nodes,Graceful node shutdown,"Graceful node shutdown FEATURE STATE: Kubernetes v1.21 [beta] The kubelet attempts to detect node system shutdown and terminates pods running on the node. Kubelet ensures that pods follow the normalpod termination processduring the node shutdown. The Graceful node shutdown feature depends on systemd since it takes advantage ofsystemd inhibitor locks todelay the node shutdown with a given duration. Graceful node shutdown is controlled with the GracefulNodeShutdownfeature gate which isenabled by default in 1.21. Note that by default, both configuration options described below,shutdownGracePeriod and shutdownGracePeriodCriticalPods are set to zero,thus not activating the graceful node shutdown functionality.To activate the feature, the two kubelet config settings should be configured appropriately andset to non-zero values. During a graceful shutdown, kubelet terminates pods in two phases: Terminate regular pods running on the node.Terminate critical podsrunning on the node. Graceful node shutdown feature is configured with twoKubeletConfiguration options: shutdownGracePeriod:Specifies the total duration that the node should delay the shutdown by. This is the totalgrace period for pod termination for both regular andcritical pods.shutdownGracePeriodCriticalPods:Specifies the duration used to terminatecritical podsduring a node shutdown. This value should be less than shutdownGracePeriod. For example, if shutdownGracePeriod=30s, andshutdownGracePeriodCriticalPods=10s, kubelet will delay the node shutdown by30 seconds. During the shutdown, the first 20 (30-10) seconds would be reservedfor gracefully terminating normal pods, and the last 10 seconds would bereserved for terminating critical pods. Note:When pods were evicted during the graceful node shutdown, they are marked as shutdown.Running kubectl get pods shows the status of the evicted pods as Terminated.And kubectl describe pod indicates that the pod was evicted because of node shutdown:Reason:         TerminatedMessage:        Pod was terminated in response to imminent node shutdown.",445
2.1 - Nodes,Pod Priority based graceful node shutdown,"Pod Priority based graceful node shutdown FEATURE STATE: Kubernetes v1.23 [alpha] To provide more flexibility during graceful node shutdown around the orderingof pods during shutdown, graceful node shutdown honors the PriorityClass forPods, provided that you enabled this feature in your cluster. The featureallows cluster administers to explicitly define the ordering of podsduring graceful node shutdown based onpriority classes. The Graceful Node Shutdown feature, as describedabove, shuts down pods in two phases, non-critical pods, followed by criticalpods. If additional flexibility is needed to explicitly define the ordering ofpods during shutdown in a more granular way, pod priority based gracefulshutdown can be used. When graceful node shutdown honors pod priorities, this makes it possible to dograceful node shutdown in multiple phases, each phase shutting down aparticular priority class of pods. The kubelet can be configured with the exactphases and shutdown time per phase. Assuming the following custom podpriority classesin a cluster, Pod priority class namePod priority class valuecustom-class-a100000custom-class-b10000custom-class-c1000regular/unset0 Within the kubelet configurationthe settings for shutdownGracePeriodByPodPriority could look like: Pod priority class valueShutdown period10000010 seconds10000180 seconds1000120 seconds060 seconds The corresponding kubelet config YAML configuration would be: shutdownGracePeriodByPodPriority:  - priority: 100000    shutdownGracePeriodSeconds: 10  - priority: 10000    shutdownGracePeriodSeconds: 180  - priority: 1000    shutdownGracePeriodSeconds: 120  - priority: 0    shutdownGracePeriodSeconds: 60 The above table implies that any pod with priority value >= 100000 will getjust 10 seconds to stop, any pod with value >= 10000 and < 100000 will get 180seconds to stop, any pod with value >= 1000 and < 10000 will get 120 seconds to stop.Finally, all other pods will get 60 seconds to stop. One doesn't have to specify values corresponding to all of the classes. Forexample, you could instead use these settings: Pod priority class valueShutdown period100000300 seconds1000120 seconds060 seconds In the above case, the pods with custom-class-b will go into the same bucketas custom-class-c for shutdown. If there are no pods in a particular range, then the kubelet does not waitfor pods in that priority range. Instead, the kubelet immediately skips to thenext priority class value range. If this feature is enabled and no configuration is provided, then no orderingaction will be taken. Using this feature requires enabling the GracefulNodeShutdownBasedOnPodPriorityfeature gate, and setting ShutdownGracePeriodByPodPriority in thekubelet configto the desired configuration containing the pod priority class values andtheir respective shutdown periods. Note: The ability to take Pod priority into account during graceful node shutdown was introducedas an Alpha feature in Kubernetes v1.23. In Kubernetes 1.26the feature is Beta and is enabled by default. Metrics graceful_shutdown_start_time_seconds and graceful_shutdown_end_time_secondsare emitted under the kubelet subsystem to monitor node shutdowns.",695
2.1 - Nodes,Non Graceful node shutdown,"Non Graceful node shutdown FEATURE STATE: Kubernetes v1.26 [beta] A node shutdown action may not be detected by kubelet's Node Shutdown Manager,either because the command does not trigger the inhibitor locks mechanism used bykubelet or because of a user error, i.e., the ShutdownGracePeriod andShutdownGracePeriodCriticalPods are not configured properly. Please refer to abovesection Graceful Node Shutdown for more details. When a node is shutdown but not detected by kubelet's Node Shutdown Manager, the podsthat are part of a StatefulSet will be stuck in terminating status onthe shutdown node and cannot move to a new running node. This is because kubelet onthe shutdown node is not available to delete the pods so the StatefulSet cannotcreate a new pod with the same name. If there are volumes used by the pods, theVolumeAttachments will not be deleted from the original shutdown node so the volumesused by these pods cannot be attached to a new running node. As a result, theapplication running on the StatefulSet cannot function properly. If the originalshutdown node comes up, the pods will be deleted by kubelet and new pods will becreated on a different running node. If the original shutdown node does not come up,these pods will be stuck in terminating status on the shutdown node forever. To mitigate the above situation, a user can manually add the taint node.kubernetes.io/out-of-service with either NoExecuteor NoSchedule effect to a Node marking it out-of-service.If the NodeOutOfServiceVolumeDetachfeature gateis enabled on kube-controller-manager, and a Node is marked out-of-service with this taint, thepods on the node will be forcefully deleted if there are no matching tolerations on it and volumedetach operations for the pods terminating on the node will happen immediately. This allows thePods on the out-of-service node to recover quickly on a different node. During a non-graceful shutdown, Pods are terminated in the two phases: Force delete the Pods that do not have matching out-of-service tolerations.Immediately perform detach volume operation for such pods. Note:Before adding the taint node.kubernetes.io/out-of-service , it should be verifiedthat the node is already in shutdown or power off state (not in the middle ofrestarting).The user is required to manually remove the out-of-service taint after the pods aremoved to a new node and the user has checked that the shutdown node has beenrecovered since the user was the one who originally added the taint.",573
2.1 - Nodes,Swap memory management,"Swap memory management FEATURE STATE: Kubernetes v1.22 [alpha] Prior to Kubernetes 1.22, nodes did not support the use of swap memory, and akubelet would by default fail to start if swap was detected on a node. In 1.22onwards, swap memory support can be enabled on a per-node basis. To enable swap on a node, the NodeSwap feature gate must be enabled onthe kubelet, and the --fail-swap-on command line flag or failSwapOnconfiguration settingmust be set to false. Warning: When the memory swap feature is turned on, Kubernetes data such as the contentof Secret objects that were written to tmpfs now could be swapped to disk. A user can also optionally configure memorySwap.swapBehavior in order tospecify how a node will use swap memory. For example, memorySwap:  swapBehavior: LimitedSwap The available configuration options for swapBehavior are: LimitedSwap: Kubernetes workloads are limited in how much swap they canuse. Workloads on the node not managed by Kubernetes can still swap.UnlimitedSwap: Kubernetes workloads can use as much swap memory as theyrequest, up to the system limit. If configuration for memorySwap is not specified and the feature gate isenabled, by default the kubelet will apply the same behaviour as theLimitedSwap setting. The behaviour of the LimitedSwap setting depends if the node is running withv1 or v2 of control groups (also known as ""cgroups""): cgroupsv1: Kubernetes workloads can use any combination of memory andswap, up to the pod's memory limit, if set.cgroupsv2: Kubernetes workloads cannot use swap memory. For more information, and to assist with testing and provide feedback, pleasesee KEP-2400 and itsdesign proposal. Learn more about the following: Components that make up a node.API definition for Node.Node section of the architecture design document.Taints and Tolerations.Node Resource Managers.Resource Management for Windows nodes.",460
2.2 - Communication between Nodes and the Control Plane,default,This document catalogs the communication paths between the API server and the Kubernetes cluster.The intent is to allow users to customize their installation to harden the network configurationsuch that the cluster can be run on an untrusted network (or on fully public IPs on a cloudprovider).,61
2.2 - Communication between Nodes and the Control Plane,Node to Control Plane,"Node to Control Plane Kubernetes has a ""hub-and-spoke"" API pattern. All API usage from nodes (or the pods they run)terminates at the API server. None of the other control plane components are designed to exposeremote services. The API server is configured to listen for remote connections on a secure HTTPSport (typically 443) with one or more forms of clientauthentication enabled.One or more forms of authorization should beenabled, especially if anonymous requestsor service account tokensare allowed. Nodes should be provisioned with the public root certificate for the cluster such that they canconnect securely to the API server along with valid client credentials. A good approach is that theclient credentials provided to the kubelet are in the form of a client certificate. Seekubelet TLS bootstrappingfor automated provisioning of kubelet client certificates. Pods that wish to connect to the API server can do so securely by leveraging a service account sothat Kubernetes will automatically inject the public root certificate and a valid bearer tokeninto the pod when it is instantiated.The kubernetes service (in default namespace) is configured with a virtual IP address that isredirected (via kube-proxy) to the HTTPS endpoint on the API server. The control plane components also communicate with the API server over the secure port. As a result, the default operating mode for connections from the nodes and pods running on thenodes to the control plane is secured by default and can run over untrusted and/or publicnetworks.",318
2.2 - Communication between Nodes and the Control Plane,Control plane to node,"Control plane to node There are two primary communication paths from the control plane (the API server) to the nodes.The first is from the API server to the kubelet process which runs on each node in the cluster.The second is from the API server to any node, pod, or service through the API server's proxyfunctionality.",69
2.2 - Communication between Nodes and the Control Plane,API server to kubelet,"API server to kubelet The connections from the API server to the kubelet are used for: Fetching logs for pods.Attaching (usually through kubectl) to running pods.Providing the kubelet's port-forwarding functionality. These connections terminate at the kubelet's HTTPS endpoint. By default, the API server does notverify the kubelet's serving certificate, which makes the connection subject to man-in-the-middleattacks and unsafe to run over untrusted and/or public networks. To verify this connection, use the --kubelet-certificate-authority flag to provide the APIserver with a root certificate bundle to use to verify the kubelet's serving certificate. If that is not possible, use SSH tunneling between the API server and kubelet ifrequired to avoid connecting over anuntrusted or public network. Finally, Kubelet authentication and/or authorizationshould be enabled to secure the kubelet API.",208
2.2 - Communication between Nodes and the Control Plane,"API server to nodes, pods, and services","API server to nodes, pods, and services The connections from the API server to a node, pod, or service default to plain HTTP connectionsand are therefore neither authenticated nor encrypted. They can be run over a secure HTTPSconnection by prefixing https: to the node, pod, or service name in the API URL, but they willnot validate the certificate provided by the HTTPS endpoint nor provide client credentials. Sowhile the connection will be encrypted, it will not provide any guarantees of integrity. Theseconnections are not currently safe to run over untrusted or public networks.",117
2.2 - Communication between Nodes and the Control Plane,SSH tunnels,"SSH tunnels Kubernetes supports SSH tunnels to protect the control plane to nodes communication paths. In thisconfiguration, the API server initiates an SSH tunnel to each node in the cluster (connecting tothe SSH server listening on port 22) and passes all traffic destined for a kubelet, node, pod, orservice through the tunnel.This tunnel ensures that the traffic is not exposed outside of the network in which the nodes arerunning. Note: SSH tunnels are currently deprecated, so you shouldn't opt to use them unless you know what youare doing. The Konnectivity service is a replacement for thiscommunication channel.",129
2.2 - Communication between Nodes and the Control Plane,Konnectivity service,"Konnectivity service FEATURE STATE: Kubernetes v1.18 [beta] As a replacement to the SSH tunnels, the Konnectivity service provides TCP level proxy for thecontrol plane to cluster communication. The Konnectivity service consists of two parts: theKonnectivity server in the control plane network and the Konnectivity agents in the nodes network.The Konnectivity agents initiate connections to the Konnectivity server and maintain the networkconnections.After enabling the Konnectivity service, all control plane to nodes traffic goes through theseconnections. Follow the Konnectivity service task to setup the Konnectivity service in your cluster.",135
2.3 - Controllers,default,"In robotics and automation, a control loop isa non-terminating loop that regulates the state of a system. Here is one example of a control loop: a thermostat in a room. When you set the temperature, that's telling the thermostatabout your desired state. The actual room temperature is thecurrent state. The thermostat acts to bring the current statecloser to the desired state, by turning equipment on or off. cluster",92
2.3 - Controllers,Controller pattern,"Controller pattern A controller tracks at least one Kubernetes resource type.These objectshave a spec field that represents the desired state. Thecontroller(s) for that resource are responsible for making the currentstate come closer to that desired state. The controller might carry the action out itself; more commonly, in Kubernetes,a controller will send messages to theAPI server that haveuseful side effects. You'll see examples of this below.",92
2.3 - Controllers,Control via API server,"Control via API server The Job controller is an example of aKubernetes built-in controller. Built-in controllers manage state byinteracting with the cluster API server. Job is a Kubernetes resource that runs aPod, or perhaps several Pods, to carry outa task and then stop. (Once scheduled, Pod objects become part of thedesired state for a kubelet). When the Job controller sees a new task it makes sure that, somewherein your cluster, the kubelets on a set of Nodes are running the rightnumber of Pods to get the work done.The Job controller does not run any Pods or containersitself. Instead, the Job controller tells the API server to create or removePods.Other components in thecontrol planeact on the new information (there are new Pods to schedule and run),and eventually the work is done. After you create a new Job, the desired state is for that Job to be completed.The Job controller makes the current state for that Job be nearer to yourdesired state: creating Pods that do the work you wanted for that Job, so thatthe Job is closer to completion. Controllers also update the objects that configure them.For example: once the work is done for a Job, the Job controllerupdates that Job object to mark it Finished. (This is a bit like how some thermostats turn a light off toindicate that your room is now at the temperature you set).",306
2.3 - Controllers,Direct control,"Direct control In contrast with Job, some controllers need to make changes tothings outside of your cluster. For example, if you use a control loop to make sure thereare enough Nodesin your cluster, then that controller needs something outside thecurrent cluster to set up new Nodes when needed. Controllers that interact with external state find their desired state fromthe API server, then communicate directly with an external system to bringthe current state closer in line. (There actually is a controllerthat horizontally scales the nodes in your cluster.) The important point here is that the controller makes some changes to bring aboutyour desired state, and then reports the current state back to your cluster's API server.Other control loops can observe that reported data and take their own actions. In the thermostat example, if the room is very cold then a different controllermight also turn on a frost protection heater. With Kubernetes clusters, the controlplane indirectly works with IP address management tools, storage services,cloud provider APIs, and other services byextending Kubernetes to implement that.",217
2.3 - Controllers,Desired versus current state,"Desired versus current state Kubernetes takes a cloud-native view of systems, and is able to handleconstant change. Your cluster could be changing at any point as work happens andcontrol loops automatically fix failures. This means that,potentially, your cluster never reaches a stable state. As long as the controllers for your cluster are running and able to makeuseful changes, it doesn't matter if the overall state is stable or not.",91
2.3 - Controllers,Design,"Design As a tenet of its design, Kubernetes uses lots of controllers that each managea particular aspect of cluster state. Most commonly, a particular control loop(controller) uses one kind of resource as its desired state, and has a differentkind of resource that it manages to make that desired state happen. For example,a controller for Jobs tracks Job objects (to discover new work) and Pod objects(to run the Jobs, and then to see when the work is finished). In this casesomething else creates the Jobs, whereas the Job controller creates Pods. It's useful to have simple controllers rather than one, monolithic set of controlloops that are interlinked. Controllers can fail, so Kubernetes is designed toallow for that. Note:There can be several controllers that create or update the same kind of object.Behind the scenes, Kubernetes controllers make sure that they only pay attentionto the resources linked to their controlling resource.For example, you can have Deployments and Jobs; these both create Pods.The Job controller does not delete the Pods that your Deployment created,because there is information (labels)the controllers can use to tell those Pods apart.",249
2.3 - Controllers,Ways of running controllers,"Ways of running controllers Kubernetes comes with a set of built-in controllers that run insidethe kube-controller-manager. Thesebuilt-in controllers provide important core behaviors. The Deployment controller and Job controller are examples of controllers thatcome as part of Kubernetes itself (""built-in"" controllers).Kubernetes lets you run a resilient control plane, so that if any of the built-incontrollers were to fail, another part of the control plane will take over the work. You can find controllers that run outside the control plane, to extend Kubernetes.Or, if you want, you can write a new controller yourself.You can run your own controller as a set of Pods,or externally to Kubernetes. What fits best will depend on what that particularcontroller does. Read about the Kubernetes control planeDiscover some of the basic Kubernetes objectsLearn more about the Kubernetes APIIf you want to write your own controller, seeExtension Patternsin Extending Kubernetes.",221
2.4 - Leases,default,"Distributed systems often have a need for leases, which provide a mechanism to lock shared resourcesand coordinate activity between members of a set.In Kubernetes, the lease concept is represented by Leaseobjects in the coordination.k8s.io API Group,which are used for system-critical capabilities such as node heartbeats and component-level leader election.",75
2.4 - Leases,Node heartbeats,"Node heartbeats Kubernetes uses the Lease API to communicate kubelet node heartbeats to the Kubernetes API server.For every Node , there is a Lease object with a matching name in the kube-node-leasenamespace. Under the hood, every kubelet heartbeat is an update request to this Lease object, updatingthe spec.renewTime field for the Lease. The Kubernetes control plane uses the time stamp of this fieldto determine the availability of this Node. See Node Lease objects for more details.",121
2.4 - Leases,Leader election,"Leader election Kubernetes also uses Leases to ensure only one instance of a component is running at any given time.This is used by control plane components like kube-controller-manager and kube-scheduler inHA configurations, where only one instance of the component should be actively running while the otherinstances are on stand-by.",72
2.4 - Leases,API server identity,"API server identity FEATURE STATE: Kubernetes v1.26 [beta] Starting in Kubernetes v1.26, each kube-apiserver uses the Lease API to publish its identity to therest of the system. While not particularly useful on its own, this provides a mechanism for clients todiscover how many instances of kube-apiserver are operating the Kubernetes control plane.Existence of kube-apiserver leases enables future capabilities that may require coordination betweeneach kube-apiserver. You can inspect Leases owned by each kube-apiserver by checking for lease objects in the kube-system namespacewith the name kube-apiserver-<sha256-hash>. Alternatively you can use the label selector k8s.io/component=kube-apiserver: kubectl -n kube-system get lease -l k8s.io/component=kube-apiserver NAME                                        HOLDER                                                                           AGEkube-apiserver-c4vwjftbvpc5os2vvzle4qg27a   kube-apiserver-c4vwjftbvpc5os2vvzle4qg27a_9cbf54e5-1136-44bd-8f9a-1dcd15c346b4   5m33skube-apiserver-dz2dqprdpsgnm756t5rnov7yka   kube-apiserver-dz2dqprdpsgnm756t5rnov7yka_84f2a85d-37c1-4b14-b6b9-603e62e4896f   4m23skube-apiserver-fyloo45sdenffw2ugwaz3likua   kube-apiserver-fyloo45sdenffw2ugwaz3likua_c5ffa286-8a9a-45d4-91e7-61118ed58d2e   4m43s The SHA256 hash used in the lease name is based on the OS hostname as seen by that API server. Each kube-apiserver should beconfigured to use a hostname that is unique within the cluster. New instances of kube-apiserver that use the same hostnamewill take over existing Leases using a new holder identity, as opposed to instantiating new Lease objects. You can check thehostname used by kube-apisever by checking the value of the kubernetes.io/hostname label: kubectl -n kube-system get lease kube-apiserver-c4vwjftbvpc5os2vvzle4qg27a -o yaml apiVersion: coordination.k8s.io/v1kind: Leasemetadata:  creationTimestamp: ""2022-11-30T15:37:15Z""  labels:    k8s.io/component: kube-apiserver    kubernetes.io/hostname: kind-control-plane  name: kube-apiserver-c4vwjftbvpc5os2vvzle4qg27a  namespace: kube-system  resourceVersion: ""18171""  uid: d6c68901-4ec5-4385-b1ef-2d783738da6cspec:  holderIdentity: kube-apiserver-c4vwjftbvpc5os2vvzle4qg27a_9cbf54e5-1136-44bd-8f9a-1dcd15c346b4  leaseDurationSeconds: 3600  renewTime: ""2022-11-30T18:04:27.912073Z"" Expired leases from kube-apiservers that no longer exist are garbage collected by new kube-apiservers after 1 hour. You can disable API server identity leases by disabling the APIServerIdentityfeature gate.",912
2.4 - Leases,Workloads,"Workloads Your own workload can define its own use of Leases. For example, you might run a customcontroller where a primary or leader memberperforms operations that its peers do not. You define a Lease so that the controller replicas can selector elect a leader, using the Kubernetes API for coordination.If you do use a Lease, it's a good practice to define a name for the Lease that is obviously linked tothe product or component. For example, if you have a component named Example Foo, use a Lease namedexample-foo. If a cluster operator or another end user could deploy multiple instances of a component, select a nameprefix and pick a mechanism (such as hash of the name of the Deployment) to avoid name collisionsfor the Leases. You can use another approach so long as it achieves the same outcome: different software products donot conflict with one another.",186
2.5 - Cloud Controller Manager,default,"FEATURE STATE: Kubernetes v1.11 [beta] Cloud infrastructure technologies let you run Kubernetes on public, private, and hybrid clouds.Kubernetes believes in automated, API-driven infrastructure without tight coupling betweencomponents. The cloud-controller-manager is a Kubernetes control plane componentthat embeds cloud-specific control logic. The cloud controller manager lets you link yourcluster into your cloud provider's API, and separates out the components that interactwith that cloud platform from components that only interact with your cluster. By decoupling the interoperability logic between Kubernetes and the underlying cloudinfrastructure, the cloud-controller-manager component enables cloud providers to releasefeatures at a different pace compared to the main Kubernetes project. The cloud-controller-manager is structured using a pluginmechanism that allows different cloud providers to integrate their platforms with Kubernetes.",190
2.5 - Cloud Controller Manager,Design,"Design  The cloud controller manager runs in the control plane as a replicated set of processes(usually, these are containers in Pods). Each cloud-controller-manager implementsmultiple controllers in a singleprocess. Note: You can also run the cloud controller manager as a Kubernetesaddon rather than as partof the control plane.",67
2.5 - Cloud Controller Manager,Node controller,"Node controller The node controller is responsible for updating Node objectswhen new servers are created in your cloud infrastructure. The node controller obtains information about thehosts running inside your tenancy with the cloud provider. The node controller performs the following functions: Update a Node object with the corresponding server's unique identifier obtained from the cloud provider API.Annotating and labelling the Node object with cloud-specific information, such as the region the nodeis deployed into and the resources (CPU, memory, etc) that it has available.Obtain the node's hostname and network addresses.Verifying the node's health. In case a node becomes unresponsive, this controller checks withyour cloud provider's API to see if the server has been deactivated / deleted / terminated.If the node has been deleted from the cloud, the controller deletes the Node object from your Kubernetescluster. Some cloud provider implementations split this into a node controller and a separate nodelifecycle controller. Node controller The Node controller only works with Node objects. It requires full accessto read and modify Node objects. v1/Node: GetListCreateUpdatePatchWatchDelete",236
2.5 - Cloud Controller Manager,Route controller,"Route controller The route controller is responsible for configuring routes in the cloudappropriately so that containers on different nodes in your Kubernetescluster can communicate with each other. Depending on the cloud provider, the route controller might also allocate blocksof IP addresses for the Pod network. Route controller The route controller listens to Node object creation and configuresroutes appropriately. It requires Get access to Node objects. v1/Node: Get",89
2.5 - Cloud Controller Manager,Service controller,"Service controller Services integrate with cloudinfrastructure components such as managed load balancers, IP addresses, networkpacket filtering, and target health checking. The service controller interacts with yourcloud provider's APIs to set up load balancers and other infrastructure componentswhen you declare a Service resource that requires them. Service controller The service controller listens to Service object Create, Update and Delete events and then configures Endpoints for those Services appropriately (for EndpointSlices, the kube-controller-manager manages these on demand). To access Services, it requires List, and Watch access. To update Services, it requires Patch and Update access. To set up Endpoints resources for the Services, it requires access to Create, List, Get, Watch, and Update. v1/Service: ListGetWatchPatchUpdate",162
2.5 - Cloud Controller Manager,Others,"Others The implementation of the core of the cloud controller manager requires access to create Event objects, and to ensure secure operation, it requires access to create ServiceAccounts. v1/Event: CreatePatchUpdate v1/ServiceAccount: Create The RBAC ClusterRole for the cloudcontroller manager looks like: apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: cloud-controller-managerrules:- apiGroups:  - """"  resources:  - events  verbs:  - create  - patch  - update- apiGroups:  - """"  resources:  - nodes  verbs:  - '*'- apiGroups:  - """"  resources:  - nodes/status  verbs:  - patch- apiGroups:  - """"  resources:  - services  verbs:  - list  - patch  - update  - watch- apiGroups:  - """"  resources:  - serviceaccounts  verbs:  - create- apiGroups:  - """"  resources:  - persistentvolumes  verbs:  - get  - list  - update  - watch- apiGroups:  - """"  resources:  - endpoints  verbs:  - create  - get  - list  - watch  - update Cloud Controller Manager Administrationhas instructions on running and managing the cloud controller manager. To upgrade a HA control plane to use the cloud controller manager, see Migrate Replicated Control Plane To Use Cloud Controller Manager. Want to know how to implement your own cloud controller manager, or extend an existing project? The cloud controller manager uses Go interfaces to allow implementations from any cloud to be plugged in. Specifically, it uses the CloudProvider interface defined in cloud.go from kubernetes/cloud-provider. The implementation of the shared controllers highlighted in this document (Node, Route, and Service), and some scaffolding along with the shared cloudprovider interface, is part of the Kubernetes core. Implementations specific to cloud providers are outside the core of Kubernetes and implement the CloudProvider interface. For more information about developing plugins, see Developing Cloud Controller Manager.",456
2.6 - About cgroup v2,default,"On Linux, control groupsconstrain resources that are allocated to processes. The kubelet and theunderlying container runtime need to interface with cgroups to enforceresource management for pods and containers whichincludes cpu/memory requests and limits for containerized workloads. There are two versions of cgroups in Linux: cgroup v1 and cgroup v2. cgroup v2 isthe new generation of the cgroup API.",87
2.6 - About cgroup v2,What is cgroup v2?,"What is cgroup v2? FEATURE STATE: Kubernetes v1.25 [stable] cgroup v2 is the next version of the Linux cgroup API. cgroup v2 provides aunified control system with enhanced resource managementcapabilities. cgroup v2 offers several improvements over cgroup v1, such as the following: Single unified hierarchy design in APISafer sub-tree delegation to containersNewer features like Pressure Stall InformationEnhanced resource allocation management and isolation across multiple resourcesUnified accounting for different types of memory allocations (network memory, kernel memory, etc)Accounting for non-immediate resource changes such as page cache write backs Some Kubernetes features exclusively use cgroup v2 for enhanced resourcemanagement and isolation. For example, theMemoryQoS feature improves memory QoSand relies on cgroup v2 primitives.",176
2.6 - About cgroup v2,Using cgroup v2,"Using cgroup v2 The recommended way to use cgroup v2 is to use a Linux distribution thatenables and uses cgroup v2 by default. To check if your distribution uses cgroup v2, refer to Identify cgroup version on Linux nodes.",54
2.6 - About cgroup v2,Requirements,Requirements cgroup v2 has the following requirements: OS distribution enables cgroup v2Linux Kernel version is 5.8 or laterContainer runtime supports cgroup v2. For example:containerd v1.4 and latercri-o v1.20 and laterThe kubelet and the container runtime are configured to use the systemd cgroup driver,72
2.6 - About cgroup v2,Linux Distribution cgroup v2 support,"Linux Distribution cgroup v2 support For a list of Linux distributions that use cgroup v2, refer to the cgroup v2 documentation Container Optimized OS (since M97)Ubuntu (since 21.10, 22.04+ recommended)Debian GNU/Linux (since Debian 11 bullseye)Fedora (since 31)Arch Linux (since April 2021)RHEL and RHEL-like distributions (since 9) To check if your distribution is using cgroup v2, refer to your distribution'sdocumentation or follow the instructions in Identify the cgroup version on Linux nodes. You can also enable cgroup v2 manually on your Linux distribution by modifyingthe kernel cmdline boot arguments. If your distribution uses GRUB,systemd.unified_cgroup_hierarchy=1 should be added in GRUB_CMDLINE_LINUXunder /etc/default/grub, followed by sudo update-grub. However, therecommended approach is to use a distribution that already enables cgroup v2 bydefault.",217
2.6 - About cgroup v2,Migrating to cgroup v2,"Migrating to cgroup v2 To migrate to cgroup v2, ensure that you meet the requirements, then upgradeto a kernel version that enables cgroup v2 by default. The kubelet automatically detects that the OS is running on cgroup v2 andperforms accordingly with no additional configuration required. There should not be any noticeable difference in the user experience whenswitching to cgroup v2, unless users are accessing the cgroup file systemdirectly, either on the node or from within the containers. cgroup v2 uses a different API than cgroup v1, so if there are anyapplications that directly access the cgroup file system, they need to beupdated to newer versions that support cgroup v2. For example: Some third-party monitoring and security agents may depend on the cgroup filesystem.Update these agents to versions that support cgroup v2.If you run cAdvisor as a stand-aloneDaemonSet for monitoring pods and containers, update it to v0.43.0 or later.If you use JDK, prefer to use JDK 11.0.16 and later or JDK 15 and later, which fully support cgroup v2.",249
2.6 - About cgroup v2,Identify the cgroup version on Linux Nodes,"Identify the cgroup version on Linux Nodes The cgroup version depends on the Linux distribution being used and thedefault cgroup version configured on the OS. To check which cgroup version yourdistribution uses, run the stat -fc %T /sys/fs/cgroup/ command onthe node: stat -fc %T /sys/fs/cgroup/ For cgroup v2, the output is cgroup2fs. For cgroup v1, the output is tmpfs. Learn more about cgroupsLearn more about container runtimeLearn more about cgroup drivers",118
2.7 - Container Runtime Interface (CRI),default,"The CRI is a plugin interface which enables the kubelet to use a wide variety ofcontainer runtimes, without having a need to recompile the cluster components. You need a workingcontainer runtime oneach Node in your cluster, so that thekubelet can launchPods and their containers. The Container Runtime Interface (CRI) is the main protocol for the communication between the kubelet and Container Runtime. The Kubernetes Container Runtime Interface (CRI) defines the maingRPC protocol for the communication between thecluster componentskubelet andcontainer runtime.",119
2.7 - Container Runtime Interface (CRI),The API,"The API FEATURE STATE: Kubernetes v1.23 [stable] The kubelet acts as a client when connecting to the container runtime via gRPC.The runtime and image service endpoints have to be available in the containerruntime, which can be configured separately within the kubelet by using the--image-service-endpoint and --container-runtime-endpoint command lineflags For Kubernetes v1.26, the kubelet prefers to use CRI v1.If a container runtime does not support v1 of the CRI, then the kubelet tries tonegotiate any older supported version.The v1.26 kubelet can also negotiate CRI v1alpha2, butthis version is considered as deprecated.If the kubelet cannot negotiate a supported CRI version, the kubelet gives upand doesn't register as a node.",187
2.7 - Container Runtime Interface (CRI),Upgrading,"Upgrading When upgrading Kubernetes, the kubelet tries to automatically select thelatest CRI version on restart of the component. If that fails, then the fallbackwill take place as mentioned above. If a gRPC re-dial was required because thecontainer runtime has been upgraded, then the container runtime must alsosupport the initially selected version or the redial is expected to fail. Thisrequires a restart of the kubelet. Learn more about the CRI protocol definition",102
2.8 - Garbage Collection,default,Garbage collection is a collective term for the various mechanisms Kubernetes uses to clean upcluster resources. Thisallows the clean up of resources like the following: Terminated podsCompleted JobsObjects without owner referencesUnused containers and container imagesDynamically provisioned PersistentVolumes with a StorageClass reclaim policy of DeleteStale or expired CertificateSigningRequests (CSRs)Nodes deleted in the following scenarios:On a cloud when the cluster uses a cloud controller managerOn-premises when the cluster uses an addon similar to a cloud controllermanagerNode Lease objects,121
2.8 - Garbage Collection,Owners and dependents,"Owners and dependents Many objects in Kubernetes link to each other through owner references.Owner references tell the control plane which objects are dependent on others.Kubernetes uses owner references to give the control plane, and other APIclients, the opportunity to clean up related resources before deleting anobject. In most cases, Kubernetes manages owner references automatically. Ownership is different from the labels and selectorsmechanism that some resources also use. For example, consider aService that createsEndpointSlice objects. The Service uses labels to allow the control plane todetermine which EndpointSlice objects are used for that Service. In additionto the labels, each EndpointSlice that is managed on behalf of a Service hasan owner reference. Owner references help different parts of Kubernetes avoidinterfering with objects they don’t control. Note:Cross-namespace owner references are disallowed by design.Namespaced dependents can specify cluster-scoped or namespaced owners.A namespaced owner must exist in the same namespace as the dependent.If it does not, the owner reference is treated as absent, and the dependentis subject to deletion once all owners are verified absent.Cluster-scoped dependents can only specify cluster-scoped owners.In v1.20+, if a cluster-scoped dependent specifies a namespaced kind as an owner,it is treated as having an unresolvable owner reference, and is not able to be garbage collected.In v1.20+, if the garbage collector detects an invalid cross-namespace ownerReference,or a cluster-scoped dependent with an ownerReference referencing a namespaced kind, a warning Eventwith a reason of OwnerRefInvalidNamespace and an involvedObject of the invalid dependent is reported.You can check for that kind of Event by runningkubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace.",403
2.8 - Garbage Collection,Cascading deletion,"Cascading deletion Kubernetes checks for and deletes objects that no longer have ownerreferences, like the pods left behind when you delete a ReplicaSet. When youdelete an object, you can control whether Kubernetes deletes the object'sdependents automatically, in a process called cascading deletion. There aretwo types of cascading deletion, as follows: Foreground cascading deletionBackground cascading deletion You can also control how and when garbage collection deletes resources that haveowner references using Kubernetes finalizers.",111
2.8 - Garbage Collection,Foreground cascading deletion,"Foreground cascading deletion In foreground cascading deletion, the owner object you're deleting first entersa deletion in progress state. In this state, the following happens to theowner object: The Kubernetes API server sets the object's metadata.deletionTimestampfield to the time the object was marked for deletion.The Kubernetes API server also sets the metadata.finalizers field toforegroundDeletion.The object remains visible through the Kubernetes API until the deletionprocess is complete. After the owner object enters the deletion in progress state, the controllerdeletes the dependents. After deleting all the dependent objects, the controllerdeletes the owner object. At this point, the object is no longer visible in theKubernetes API. During foreground cascading deletion, the only dependents that block ownerdeletion are those that have the ownerReference.blockOwnerDeletion=true field.See Use foreground cascading deletionto learn more.",201
2.8 - Garbage Collection,Background cascading deletion,"Background cascading deletion In background cascading deletion, the Kubernetes API server deletes the ownerobject immediately and the controller cleans up the dependent objects inthe background. By default, Kubernetes uses background cascading deletion unlessyou manually use foreground deletion or choose to orphan the dependent objects. See Use background cascading deletionto learn more.",71
2.8 - Garbage Collection,Orphaned dependents,"Orphaned dependents When Kubernetes deletes an owner object, the dependents left behind are calledorphan objects. By default, Kubernetes deletes dependent objects. To learn howto override this behaviour, see Delete owner objects and orphan dependents.",56
2.8 - Garbage Collection,Garbage collection of unused containers and images,"Garbage collection of unused containers and images The kubelet performs garbagecollection on unused images every five minutes and on unused containers everyminute. You should avoid using external garbage collection tools, as these canbreak the kubelet behavior and remove containers that should exist. To configure options for unused container and image garbage collection, tune thekubelet using a configuration fileand change the parameters related to garbage collection using theKubeletConfigurationresource type.",90
2.8 - Garbage Collection,Container image lifecycle,"Container image lifecycle Kubernetes manages the lifecycle of all images through its image manager,which is part of the kubelet, with the cooperation ofcadvisor. The kubeletconsiders the following disk usage limits when making garbage collectiondecisions: HighThresholdPercentLowThresholdPercent Disk usage above the configured HighThresholdPercent value triggers garbagecollection, which deletes images in order based on the last time they were used,starting with the oldest first. The kubelet deletes imagesuntil disk usage reaches the LowThresholdPercent value.",116
2.8 - Garbage Collection,Container garbage collection,"Container garbage collection The kubelet garbage collects unused containers based on the following variables,which you can define: MinAge: the minimum age at which the kubelet can garbage collect acontainer. Disable by setting to 0.MaxPerPodContainer: the maximum number of dead containers each Podcan have. Disable by setting to less than 0.MaxContainers: the maximum number of dead containers the cluster can have.Disable by setting to less than 0. In addition to these variables, the kubelet garbage collects unidentified anddeleted containers, typically starting with the oldest first. MaxPerPodContainer and MaxContainers may potentially conflict with each otherin situations where retaining the maximum number of containers per Pod(MaxPerPodContainer) would go outside the allowable total of global deadcontainers (MaxContainers). In this situation, the kubelet adjustsMaxPerPodContainer to address the conflict. A worst-case scenario would be todowngrade MaxPerPodContainer to 1 and evict the oldest containers.Additionally, containers owned by pods that have been deleted are removed oncethey are older than MinAge. Note: The kubelet only garbage collects the containers it manages.",242
2.8 - Garbage Collection,Configuring garbage collection,Configuring garbage collection You can tune garbage collection of resources by configuring options specific tothe controllers managing those resources. The following pages show you how toconfigure garbage collection: Configuring cascading deletion of Kubernetes objectsConfiguring cleanup of finished Jobs Learn more about ownership of Kubernetes objects.Learn more about Kubernetes finalizers.Learn about the TTL controller (beta) that cleans up finished Jobs.,87
3 - Containers,default,Technology for packaging an application along with its runtime dependencies. Each container that you run is repeatable; the standardization from havingdependencies included means that you get the same behavior wherever yourun it. Containers decouple applications from underlying host infrastructure.This makes deployment easier in different cloud or OS environments. Each node in a Kubernetescluster runs the containers that form thePods assigned to that node.Containers in a Pod are co-located and co-scheduled to run on the same node.,107
3 - Containers,Container images,"Container images A container image is a ready-to-runsoftware package, containing everything needed to run an application:the code and any runtime it requires, application and system libraries,and default values for any essential settings. Containers are intended to be stateless andimmutable:you should not changethe code of a container that is already running. If you have a containerizedapplication and want to make changes, the correct process is to build a newimage that includes the change, then recreate the container to start from theupdated image.",108
3 - Containers,Container runtimes,"Container runtimes The container runtime is the software that is responsible for running containers. Kubernetes supports container runtimes such ascontainerd, CRI-O,and any other implementation of the Kubernetes CRI (Container RuntimeInterface). Usually, you can allow your cluster to pick the default container runtimefor a Pod. If you need to use more than one container runtime in your cluster,you can specify the RuntimeClassfor a Pod to make sure that Kubernetes runs those containers using aparticular container runtime. You can also use RuntimeClass to run different Pods with the same containerruntime but with different settings.",129
3.1 - Images,default,"A container image represents binary data that encapsulates an application and all itssoftware dependencies. Container images are executable software bundles that can runstandalone and that make very well defined assumptions about their runtime environment. You typically create a container image of your application and push it to a registrybefore referring to it in a Pod. This page provides an outline of the container image concept. Note: If you are looking for the container images for a Kubernetesrelease (such as v1.26, the latest minor release),visit Download Kubernetes.",112
3.1 - Images,Image names,"Image names Container images are usually given a name such as pause, example/mycontainer, or kube-apiserver.Images can also include a registry hostname; for example: fictional.registry.example/imagename,and possibly a port number as well; for example: fictional.registry.example:10443/imagename. If you don't specify a registry hostname, Kubernetes assumes that you mean the Docker public registry. After the image name part you can add a tag (in the same way you would when using with commandslike docker or podman). Tags let you identify different versions of the same series of images. Image tags consist of lowercase and uppercase letters, digits, underscores (_),periods (.), and dashes (-).There are additional rules about where you can place the separatorcharacters (_, -, and .) inside an image tag.If you don't specify a tag, Kubernetes assumes you mean the tag latest.",207
3.1 - Images,Updating images,"Updating images When you first create a Deployment,StatefulSet, Pod, or otherobject that includes a Pod template, then by default the pull policy of allcontainers in that pod will be set to IfNotPresent if it is not explicitlyspecified. This policy causes thekubelet to skip pulling animage if it already exists.",72
3.1 - Images,Image pull policy,"Image pull policy The imagePullPolicy for a container and the tag of the image affect when thekubelet attempts to pull (download) the specified image. Here's a list of the values you can set for imagePullPolicy and the effectsthese values have: IfNotPresentthe image is pulled only if it is not already present locally.Alwaysevery time the kubelet launches a container, the kubelet queries the containerimage registry to resolve the name to an imagedigest.If the kubelet has a container image with that exact digest cached locally, the kubelet uses itscached image; otherwise, the kubelet pulls the image with the resolved digest, and uses that imageto launch the container.Neverthe kubelet does not try fetching the image. If the image is somehow already presentlocally, the kubelet attempts to start the container; otherwise, startup fails.See pre-pulled images for more details. The caching semantics of the underlying image provider make evenimagePullPolicy: Always efficient, as long as the registry is reliably accessible.Your container runtime can notice that the image layers already exist on the nodeso that they don't need to be downloaded again. Note:You should avoid using the :latest tag when deploying containers in production asit is harder to track which version of the image is running and more difficult toroll back properly.Instead, specify a meaningful tag such as v1.42.0. To make sure the Pod always uses the same version of a container image, you can specifythe image's digest;replace <image-name>:<tag> with <image-name>@<digest>(for example, image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2). When using image tags, if the image registry were to change the code that the tag on that imagerepresents, you might end up with a mix of Pods running the old and new code. An image digestuniquely identifies a specific version of the image, so Kubernetes runs the same code every timeit starts a container with that image name and digest specified. Specifying an image by digestfixes the code that you run so that a change at the registry cannot lead to that mix of versions. There are third-party admission controllersthat mutate Pods (and pod templates) when they are created, so that therunning workload is defined based on an image digest rather than a tag.That might be useful if you want to make sure that all your workload isrunning the same code no matter what tag changes happen at the registry.",566
3.1 - Images,Default image pull policy,"Default image pull policy When you (or a controller) submit a new Pod to the API server, your cluster sets theimagePullPolicy field when specific conditions are met: if you omit the imagePullPolicy field, and the tag for the container image is:latest, imagePullPolicy is automatically set to Always;if you omit the imagePullPolicy field, and you don't specify the tag for thecontainer image, imagePullPolicy is automatically set to Always;if you omit the imagePullPolicy field, and you specify the tag for thecontainer image that isn't :latest, the imagePullPolicy is automatically set toIfNotPresent. Note:The value of imagePullPolicy of the container is always set when the object isfirst created, and is not updated if the image's tag later changes.For example, if you create a Deployment with an image whose tag is not:latest, and later update that Deployment's image to a :latest tag, theimagePullPolicy field will not change to Always. You must manually changethe pull policy of any object after its initial creation.",221
3.1 - Images,Required image pull,"Required image pull If you would like to always force a pull, you can do one of the following: Set the imagePullPolicy of the container to Always.Omit the imagePullPolicy and use :latest as the tag for the image to use;Kubernetes will set the policy to Always when you submit the Pod.Omit the imagePullPolicy and the tag for the image to use;Kubernetes will set the policy to Always when you submit the Pod.Enable the AlwaysPullImagesadmission controller.",107
3.1 - Images,ImagePullBackOff,"ImagePullBackOff When a kubelet starts creating containers for a Pod using a container runtime,it might be possible the container is in Waitingstate because of ImagePullBackOff. The status ImagePullBackOff means that a container could not start because Kubernetescould not pull a container image (for reasons such as invalid image name, or pullingfrom a private registry without imagePullSecret). The BackOff part indicatesthat Kubernetes will keep trying to pull the image, with an increasing back-off delay. Kubernetes raises the delay between each attempt until it reaches a compiled-in limit,which is 300 seconds (5 minutes).",136
3.1 - Images,Multi-architecture images with image indexes,"Multi-architecture images with image indexes As well as providing binary images, a container registry can also serve acontainer image index.An image index can point to multiple image manifestsfor architecture-specific versions of a container. The idea is that you can have a name for an image(for example: pause, example/mycontainer, kube-apiserver) and allow different systems tofetch the right binary image for the machine architecture they are using. Kubernetes itself typically names container images with a suffix -$(ARCH). For backwardcompatibility, please generate the older images with suffixes. The idea is to generate say pauseimage which has the manifest for all the arch(es) and say pause-amd64 which is backwardscompatible for older configurations or YAML files which may have hard coded the images withsuffixes.",175
3.1 - Images,Using a private registry,"Using a private registry Private registries may require keys to read images from them.Credentials can be provided in several ways: Configuring Nodes to Authenticate to a Private Registryall pods can read any configured private registriesrequires node configuration by cluster administratorKubelet Credential Provider to dynamically fetch credentials for private registrieskubelet can be configured to use credential provider exec pluginfor the respective private registry.Pre-pulled Imagesall pods can use any images cached on a noderequires root access to all nodes to set upSpecifying ImagePullSecrets on a Podonly pods which provide own keys can access the private registryVendor-specific or local extensionsif you're using a custom node configuration, you (or your cloudprovider) can implement your mechanism for authenticating the nodeto the container registry. These options are explained in more detail below.",178
3.1 - Images,Configuring nodes to authenticate to a private registry,"Configuring nodes to authenticate to a private registry Specific instructions for setting credentials depends on the container runtime and registry youchose to use. You should refer to your solution's documentation for the most accurate information. For an example of configuring a private container image registry, see thePull an Image from a Private Registrytask. That example uses a private registry in Docker Hub.",75
3.1 - Images,Kubelet credential provider for authenticated image pulls,"Kubelet credential provider for authenticated image pulls Note: This approach is especially suitable when kubelet needs to fetch registry credentials dynamically.Most commonly used for registries provided by cloud providers where auth tokens are short-lived. You can configure the kubelet to invoke a plugin binary to dynamically fetch registry credentials for a container image.This is the most robust and versatile way to fetch credentials for private registries, but also requires kubelet-level configuration to enable. See Configure a kubelet image credential provider for more details.",109
3.1 - Images,Interpretation of config.json,"Interpretation of config.json The interpretation of config.json varies between the original Dockerimplementation and the Kubernetes interpretation. In Docker, the auths keyscan only specify root URLs, whereas Kubernetes allows glob URLs as well asprefix-matched paths. This means that a config.json like this is valid: {    ""auths"": {        ""*my-registry.io/images"": {            ""auth"": ""…""        }    }} The root URL (*my-registry.io) is matched by using the following syntax: pattern:    { term }term:    '*'         matches any sequence of non-Separator characters    '?'         matches any single non-Separator character    '[' [ '^' ] { character-range } ']'                character class (must be non-empty)    c           matches character c (c != '*', '?', '\\', '[')    '\\' c      matches character ccharacter-range:    c           matches character c (c != '\\', '-', ']')    '\\' c      matches character c    lo '-' hi   matches character c for lo <= c <= hi Image pull operations would now pass the credentials to the CRI containerruntime for every valid pattern. For example the following container image nameswould match successfully: my-registry.io/imagesmy-registry.io/images/my-imagemy-registry.io/images/another-imagesub.my-registry.io/images/my-imagea.sub.my-registry.io/images/my-image The kubelet performs image pulls sequentially for every found credential. Thismeans, that multiple entries in config.json are possible, too: {    ""auths"": {        ""my-registry.io/images"": {            ""auth"": ""…""        },        ""my-registry.io/images/subpath"": {            ""auth"": ""…""        }    }} If now a container specifies an image my-registry.io/images/subpath/my-imageto be pulled, then the kubelet will try to download them from bothauthentication sources if one of them fails.",475
3.1 - Images,Pre-pulled images,"Pre-pulled images Note: This approach is suitable if you can control node configuration. Itwill not work reliably if your cloud provider manages nodes and replacesthem automatically. By default, the kubelet tries to pull each image from the specified registry.However, if the imagePullPolicy property of the container is set to IfNotPresent or Never,then a local image is used (preferentially or exclusively, respectively). If you want to rely on pre-pulled images as a substitute for registry authentication,you must ensure all nodes in the cluster have the same pre-pulled images. This can be used to preload certain images for speed or as an alternative to authenticating to aprivate registry. All pods will have read access to any pre-pulled images.",162
3.1 - Images,Specifying imagePullSecrets on a Pod,Specifying imagePullSecrets on a Pod Note: This is the recommended approach to run containers based on imagesin private registries. Kubernetes supports specifying container image registry keys on a Pod.imagePullSecrets must all be in the same namespace as the Pod. The referencedSecrets must be of type kubernetes.io/dockercfg or kubernetes.io/dockerconfigjson.,87
3.1 - Images,Creating a Secret with a Docker config,"Creating a Secret with a Docker config You need to know the username, registry password and client email address for authenticatingto the registry, as well as its hostname.Run the following command, substituting the appropriate uppercase values: kubectl create secret docker-registry <name> \  --docker-server=DOCKER_REGISTRY_SERVER \  --docker-username=DOCKER_USER \  --docker-password=DOCKER_PASSWORD \  --docker-email=DOCKER_EMAIL If you already have a Docker credentials file then, rather than using the abovecommand, you can import the credentials file as a KubernetesSecrets.Create a Secret based on existing Docker credentialsexplains how to set this up. This is particularly useful if you are using multiple private containerregistries, as kubectl create secret docker-registry creates a Secret thatonly works with a single private registry. Note: Pods can only reference image pull secrets in their own namespace,so this process needs to be done one time per namespace.",230
3.1 - Images,Referring to an imagePullSecrets on a Pod,"Referring to an imagePullSecrets on a Pod Now, you can create pods which reference that secret by adding an imagePullSecretssection to a Pod definition. Each item in the imagePullSecrets array can onlyreference a Secret in the same namespace. For example: cat <<EOF > pod.yamlapiVersion: v1kind: Podmetadata:  name: foo  namespace: awesomeappsspec:  containers:    - name: foo      image: janedoe/awesomeapp:v1  imagePullSecrets:    - name: myregistrykeyEOFcat <<EOF >> ./kustomization.yamlresources:- pod.yamlEOF This needs to be done for each pod that is using a private registry. However, setting of this field can be automated by setting the imagePullSecretsin a ServiceAccount resource. Check Add ImagePullSecrets to a Service Accountfor detailed instructions. You can use this in conjunction with a per-node .docker/config.json. The credentialswill be merged.",220
3.1 - Images,Use cases,"Use cases There are a number of solutions for configuring private registries. Here are somecommon use cases and suggested solutions. Cluster running only non-proprietary (e.g. open-source) images. No need to hide images.Use public images from a public registryNo configuration required.Some cloud providers automatically cache or mirror public images, which improvesavailability and reduces the time to pull images.Cluster running some proprietary images which should be hidden to those outside the company, butvisible to all cluster users.Use a hosted private registryManual configuration may be required on the nodes that need to access to private registryOr, run an internal private registry behind your firewall with open read access.No Kubernetes configuration is required.Use a hosted container image registry service that controls image accessIt will work better with cluster autoscaling than manual node configuration.Or, on a cluster where changing the node configuration is inconvenient, use imagePullSecrets.Cluster with proprietary images, a few of which require stricter access control.Ensure AlwaysPullImages admission controlleris active. Otherwise, all Pods potentially have access to all images.Move sensitive data into a ""Secret"" resource, instead of packaging it in an image.A multi-tenant cluster where each tenant needs own private registry.Ensure AlwaysPullImages admission controlleris active. Otherwise, all Pods of all tenants potentially have access to all images.Run a private registry with authorization required.Generate registry credential for each tenant, put into secret, and populate secret to eachtenant namespace.The tenant adds that secret to imagePullSecrets of each namespace. If you need access to multiple registries, you can create one secret for each registry. Read the OCI Image Manifest Specification.Learn about container image garbage collection.Learn more about pulling an Image from a Private Registry.",376
3.2 - Container Environment,Container environment,"Container environment The Kubernetes Container environment provides several important resources to Containers: A filesystem, which is a combination of an image and one or more volumes.Information about the Container itself.Information about other objects in the cluster.",47
3.2 - Container Environment,Container information,"Container information The hostname of a Container is the name of the Pod in which the Container is running.It is available through the hostname command or thegethostnamefunction call in libc. The Pod name and namespace are available as environment variables through thedownward API. User defined environment variables from the Pod definition are also available to the Container,as are any environment variables specified statically in the container image.",83
3.2 - Container Environment,Cluster information,"Cluster information A list of all services that were running when a Container was created is available to that Container as environment variables.This list is limited to services within the same namespace as the new Container's Pod and Kubernetes control plane services. For a service named foo that maps to a Container named bar,the following variables are defined: FOO_SERVICE_HOST=<the host the service is running on>FOO_SERVICE_PORT=<the port the service is running on> Services have dedicated IP addresses and are available to the Container via DNS,if DNS addon is enabled. Learn more about Container lifecycle hooks.Get hands-on experienceattaching handlers to Container lifecycle events.",146
3.3 - Runtime Class,default,FEATURE STATE: Kubernetes v1.20 [stable] This page describes the RuntimeClass resource and runtime selection mechanism. RuntimeClass is a feature for selecting the container runtime configuration. The container runtimeconfiguration is used to run a Pod's containers.,53
3.3 - Runtime Class,Motivation,"Motivation You can set a different RuntimeClass between different Pods to provide a balance ofperformance versus security. For example, if part of your workload deserves a highlevel of information security assurance, you might choose to schedule those Pods sothat they run in a container runtime that uses hardware virtualization. You'd thenbenefit from the extra isolation of the alternative runtime, at the expense of someadditional overhead. You can also use RuntimeClass to run different Pods with the same container runtimebut with different settings.",105
3.3 - Runtime Class,1. Configure the CRI implementation on nodes,"1. Configure the CRI implementation on nodes The configurations available through RuntimeClass are Container Runtime Interface (CRI)implementation dependent. See the corresponding documentation (below) for yourCRI implementation for how to configure. Note: RuntimeClass assumes a homogeneous node configuration across the cluster by default (which meansthat all nodes are configured the same way with respect to container runtimes). To supportheterogeneous node configurations, see Scheduling below. The configurations have a corresponding handler name, referenced by the RuntimeClass. Thehandler must be a valid DNS label name.",115
3.3 - Runtime Class,2. Create the corresponding RuntimeClass resources,"2. Create the corresponding RuntimeClass resources The configurations setup in step 1 should each have an associated handler name, which identifiesthe configuration. For each handler, create a corresponding RuntimeClass object. The RuntimeClass resource currently only has 2 significant fields: the RuntimeClass name(metadata.name) and the handler (handler). The object definition looks like this: # RuntimeClass is defined in the node.k8s.io API groupapiVersion: node.k8s.io/v1kind: RuntimeClassmetadata:  # The name the RuntimeClass will be referenced by.  # RuntimeClass is a non-namespaced resource.  name: myclass # The name of the corresponding CRI configurationhandler: myconfiguration The name of a RuntimeClass object must be a validDNS subdomain name. Note: It is recommended that RuntimeClass write operations (create/update/patch/delete) berestricted to the cluster administrator. This is typically the default. SeeAuthorization Overview for more details.",208
3.3 - Runtime Class,Usage,"Usage Once RuntimeClasses are configured for the cluster, you can specify aruntimeClassName in the Pod spec to use it. For example: apiVersion: v1kind: Podmetadata:  name: mypodspec:  runtimeClassName: myclass  # ... This will instruct the kubelet to use the named RuntimeClass to run this pod. If the namedRuntimeClass does not exist, or the CRI cannot run the corresponding handler, the pod will enter theFailed terminal phase. Look for acorresponding event for anerror message. If no runtimeClassName is specified, the default RuntimeHandler will be used, which is equivalentto the behavior when the RuntimeClass feature is disabled.",146
3.3 - Runtime Class,containerd,"containerd Runtime handlers are configured through containerd's configuration at/etc/containerd/config.toml. Valid handlers are configured under the runtimes section: [plugins.""io.containerd.grpc.v1.cri"".containerd.runtimes.${HANDLER_NAME}] See containerd's config documentationfor more details:",75
3.3 - Runtime Class,CRI-O,"CRI-O Runtime handlers are configured through CRI-O's configuration at /etc/crio/crio.conf. Validhandlers are configured under thecrio.runtime table: [crio.runtime.runtimes.${HANDLER_NAME}]  runtime_path = ""${PATH_TO_BINARY}"" See CRI-O's config documentation for more details.",85
3.3 - Runtime Class,Scheduling,"Scheduling FEATURE STATE: Kubernetes v1.16 [beta] By specifying the scheduling field for a RuntimeClass, you can set constraints toensure that Pods running with this RuntimeClass are scheduled to nodes that support it.If scheduling is not set, this RuntimeClass is assumed to be supported by all nodes. To ensure pods land on nodes supporting a specific RuntimeClass, that set of nodes should have acommon label which is then selected by the runtimeclass.scheduling.nodeSelector field. TheRuntimeClass's nodeSelector is merged with the pod's nodeSelector in admission, effectively takingthe intersection of the set of nodes selected by each. If there is a conflict, the pod will berejected. If the supported nodes are tainted to prevent other RuntimeClass pods from running on the node, youcan add tolerations to the RuntimeClass. As with the nodeSelector, the tolerations are mergedwith the pod's tolerations in admission, effectively taking the union of the set of nodes toleratedby each. To learn more about configuring the node selector and tolerations, seeAssigning Pods to Nodes.",236
3.3 - Runtime Class,Pod Overhead,"Pod Overhead FEATURE STATE: Kubernetes v1.24 [stable] You can specify overhead resources that are associated with running a Pod. Declaring overhead allowsthe cluster (including the scheduler) to account for it when making decisions about Pods and resources. Pod overhead is defined in RuntimeClass through the overhead field. Through the use of this field,you can specify the overhead of running pods utilizing this RuntimeClass and ensure these overheadsare accounted for in Kubernetes. RuntimeClass DesignRuntimeClass Scheduling DesignRead about the Pod Overhead conceptPodOverhead Feature Design",122
3.4 - Container Lifecycle Hooks,Overview,"Overview Analogous to many programming language frameworks that have component lifecycle hooks, such as Angular,Kubernetes provides Containers with lifecycle hooks.The hooks enable Containers to be aware of events in their management lifecycleand run code implemented in a handler when the corresponding lifecycle hook is executed.",62
3.4 - Container Lifecycle Hooks,Container hooks,"Container hooks There are two hooks that are exposed to Containers: PostStart This hook is executed immediately after a container is created.However, there is no guarantee that the hook will execute before the container ENTRYPOINT.No parameters are passed to the handler. PreStop This hook is called immediately before a container is terminated due to an API request or managementevent such as a liveness/startup probe failure, preemption, resource contention and others. A callto the PreStop hook fails if the container is already in a terminated or completed state and thehook must complete before the TERM signal to stop the container can be sent. The Pod's terminationgrace period countdown begins before the PreStop hook is executed, so regardless of the outcome ofthe handler, the container will eventually terminate within the Pod's termination grace period. Noparameters are passed to the handler. A more detailed description of the termination behavior can be found inTermination of Pods.",196
3.4 - Container Lifecycle Hooks,Hook handler implementations,"Hook handler implementations Containers can access a hook by implementing and registering a handler for that hook.There are two types of hook handlers that can be implemented for Containers: Exec - Executes a specific command, such as pre-stop.sh, inside the cgroups and namespaces of the Container.Resources consumed by the command are counted against the Container.HTTP - Executes an HTTP request against a specific endpoint on the Container.",88
3.4 - Container Lifecycle Hooks,Hook handler execution,"Hook handler execution When a Container lifecycle management hook is called,the Kubernetes management system executes the handler according to the hook action,httpGet and tcpSocket are executed by the kubelet process, and exec is executed in the container. Hook handler calls are synchronous within the context of the Pod containing the Container.This means that for a PostStart hook,the Container ENTRYPOINT and hook fire asynchronously.However, if the hook takes too long to run or hangs,the Container cannot reach a running state. PreStop hooks are not executed asynchronously from the signal to stop the Container; the hook mustcomplete its execution before the TERM signal can be sent. If a PreStop hook hangs duringexecution, the Pod's phase will be Terminating and remain there until the Pod is killed after itsterminationGracePeriodSeconds expires. This grace period applies to the total time it takes forboth the PreStop hook to execute and for the Container to stop normally. If, for example,terminationGracePeriodSeconds is 60, and the hook takes 55 seconds to complete, and the Containertakes 10 seconds to stop normally after receiving the signal, then the Container will be killedbefore it can stop normally, since terminationGracePeriodSeconds is less than the total time(55+10) it takes for these two things to happen. If either a PostStart or PreStop hook fails,it kills the Container. Users should make their hook handlers as lightweight as possible.There are cases, however, when long running commands make sense,such as when saving state prior to stopping a Container.",338
3.4 - Container Lifecycle Hooks,Hook delivery guarantees,"Hook delivery guarantees Hook delivery is intended to be at least once,which means that a hook may be called multiple times for any given event,such as for PostStart or PreStop.It is up to the hook implementation to handle this correctly. Generally, only single deliveries are made.If, for example, an HTTP hook receiver is down and is unable to take traffic,there is no attempt to resend.In some rare cases, however, double delivery may occur.For instance, if a kubelet restarts in the middle of sending a hook,the hook might be resent after the kubelet comes back up.",128
3.4 - Container Lifecycle Hooks,Debugging Hook handlers,"Debugging Hook handlers The logs for a Hook handler are not exposed in Pod events.If a handler fails for some reason, it broadcasts an event.For PostStart, this is the FailedPostStartHook event,and for PreStop, this is the FailedPreStopHook event.To generate a failed FailedPostStartHook event yourself, modify the lifecycle-events.yaml file to change the postStart command to ""badcommand"" and apply it.Here is some example output of the resulting events you see from running kubectl describe pod lifecycle-demo: Events:  Type     Reason               Age              From               Message  ----     ------               ----             ----               -------  Normal   Scheduled            7s               default-scheduler  Successfully assigned default/lifecycle-demo to ip-XXX-XXX-XX-XX.us-east-2...  Normal   Pulled               6s               kubelet            Successfully pulled image ""nginx"" in 229.604315ms  Normal   Pulling              4s (x2 over 6s)  kubelet            Pulling image ""nginx""  Normal   Created              4s (x2 over 5s)  kubelet            Created container lifecycle-demo-container  Normal   Started              4s (x2 over 5s)  kubelet            Started container lifecycle-demo-container  Warning  FailedPostStartHook  4s (x2 over 5s)  kubelet            Exec lifecycle hook ([badcommand]) for Container ""lifecycle-demo-container"" in Pod ""lifecycle-demo_default(30229739-9651-4e5a-9a32-a8f1688862db)"" failed - error: command 'badcommand' exited with 126: , message: ""OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \""badcommand\"": executable file not found in $PATH: unknown\r\n""  Normal   Killing              4s (x2 over 5s)  kubelet            FailedPostStartHook  Normal   Pulled               4s               kubelet            Successfully pulled image ""nginx"" in 215.66395ms  Warning  BackOff              2s (x2 over 3s)  kubelet            Back-off restarting failed container Learn more about the Container environment.Get hands-on experienceattaching handlers to Container lifecycle events.",533
4.1 - Windows containers in Kubernetes,default,"Windows applications constitute a large portion of the services and applications thatrun in many organizations. Windows containersprovide a way to encapsulate processes and package dependencies, making it easierto use DevOps practices and follow cloud native patterns for Windows applications. Organizations with investments in Windows-based applications and Linux-basedapplications don't have to look for separate orchestrators to manage their workloads,leading to increased operational efficiencies across their deployments, regardlessof operating system.",94
4.1 - Windows containers in Kubernetes,Windows nodes in Kubernetes,"Windows nodes in Kubernetes To enable the orchestration of Windows containers in Kubernetes, include Windows nodesin your existing Linux cluster. Scheduling Windows containers inPods on Kubernetes is similar toscheduling Linux-based containers. In order to run Windows containers, your Kubernetes cluster must includemultiple operating systems.While you can only run the control plane on Linux,you can deploy worker nodes running either Windows or Linux. Windows nodes aresupported provided that the operating system isWindows Server 2019. This document uses the term Windows containers to mean Windows containers withprocess isolation. Kubernetes does not support running Windows containers withHyper-V isolation.",141
4.1 - Windows containers in Kubernetes,Compatibility and limitations,"Compatibility and limitations Some node features are only available if you use a specificcontainer runtime; others are not available on Windows nodes,including: HugePages: not supported for Windows containersPrivileged containers: not supported for Windows containers.HostProcess Containers offer similar functionality.TerminationGracePeriod: requires containerD Not all features of shared namespaces are supported. See API compatibilityfor more details. See Windows OS version compatibility for details onthe Windows versions that Kubernetes is tested against. From an API and kubectl perspective, Windows containers behave in much the sameway as Linux-based containers. However, there are some notable differences in keyfunctionality which are outlined in this section.",143
4.1 - Windows containers in Kubernetes,Comparison with Linux,"Comparison with Linux Key Kubernetes elements work the same way in Windows as they do in Linux. Thissection refers to several key workload abstractions and how they map to Windows. PodsA Pod is the basic building block of Kubernetes–the smallest and simplest unit inthe Kubernetes object model that you create or deploy. You may not deploy Windows andLinux containers in the same Pod. All containers in a Pod are scheduled onto a singleNode where each Node represents a specific platform and architecture. The followingPod capabilities, properties and events are supported with Windows containers:Single or multiple containers per Pod with process isolation and volume sharingPod status fieldsReadiness, liveness, and startup probespostStart & preStop container lifecycle hooksConfigMap, Secrets: as environment variables or volumesemptyDir volumesNamed pipe host mountsResource limitsOS field:The .spec.os.name field should be set to windows to indicate that the current Pod uses Windows containers.Note: Starting from 1.25, the IdentifyPodOS feature gate is in GA stage and defaults to be enabled.If you set the .spec.os.name field to windows,you must not set the following fields in the .spec of that Pod:spec.hostPIDspec.hostIPCspec.securityContext.seLinuxOptionsspec.securityContext.seccompProfilespec.securityContext.fsGroupspec.securityContext.fsGroupChangePolicyspec.securityContext.sysctlsspec.shareProcessNamespacespec.securityContext.runAsUserspec.securityContext.runAsGroupspec.securityContext.supplementalGroupsspec.containers[*].securityContext.seLinuxOptionsspec.containers[*].securityContext.seccompProfilespec.containers[*].securityContext.capabilitiesspec.containers[*].securityContext.readOnlyRootFilesystemspec.containers[*].securityContext.privilegedspec.containers[*].securityContext.allowPrivilegeEscalationspec.containers[*].securityContext.procMountspec.containers[*].securityContext.runAsUserspec.containers[*].securityContext.runAsGroupIn the above list, wildcards (*) indicate all elements in a list.For example, spec.containers[*].securityContext refers to the SecurityContext objectfor all containers. If any of these fields is specified, the Pod willnot be admitted by the API server.Workload resources including:ReplicaSetDeploymentStatefulSetDaemonSetJobCronJobReplicationControllerServicesSee Load balancing and Services for more details. Pods, workload resources, and Services are critical elements to managing Windowsworkloads on Kubernetes. However, on their own they are not enough to enablethe proper lifecycle management of Windows workloads in a dynamic cloud nativeenvironment. kubectl execPod and container metricsHorizontal pod autoscalingResource quotasScheduler preemption",626
4.1 - Windows containers in Kubernetes,Command line options for the kubelet,"Command line options for the kubelet Some kubelet command line options behave differently on Windows, as described below: The --windows-priorityclass lets you set the scheduling priority of the kubelet process(see CPU resource management)The --kube-reserved, --system-reserved , and --eviction-hard flags updateNodeAllocatableEviction by using --enforce-node-allocable is not implementedEviction by using --eviction-hard and --eviction-soft are not implementedWhen running on a Windows node the kubelet does not have memory or CPUrestrictions. --kube-reserved and --system-reserved only subtract from NodeAllocatableand do not guarantee resource provided for workloads.See Resource Management for Windows nodesfor more information.The MemoryPressure Condition is not implementedThe kubelet does not take OOM eviction actions",185
4.1 - Windows containers in Kubernetes,API compatibility,"API compatibility There are subtle differences in the way the Kubernetes APIs work for Windows due to the OSand container runtime. Some workload properties were designed for Linux, and fail to run on Windows. At a high level, these OS concepts are different: Identity - Linux uses userID (UID) and groupID (GID) whichare represented as integer types. User and group namesare not canonical - they are just an alias in /etc/groupsor /etc/passwd back to UID+GID. Windows uses a larger binarysecurity identifier (SID)which is stored in the Windows Security Access Manager (SAM) database. Thisdatabase is not shared between the host and containers, or between containers.File permissions - Windows uses an access control list based on (SIDs), whereasPOSIX systems such as Linux use a bitmask based on object permissions and UID+GID,plus optional access control lists.File paths - the convention on Windows is to use \ instead of /. The Go IOlibraries typically accept both and just make it work, but when you're setting apath or command line that's interpreted inside a container, \ may be needed.Signals - Windows interactive apps handle termination differently, and canimplement one or more of these:A UI thread handles well-defined messages including WM_CLOSE.Console apps handle Ctrl-C or Ctrl-break using a Control Handler.Services register a Service Control Handler function that can acceptSERVICE_CONTROL_STOP control codes. Container exit codes follow the same convention where 0 is success, and nonzero is failure.The specific error codes may differ across Windows and Linux. However, exit codespassed from the Kubernetes components (kubelet, kube-proxy) are unchanged.",364
4.1 - Windows containers in Kubernetes,Field compatibility for container specifications,"Field compatibility for container specifications The following list documents differences between how Pod container specificationswork between Windows and Linux: Huge pages are not implemented in the Windows containerruntime, and are not available. They require asserting a userprivilegethat's not configurable for containers.requests.cpu and requests.memory - requests are subtractedfrom node available resources, so they can be used to avoid overprovisioning anode. However, they cannot be used to guarantee resources in an overprovisionednode. They should be applied to all containers as a best practice if the operatorwants to avoid overprovisioning entirely.securityContext.allowPrivilegeEscalation -not possible on Windows; none of the capabilities are hooked upsecurityContext.capabilities -POSIX capabilities are not implemented on WindowssecurityContext.privileged -Windows doesn't support privileged containers, use HostProcess Containers insteadsecurityContext.procMount -Windows doesn't have a /proc filesystemsecurityContext.readOnlyRootFilesystem -not possible on Windows; write access is required for registry & systemprocesses to run inside the containersecurityContext.runAsGroup -not possible on Windows as there is no GID supportsecurityContext.runAsNonRoot -this setting will prevent containers from running as ContainerAdministratorwhich is the closest equivalent to a root user on Windows.securityContext.runAsUser -use runAsUserNameinsteadsecurityContext.seLinuxOptions -not possible on Windows as SELinux is Linux-specificterminationMessagePath -this has some limitations in that Windows doesn't support mapping single files. Thedefault value is /dev/termination-log, which does work because it does notexist on Windows by default.",348
4.1 - Windows containers in Kubernetes,Field compatibility for Pod specifications,"Field compatibility for Pod specifications The following list documents differences between how Pod specifications work between Windows and Linux: hostIPC and hostpid - host namespace sharing is not possible on WindowshostNetwork - see belowdnsPolicy - setting the Pod dnsPolicy to ClusterFirstWithHostNet isnot supported on Windows because host networking is not provided. Pods alwaysrun with a container network.podSecurityContext see belowshareProcessNamespace - this is a beta feature, and depends on Linux namespaceswhich are not implemented on Windows. Windows cannot share process namespaces orthe container's root filesystem. Only the network can be shared.terminationGracePeriodSeconds - this is not fully implemented in Docker on Windows,see the GitHub issue.The behavior today is that the ENTRYPOINT process is sent CTRL_SHUTDOWN_EVENT,then Windows waits 5 seconds by default, and finally shuts downall processes using the normal Windows shutdown behavior. The 5second default is actually in the Windows registryinside the container,so it can be overridden when the container is built.volumeDevices - this is a beta feature, and is not implemented on Windows.Windows cannot attach raw block devices to pods.volumesIf you define an emptyDir volume, you cannot set its volume source to memory.You cannot enable mountPropagation for volume mounts as this is notsupported on Windows.",282
4.1 - Windows containers in Kubernetes,Field compatibility for hostNetwork,Field compatibility for hostNetwork FEATURE STATE: Kubernetes v1.26 [alpha] The kubelet can now request that pods running on Windows nodes use the host's network namespace insteadof creating a new pod network namespace. To enable this functionality pass --feature-gates=WindowsHostNetwork=true to the kubelet. Note: This functionality requires a container runtime that supports this functionality.,83
4.1 - Windows containers in Kubernetes,Pause container,"Pause container In a Kubernetes Pod, an infrastructure or “pause” container is first createdto host the container. In Linux, the cgroups and namespaces that make up a podneed a process to maintain their continued existence; the pause process providesthis. Containers that belong to the same pod, including infrastructure and workercontainers, share a common network endpoint (same IPv4 and / or IPv6 address, samenetwork port spaces). Kubernetes uses pause containers to allow for worker containerscrashing or restarting without losing any of the networking configuration. Kubernetes maintains a multi-architecture image that includes support for Windows.For Kubernetes v1.26 the recommended pause image is registry.k8s.io/pause:3.6.The source codeis available on GitHub. Microsoft maintains a different multi-architecture image, with Linux and Windowsamd64 support, that you can find as mcr.microsoft.com/oss/kubernetes/pause:3.6.This image is built from the same source as the Kubernetes maintained image butall of the Windows binaries are authenticode signed by Microsoft.The Kubernetes project recommends using the Microsoft maintained image if you aredeploying to a production or production-like environment that requires signedbinaries.",282
4.1 - Windows containers in Kubernetes,Container runtimes,"Container runtimes You need to install acontainer runtimeinto each node in the cluster so that Pods can run there. The following container runtimes work with Windows: Note:This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.",93
4.1 - Windows containers in Kubernetes,ContainerD,"ContainerD FEATURE STATE: Kubernetes v1.20 [stable] You can use ContainerD 1.4.0+as the container runtime for Kubernetes nodes that run Windows. Learn how to install ContainerD on a Windows node. Note: There is a known limitationwhen using GMSA with containerd to access Windows network shares, which requires akernel patch.",79
4.1 - Windows containers in Kubernetes,Windows OS version compatibility,"Windows OS version compatibility On Windows nodes, strict compatibility rules apply where the host OS version mustmatch the container base image OS version. Only Windows containers with a containeroperating system of Windows Server 2019 are fully supported. For Kubernetes v1.26, operating system compatibility for Windows nodes (and Pods)is as follows: Windows Server LTSC releaseWindows Server 2019Windows Server 2022Windows Server SAC releaseWindows Server version 20H2 The Kubernetes version-skew policy also applies.",105
4.1 - Windows containers in Kubernetes,Getting help and troubleshooting,"Getting help and troubleshooting Your main source of help for troubleshooting your Kubernetes cluster should startwith the Troubleshootingpage. Some additional, Windows-specific troubleshooting help is includedin this section. Logs are an important element of troubleshootingissues in Kubernetes. Make sure to include them any time you seektroubleshooting assistance from other contributors. Follow theinstructions in theSIG Windows contributing guide on gathering logs.",93
4.1 - Windows containers in Kubernetes,Reporting issues and feature requests,"Reporting issues and feature requests If you have what looks like a bug, or you would like tomake a feature request, please follow the SIG Windows contributing guide to create a new issue.You should first search the list of issues in case it wasreported previously and comment with your experience on the issue and add additionallogs. SIG Windows channel on the Kubernetes Slack is also a great avenue to get some initial support andtroubleshooting ideas prior to creating a ticket.",97
4.1 - Windows containers in Kubernetes,Deployment tools,"Deployment tools The kubeadm tool helps you to deploy a Kubernetes cluster, providing the controlplane to manage the cluster it, and nodes to run your workloads. The Kubernetes cluster API project also provides means to automate deployment of Windows nodes.",56
4.2 - Guide for scheduling Windows containers in Kubernetes,Before you begin,Before you begin Create a Kubernetes cluster that includes a control plane and a worker node running Windows ServerIt is important to note that creating and deploying services and workloads on Kubernetesbehaves in much the same way for Linux and Windows containers.Kubectl commands to interface with the cluster are identical.The example in the section below is provided to jumpstart your experience with Windows containers.,83
4.2 - Guide for scheduling Windows containers in Kubernetes,Getting Started: Deploying a Windows container,"Getting Started: Deploying a Windows container The example YAML file below deploys a simple webserver application running inside a Windows container. Create a service spec named win-webserver.yaml with the contents below: apiVersion: v1kind: Servicemetadata:  name: win-webserver  labels:    app: win-webserverspec:  ports:    # the port that this service should serve on    - port: 80      targetPort: 80  selector:    app: win-webserver  type: NodePort---apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: win-webserver  name: win-webserverspec:  replicas: 2  selector:    matchLabels:      app: win-webserver  template:    metadata:      labels:        app: win-webserver      name: win-webserver    spec:     containers:      - name: windowswebserver        image: mcr.microsoft.com/windows/servercore:ltsc2019        command:        - powershell.exe        - -command        - ""<#code used from https://gist.github.com/19WAS85/5424431#> ; $$listener = New-Object System.Net.HttpListener ; $$listener.Prefixes.Add('http://*:80/') ; $$listener.Start() ; $$callerCounts = @{} ; Write-Host('Listening at http://*:80/') ; while ($$listener.IsListening) { ;$$context = $$listener.GetContext() ;$$requestUrl = $$context.Request.Url ;$$clientIP = $$context.Request.RemoteEndPoint.Address ;$$response = $$context.Response ;Write-Host '' ;Write-Host('> {0}' -f $$requestUrl) ;  ;$$count = 1 ;$$k=$$callerCounts.Get_Item($$clientIP) ;if ($$k -ne $$null) { $$count += $$k } ;$$callerCounts.Set_Item($$clientIP, $$count) ;$$ip=(Get-NetAdapter | Get-NetIpAddress); $$header='<html><body><H1>Windows Container Web Server</H1>' ;$$callerCountsString='' ;$$callerCounts.Keys | % { $$callerCountsString+='<p>IP {0} callerCount {1} ' -f $$ip[1].IPAddress,$$callerCounts.Item($$_) } ;$$footer='</body></html>' ;$$content='{0}{1}{2}' -f $$header,$$callerCountsString,$$footer ;Write-Output $$content ;$$buffer = [System.Text.Encoding]::UTF8.GetBytes($$content) ;$$response.ContentLength64 = $$buffer.Length ;$$response.OutputStream.Write($$buffer, 0, $$buffer.Length) ;$$response.Close() ;$$responseStatus = $$response.StatusCode ;Write-Host('< {0}' -f $$responseStatus)  } ; ""     nodeSelector:      kubernetes.io/os: windows Note: Port mapping is also supported, but for simplicity this example exposesport 80 of the container directly to the Service. Check that all nodes are healthy:kubectl get nodesDeploy the service and watch for pod updates:kubectl apply -f win-webserver.yamlkubectl get pods -o wide -wWhen the service is deployed correctly both Pods are marked as Ready. To exit the watch command, press Ctrl+C.Check that the deployment succeeded. To verify:Two pods listed from the Linux control plane node, use kubectl get podsNode-to-pod communication across the network, curl port 80 of your pod IPs from the Linux control plane nodeto check for a web server responsePod-to-pod communication, ping between pods (and across hosts, if you have more than one Windows node)using docker exec or kubectl execService-to-pod communication, curl the virtual service IP (seen under kubectl get services)from the Linux control plane node and from individual podsService discovery, curl the service name with the Kubernetes default DNS suffixInbound connectivity, curl the NodePort from the Linux control plane node or machines outside of the clusterOutbound connectivity, curl external IPs from inside the pod using kubectl exec Note: Windows container hosts are not able to access the IP of services scheduled on them due to current platform limitations of the Windows networking stack.Only Windows pods are able to access service IPs.",1033
4.2 - Guide for scheduling Windows containers in Kubernetes,Capturing logs from workloads,"Capturing logs from workloads Logs are an important element of observability; they enable users to gain insightsinto the operational aspect of workloads and are a key ingredient to troubleshooting issues.Because Windows containers and workloads inside Windows containers behave differently from Linux containers,users had a hard time collecting logs, limiting operational visibility.Windows workloads for example are usually configured to log to ETW (Event Tracing for Windows)or push entries to the application event log.LogMonitor, an open source tool by Microsoft,is the recommended way to monitor configured log sources inside a Windows container.LogMonitor supports monitoring event logs, ETW providers, and custom application logs,piping them to STDOUT for consumption by kubectl logs <pod>. Follow the instructions in the LogMonitor GitHub page to copy its binaries and configuration filesto all your containers and add the necessary entrypoints for LogMonitor to push your logs to STDOUT.",190
4.2 - Guide for scheduling Windows containers in Kubernetes,Managing Workload Identity with Group Managed Service Accounts,"Managing Workload Identity with Group Managed Service Accounts Windows container workloads can be configured to use Group Managed Service Accounts (GMSA).Group Managed Service Accounts are a specific type of Active Directory account that provide automatic password management,simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers.Containers configured with a GMSA can access external Active Directory Domain resources while carrying the identity configured with the GMSA.Learn more about configuring and using GMSA for Windows containers here.",112
4.2 - Guide for scheduling Windows containers in Kubernetes,Taints and Tolerations,"Taints and Tolerations Users need to use some combination of taints and node selectors in order toschedule Linux and Windows workloads to their respective OS-specific nodes.The recommended approach is outlined below,with one of its main goals being that this approach should not break compatibility for existing Linux workloads. Starting from 1.25, you can (and should) set .spec.os.name for each Pod, to indicate the operating systemthat the containers in that Pod are designed for. For Pods that run Linux containers, set.spec.os.name to linux. For Pods that run Windows containers, set .spec.os.nameto windows. Note: Starting from 1.25, the IdentifyPodOS feature is in GA stage and defaults to be enabled. The scheduler does not use the value of .spec.os.name when assigning Pods to nodes. You shoulduse normal Kubernetes mechanisms forassigning pods to nodesto ensure that the control plane for your cluster places pods onto nodes that are running theappropriate operating system. The .spec.os.name value has no effect on the scheduling of the Windows pods,so taints and tolerations and node selectors are still requiredto ensure that the Windows pods land onto appropriate Windows nodes.",265
4.2 - Guide for scheduling Windows containers in Kubernetes,Ensuring OS-specific workloads land on the appropriate container host,"Ensuring OS-specific workloads land on the appropriate container host Users can ensure Windows containers can be scheduled on the appropriate host using Taints and Tolerations.All Kubernetes nodes today have the following default labels: kubernetes.io/os = [windows|linux]kubernetes.io/arch = [amd64|arm64|...] If a Pod specification does not specify a nodeSelector like ""kubernetes.io/os"": windows,it is possible the Pod can be scheduled on any host, Windows or Linux.This can be problematic since a Windows container can only run on Windows and a Linux container can only run on Linux.The best practice is to use a nodeSelector. However, we understand that in many cases users have a pre-existing large number of deployments for Linux containers,as well as an ecosystem of off-the-shelf configurations, such as community Helm charts, and programmatic Pod generation cases, such as with Operators.In those situations, you may be hesitant to make the configuration change to add nodeSelectors.The alternative is to use Taints. Because the kubelet can set Taints during registration,it could easily be modified to automatically add a taint when running on Windows only. For example: --register-with-taints='os=windows:NoSchedule' By adding a taint to all Windows nodes, nothing will be scheduled on them (that includes existing Linux Pods).In order for a Windows Pod to be scheduled on a Windows node,it would need both the nodeSelector and the appropriate matching toleration to choose Windows. nodeSelector:    kubernetes.io/os: windows    node.kubernetes.io/windows-build: '10.0.17763'tolerations:    - key: ""os""      operator: ""Equal""      value: ""windows""      effect: ""NoSchedule""",406
4.2 - Guide for scheduling Windows containers in Kubernetes,Handling multiple Windows versions in the same cluster,"Handling multiple Windows versions in the same cluster The Windows Server version used by each pod must match that of the node. If you want to use multiple WindowsServer versions in the same cluster, then you should set additional node labels and nodeSelectors. Kubernetes 1.17 automatically adds a new label node.kubernetes.io/windows-build to simplify this.If you're running an older version, then it's recommended to add this label manually to Windows nodes. This label reflects the Windows major, minor, and build number that need to match for compatibility.Here are values used today for each Windows Server version. Product NameBuild Number(s)Windows Server 201910.0.17763Windows Server, Version 20H210.0.19042Windows Server 202210.0.20348",167
4.2 - Guide for scheduling Windows containers in Kubernetes,Simplifying with RuntimeClass,"Simplifying with RuntimeClass RuntimeClass can be used to simplify the process of using taints and tolerations.A cluster administrator can create a RuntimeClass object which is used to encapsulate these taints and tolerations. Save this file to runtimeClasses.yml. It includes the appropriate nodeSelectorfor the Windows OS, architecture, and version. apiVersion: node.k8s.io/v1kind: RuntimeClassmetadata:  name: windows-2019handler: 'docker'scheduling:  nodeSelector:    kubernetes.io/os: 'windows'    kubernetes.io/arch: 'amd64'    node.kubernetes.io/windows-build: '10.0.17763'  tolerations:  - effect: NoSchedule    key: os    operator: Equal    value: ""windows"" Run kubectl create -f runtimeClasses.yml using as a cluster administratorAdd runtimeClassName: windows-2019 as appropriate to Pod specs For example: apiVersion: apps/v1kind: Deploymentmetadata:  name: iis-2019  labels:    app: iis-2019spec:  replicas: 1  template:    metadata:      name: iis-2019      labels:        app: iis-2019    spec:      runtimeClassName: windows-2019      containers:      - name: iis        image: mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019        resources:          limits:            cpu: 1            memory: 800Mi          requests:            cpu: .1            memory: 300Mi        ports:          - containerPort: 80 selector:    matchLabels:      app: iis-2019---apiVersion: v1kind: Servicemetadata:  name: iisspec:  type: LoadBalancer  ports:  - protocol: TCP    port: 80  selector:    app: iis-2019",426
5 - Workloads,default,"Understand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them. A workload is an application running on Kubernetes.Whether your workload is a single component or several that work together, on Kubernetes you runit inside a set of pods.In Kubernetes, a Pod represents a set of runningcontainers on your cluster. Kubernetes pods have a defined lifecycle.For example, once a pod is running in your cluster then a critical fault on thenode where that pod is running means thatall the pods on that node fail. Kubernetes treats that level of failure as final: youwould need to create a new Pod to recover, even if the node later becomes healthy. However, to make life considerably easier, you don't need to manage each Pod directly.Instead, you can use workload resources that manage a set of pods on your behalf.These resources configure controllersthat make sure the right number of the right kind of pod are running, to match the stateyou specified. Kubernetes provides several built-in workload resources: Deployment and ReplicaSet(replacing the legacy resourceReplicationController).Deployment is a good fit for managing a stateless application workload on your cluster,where any Pod in the Deployment is interchangeable and can be replaced if needed.StatefulSet lets yourun one or more related Pods that do track state somehow. For example, if your workloadrecords data persistently, you can run a StatefulSet that matches each Pod with aPersistentVolume. Your code, running in thePods for that StatefulSet, can replicate data to other Pods in the same StatefulSetto improve overall resilience.DaemonSet defines Pods that providenode-local facilities. These might be fundamental to the operation of your cluster, suchas a networking helper tool, or be part of anadd-on.Every time you add a node to your cluster that matches the specification in a DaemonSet,the control plane schedules a Pod for that DaemonSet onto the new node.Job andCronJobdefine tasks that run to completion and then stop. Jobs represent one-off tasks, whereasCronJobs recur according to a schedule. In the wider Kubernetes ecosystem, you can find third-party workload resources that provideadditional behaviors. Using acustom resource definition,you can add in a third-party workload resource if you want a specific behavior that's not partof Kubernetes' core. For example, if you wanted to run a group of Pods for your application butstop work unless all the Pods are available (perhaps for some high-throughput distributed task),then you can implement or install an extension that does provide that feature. As well as reading about each resource, you can learn about specific tasks that relate to them: Run a stateless application using a DeploymentRun a stateful application either as a single instanceor as a replicated setRun automated tasks with a CronJob To learn about Kubernetes' mechanisms for separating code from configuration,visit Configuration. There are two supporting concepts that provide backgrounds about how Kubernetes manages podsfor applications: Garbage collection tidies up objectsfrom your cluster after their owning resource has been removed.The time-to-live after finished controllerremoves Jobs once a defined time has passed since they completed. Once your application is running, you might want to make it available on the internet asa Service or, for web application only,using an Ingress.",743
5.1 - Pods,default,"Pods are the smallest deployable units of computing that you can create and manage in Kubernetes. A Pod (as in a pod of whales or pea pod) is a group of one or morecontainers, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located andco-scheduled, and run in a shared context. A Pod models anapplication-specific ""logical host"": it contains one or more applicationcontainers which are relatively tightly coupled.In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host. As well as application containers, a Pod can containinit containers that runduring Pod startup. You can also injectephemeral containersfor debugging if your cluster offers this.",175
5.1 - Pods,What is a Pod?,"What is a Pod? Note: While Kubernetes supports morecontainer runtimesthan just Docker, Docker is the most commonly knownruntime, and it helps to describe Pods using some terminology from Docker. The shared context of a Pod is a set of Linux namespaces, cgroups, andpotentially other facets of isolation - the same things that isolate a container. Within a Pod's context, the individual applications may havefurther sub-isolations applied. A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.",113
5.1 - Pods,Using Pods,"Using Pods The following is an example of a Pod which consists of a container running the image nginx:1.14.2. pods/simple-pod.yamlapiVersion: v1kind: Podmetadata:  name: nginxspec:  containers:  - name: nginx    image: nginx:1.14.2    ports:    - containerPort: 80 To create the Pod shown above, run the following command: kubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml Pods are generally not created directly and are created using workload resources.See Working with Pods for more information on how Pods are usedwith workload resources.",153
5.1 - Pods,Workload resources for managing pods,"Workload resources for managing pods Usually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as Deployment or Job.If your Pods need to track state, consider theStatefulSet resource. Pods in a Kubernetes cluster are used in two main ways: Pods that run a single container. The ""one-container-per-Pod"" model is themost common Kubernetes use case; in this case, you can think of a Pod as awrapper around a single container; Kubernetes manages Pods rather than managingthe containers directly.Pods that run multiple containers that need to work together. A Pod canencapsulate an application composed of multiple co-located containers that aretightly coupled and need to share resources. These co-located containersform a single cohesive unit of service—for example, one container serving datastored in a shared volume to the public, while a separate sidecar containerrefreshes or updates those files.The Pod wraps these containers, storage resources, and an ephemeral networkidentity together as a single unit.Note: Grouping multiple co-located and co-managed containers in a single Pod is arelatively advanced use case. You should use this pattern only in specificinstances in which your containers are tightly coupled. Each Pod is meant to run a single instance of a given application. If you want toscale your application horizontally (to provide more overall resources by runningmore instances), you should use multiple Pods, one for each instance. InKubernetes, this is typically referred to as replication.Replicated Pods are usually created and managed as a group by a workload resourceand its controller. See Pods and controllers for more information on howKubernetes uses workload resources, and their controllers, to implement applicationscaling and auto-healing.",395
5.1 - Pods,How Pods manage multiple containers,"How Pods manage multiple containers Pods are designed to support multiple cooperating processes (as containers) that forma cohesive unit of service. The containers in a Pod are automatically co-located andco-scheduled on the same physical or virtual machine in the cluster. The containerscan share resources and dependencies, communicate with one another, and coordinatewhen and how they are terminated. For example, you might have a container thatacts as a web server for files in a shared volume, and a separate ""sidecar"" containerthat updates those files from a remote source, as in the following diagram:  Some Pods have init containers as well as app containers. Init containers run and complete before the app containers are started. Pods natively provide two kinds of shared resources for their constituent containers:networking and storage.",166
5.1 - Pods,Working with Pods,"Working with Pods You'll rarely create individual Pods directly in Kubernetes—even singleton Pods. Thisis because Pods are designed as relatively ephemeral, disposable entities. Whena Pod gets created (directly by you, or indirectly by acontroller), the new Pod isscheduled to run on a Node in your cluster.The Pod remains on that node until the Pod finishes execution, the Pod object is deleted,the Pod is evicted for lack of resources, or the node fails. Note: Restarting a container in a Pod should not be confused with restarting a Pod. A Podis not a process, but an environment for running container(s). A Pod persists untilit is deleted. The name of a Pod must be a validDNS subdomainvalue, but this can produce unexpected results for the Pod hostname. For best compatibility,the name should follow the more restrictive rules for aDNS label.",195
5.1 - Pods,Pod OS,"Pod OS FEATURE STATE: Kubernetes v1.25 [stable] You should set the .spec.os.name field to either windows or linux to indicate the OS onwhich you want the pod to run. These two are the only operating systems supported for now byKubernetes. In future, this list may be expanded. In Kubernetes v1.26, the value you set for this field has noeffect on scheduling of the pods.Setting the .spec.os.name helps to identify the pod OSauthoratitively and is used for validation. The kubelet refuses to run a Pod where you havespecified a Pod OS, if this isn't the same as the operating system for the node wherethat kubelet is running.The Pod security standards also use thisfield to avoid enforcing policies that aren't relevant to that operating system.",182
5.1 - Pods,Pods and controllers,"Pods and controllers You can use workload resources to create and manage multiple Pods for you. A controllerfor the resource handles replication and rollout and automatic healing in case ofPod failure. For example, if a Node fails, a controller notices that Pods on thatNode have stopped working and creates a replacement Pod. The scheduler places thereplacement Pod onto a healthy Node. Here are some examples of workload resources that manage one or more Pods: DeploymentStatefulSetDaemonSet",99
5.1 - Pods,Pod templates,"Pod templates Controllers for workload resources create Podsfrom a pod template and manage those Pods on your behalf. PodTemplates are specifications for creating Pods, and are included in workload resources such asDeployments,Jobs, andDaemonSets. Each controller for a workload resource uses the PodTemplate inside the workloadobject to make actual Pods. The PodTemplate is part of the desired state of whateverworkload resource you used to run your app. The sample below is a manifest for a simple Job with a template that starts onecontainer. The container in that Pod prints a message then pauses. apiVersion: batch/v1kind: Jobmetadata:  name: hellospec:  template:    # This is the pod template    spec:      containers:      - name: hello        image: busybox:1.28        command: ['sh', '-c', 'echo ""Hello, Kubernetes!"" && sleep 3600']      restartPolicy: OnFailure    # The pod template ends here Modifying the pod template or switching to a new pod template has no direct effecton the Pods that already exist. If you change the pod template for a workloadresource, that resource needs to create replacement Pods that use the updated template. For example, the StatefulSet controller ensures that the running Pods match the currentpod template for each StatefulSet object. If you edit the StatefulSet to change its podtemplate, the StatefulSet starts to create new Pods based on the updated template.Eventually, all of the old Pods are replaced with new Pods, and the update is complete. Each workload resource implements its own rules for handling changes to the Pod template.If you want to read more about StatefulSet specifically, readUpdate strategy in the StatefulSet Basics tutorial. On Nodes, the kubelet does notdirectly observe or manage any of the details around pod templates and updates; thosedetails are abstracted away. That abstraction and separation of concerns simplifiessystem semantics, and makes it feasible to extend the cluster's behavior withoutchanging existing code.",434
5.1 - Pods,Pod update and replacement,"Pod update and replacement As mentioned in the previous section, when the Pod template for a workloadresource is changed, the controller creates new Pods based on the updatedtemplate instead of updating or patching the existing Pods. Kubernetes doesn't prevent you from managing Pods directly. It is possible toupdate some fields of a running Pod, in place. However, Pod update operationslikepatch, andreplacehave some limitations: Most of the metadata about a Pod is immutable. For example, you cannotchange the namespace, name, uid, or creationTimestamp fields;the generation field is unique. It only accepts updates that increment thefield's current value.If the metadata.deletionTimestamp is set, no new entry can be added to themetadata.finalizers list.Pod updates may not change fields other than spec.containers[*].image,spec.initContainers[*].image, spec.activeDeadlineSeconds orspec.tolerations. For spec.tolerations, you can only add new entries.When updating the spec.activeDeadlineSeconds field, two types of updatesare allowed:setting the unassigned field to a positive number;updating the field from a positive number to a smaller, non-negativenumber.",267
5.1 - Pods,Storage in Pods,"Storage in Pods A Pod can specify a set of shared storagevolumes. All containersin the Pod can access the shared volumes, allowing those containers toshare data. Volumes also allow persistent data in a Pod to survivein case one of the containers within needs to be restarted. SeeStorage for more information on howKubernetes implements shared storage and makes it available to Pods.",80
5.1 - Pods,Pod networking,"Pod networking Each Pod is assigned a unique IP address for each address family. Everycontainer in a Pod shares the network namespace, including the IP address andnetwork ports. Inside a Pod (and only then), the containers that belong to the Podcan communicate with one another using localhost. When containers in a Pod communicatewith entities outside the Pod,they must coordinate how they use the shared network resources (such as ports).Within a Pod, containers share an IP address and port space, andcan find each other via localhost. The containers in a Pod can also communicatewith each other using standard inter-process communications like SystemV semaphoresor POSIX shared memory. Containers in different Pods have distinct IP addressesand can not communicate by OS-level IPC without special configuration.Containers that want to interact with a container running in a different Pod canuse IP networking to communicate. Containers within the Pod see the system hostname as being the same as the configuredname for the Pod. There's more about this in the networkingsection.",212
5.1 - Pods,Privileged mode for containers,Privileged mode for containers Note: Your container runtime must support the concept of a privileged container for this setting to be relevant. Any container in a pod can run in privileged mode to use operating system administrative capabilitiesthat would otherwise be inaccessible. This is available for both Windows and Linux.,59
5.1 - Pods,Linux priviledged containers,"Linux priviledged containers In Linux, any container in a Pod can enable privileged mode using the privileged (Linux) flagon the security context of thecontainer spec. This is useful for containers that want to use operating system administrativecapabilities such as manipulating the network stack or accessing hardware devices.",58
5.1 - Pods,Windows priviledged containers,"Windows priviledged containers FEATURE STATE: Kubernetes v1.26 [stable] In Windows, you can create a Windows HostProcess podby setting the windowsOptions.hostProcess flag on the security context of the pod spec. All containers in thesepods must run as Windows HostProcess containers. HostProcess pods run directly on the host and can also be usedto perform administrative tasks as is done with Linux privileged containers. In order to use this feature, theWindowsHostProcessContainers feature gate must be enabled.",107
5.1 - Pods,Static Pods,"Static Pods Static Pods are managed directly by the kubelet daemon on a specific node,without the API serverobserving them.Whereas most Pods are managed by the control plane (for example, aDeployment), for staticPods, the kubelet directly supervises each static Pod (and restarts it if it fails). Static Pods are always bound to one Kubelet on a specific node.The main use for static Pods is to run a self-hosted control plane: in other words,using the kubelet to supervise the individual control plane components. The kubelet automatically tries to create a mirror Podon the Kubernetes API server for each static Pod.This means that the Pods running on a node are visible on the API server,but cannot be controlled from there. Note: The spec of a static Pod cannot refer to other API objects(e.g., ServiceAccount,ConfigMap,Secret, etc).",199
5.1 - Pods,Container probes,"Container probes A probe is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet can invoke different actions: ExecAction (performed with the help of the container runtime)TCPSocketAction (checked directly by the kubelet)HTTPGetAction (checked directly by the kubelet) You can read more about probesin the Pod Lifecycle documentation. Learn about the lifecycle of a Pod.Learn about RuntimeClass and how you can use it toconfigure different Pods with different container runtime configurations.Read about PodDisruptionBudget and how you can use it to manage application availability during disruptions.Pod is a top-level resource in the Kubernetes REST API.ThePodobject definition describes the object in detail.The Distributed System Toolkit: Patterns for Composite Containers explains common layouts for Pods with more than one container.Read about Pod topology spread constraints To understand the context for why Kubernetes wraps a common Pod API in other resources (such as StatefulSets or Deployments), you can read about the prior art, including: AuroraBorgMarathonOmegaTupperware.",241
5.1.1 - Pod Lifecycle,default,"This page describes the lifecycle of a Pod. Pods follow a defined lifecycle, startingin the Pending phase, moving through Running if at least oneof its primary containers starts OK, and then through either the Succeeded orFailed phases depending on whether any container in the Pod terminated in failure. Whilst a Pod is running, the kubelet is able to restart containers to handle somekind of faults. Within a Pod, Kubernetes tracks different containerstates and determines what action to take to make the Podhealthy again. In the Kubernetes API, Pods have both a specification and an actual status. Thestatus for a Pod object consists of a set of Pod conditions.You can also inject custom readiness information into thecondition data for a Pod, if that is useful to your application. Pods are only scheduled once in their lifetime.Once a Pod is scheduled (assigned) to a Node, the Pod runs on that Node until it stopsor is terminated.",202
5.1.1 - Pod Lifecycle,Pod lifetime,"Pod lifetime Like individual application containers, Pods are considered to be relativelyephemeral (rather than durable) entities. Pods are created, assigned a uniqueID (UID), and scheduledto nodes where they remain until termination (according to restart policy) ordeletion.If a Node dies, the Pods scheduled to that nodeare scheduled for deletion after a timeout period. Pods do not, by themselves, self-heal. If a Pod is scheduled to anode that then fails, the Pod is deleted; likewise, a Pod won'tsurvive an eviction due to a lack of resources or Node maintenance. Kubernetes uses ahigher-level abstraction, called acontroller, that handles the work ofmanaging the relatively disposable Pod instances. A given Pod (as defined by a UID) is never ""rescheduled"" to a different node; instead,that Pod can be replaced by a new, near-identical Pod, with even the same name ifdesired, but with a different UID. When something is said to have the same lifetime as a Pod, such as avolume,that means that the thing exists as long as that specific Pod (with that exact UID)exists. If that Pod is deleted for any reason, and even if an identical replacementis created, the related thing (a volume, in this example) is also destroyed andcreated anew. Pod diagram A multi-container Pod that contains a file puller and aweb server that uses a persistent volume for shared storage between the containers.",315
5.1.1 - Pod Lifecycle,Pod phase,"Pod phase A Pod's status field is aPodStatusobject, which has a phase field. The phase of a Pod is a simple, high-level summary of where the Pod is in itslifecycle. The phase is not intended to be a comprehensive rollup of observationsof container or Pod state, nor is it intended to be a comprehensive state machine. The number and meanings of Pod phase values are tightly guarded.Other than what is documented here, nothing should be assumed about Pods thathave a given phase value. Here are the possible values for phase: ValueDescriptionPendingThe Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.RunningThe Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.SucceededAll containers in the Pod have terminated in success, and will not be restarted.FailedAll containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system.UnknownFor some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running. Note: When a Pod is being deleted, it is shown as Terminating by some kubectl commands.This Terminating status is not one of the Pod phases.A Pod is granted a term to terminate gracefully, which defaults to 30 seconds.You can use the flag --force to terminate a Pod by force. If a node dies or is disconnected from the rest of the cluster, Kubernetesapplies a policy for setting the phase of all Pods on the lost node to Failed.",407
5.1.1 - Pod Lifecycle,Container states,"Container states As well as the phase of the Pod overall, Kubernetes tracks the state ofeach container inside a Pod. You can usecontainer lifecycle hooks totrigger events to run at certain points in a container's lifecycle. Once the schedulerassigns a Pod to a Node, the kubelet starts creating containers for that Podusing a container runtime.There are three possible container states: Waiting, Running, and Terminated. To check the state of a Pod's containers, you can usekubectl describe pod <name-of-pod>. The output shows the state for each containerwithin that Pod. Each state has a specific meaning:",137
5.1.1 - Pod Lifecycle,Waiting,"Waiting If a container is not in either the Running or Terminated state, it is Waiting.A container in the Waiting state is still running the operations it requires inorder to complete start up: for example, pulling the container image from a containerimage registry, or applying Secretdata.When you use kubectl to query a Pod with a container that is Waiting, you also seea Reason field to summarize why the container is in that state.",92
5.1.1 - Pod Lifecycle,Running,"Running The Running status indicates that a container is executing without issues. If therewas a postStart hook configured, it has already executed and finished. When you usekubectl to query a Pod with a container that is Running, you also see informationabout when the container entered the Running state.",60
5.1.1 - Pod Lifecycle,Terminated,"Terminated A container in the Terminated state began execution and then either ran tocompletion or failed for some reason. When you use kubectl to query a Pod witha container that is Terminated, you see a reason, an exit code, and the start andfinish time for that container's period of execution. If a container has a preStop hook configured, this hook runs before the container entersthe Terminated state.",90
5.1.1 - Pod Lifecycle,Container restart policy,"Container restart policy The spec of a Pod has a restartPolicy field with possible values Always, OnFailure,and Never. The default value is Always. The restartPolicy applies to all containers in the Pod. restartPolicy onlyrefers to restarts of the containers by the kubelet on the same node. After containersin a Pod exit, the kubelet restarts them with an exponential back-off delay (10s, 20s,40s, …), that is capped at five minutes. Once a container has executed for 10 minuteswithout any problems, the kubelet resets the restart backoff timer for that container.",129
5.1.1 - Pod Lifecycle,Pod conditions,"Pod conditions A Pod has a PodStatus, which has an array ofPodConditionsthrough which the Pod has or has not passed. Kubelet manages the followingPodConditions: PodScheduled: the Pod has been scheduled to a node.PodHasNetwork: (alpha feature; must be enabled explicitly) thePod sandbox has been successfully created and networking configured.ContainersReady: all containers in the Pod are ready.Initialized: all init containershave completed successfully.Ready: the Pod is able to serve requests and should be added to the loadbalancing pools of all matching Services. Field nameDescriptiontypeName of this Pod condition.statusIndicates whether that condition is applicable, with possible values ""True"", ""False"", or ""Unknown"".lastProbeTimeTimestamp of when the Pod condition was last probed.lastTransitionTimeTimestamp for when the Pod last transitioned from one status to another.reasonMachine-readable, UpperCamelCase text indicating the reason for the condition's last transition.messageHuman-readable message indicating details about the last status transition.",219
5.1.1 - Pod Lifecycle,Pod readiness,"Pod readiness FEATURE STATE: Kubernetes v1.14 [stable] Your application can inject extra feedback or signals into PodStatus:Pod readiness. To use this, set readinessGates in the Pod's spec tospecify a list of additional conditions that the kubelet evaluates for Pod readiness. Readiness gates are determined by the current state of status.conditionfields for the Pod. If Kubernetes cannot find such a condition in thestatus.conditions field of a Pod, the status of the conditionis defaulted to ""False"". Here is an example: kind: Pod...spec:  readinessGates:    - conditionType: ""www.example.com/feature-1""status:  conditions:    - type: Ready                              # a built in PodCondition      status: ""False""      lastProbeTime: null      lastTransitionTime: 2018-01-01T00:00:00Z    - type: ""www.example.com/feature-1""        # an extra PodCondition      status: ""False""      lastProbeTime: null      lastTransitionTime: 2018-01-01T00:00:00Z  containerStatuses:    - containerID: docker://abcd...      ready: true... The Pod conditions you add must have names that meet the Kubernetes label key format.",286
5.1.1 - Pod Lifecycle,Status for Pod readiness,"Status for Pod readiness The kubectl patch command does not support patching object status.To set these status.conditions for the pod, applications andoperators should usethe PATCH action.You can use a Kubernetes client library towrite code that sets custom Pod conditions for Pod readiness. For a Pod that uses custom conditions, that Pod is evaluated to be ready onlywhen both the following statements apply: All containers in the Pod are ready.All conditions specified in readinessGates are True. When a Pod's containers are Ready but at least one custom condition is missing orFalse, the kubelet sets the Pod's condition to ContainersReady.",136
5.1.1 - Pod Lifecycle,Pod network readiness,"Pod network readiness FEATURE STATE: Kubernetes v1.25 [alpha] After a Pod gets scheduled on a node, it needs to be admitted by the Kubelet andhave any volumes mounted. Once these phases are complete, the Kubelet works witha container runtime (using Container runtime interface (CRI)) to set up aruntime sandbox and configure networking for the Pod. If thePodHasNetworkCondition feature gate is enabled,Kubelet reports whether a pod has reached this initialization milestone throughthe PodHasNetwork condition in the status.conditions field of a Pod. The PodHasNetwork condition is set to False by the Kubelet when it detects aPod does not have a runtime sandbox with networking configured. This occurs inthe following scenarios: Early in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox for the Pod using the container runtime.Later in the lifecycle of the Pod, when the Pod sandbox has been destroyed dueto either:the node rebooting, without the Pod getting evictedfor container runtimes that use virtual machines for isolation, the Podsandbox virtual machine rebooting, which then requires creating a new sandbox and fresh container network configuration. The PodHasNetwork condition is set to True by the kubelet after thesuccessful completion of sandbox creation and network configuration for the Podby the runtime plugin. The kubelet can start pulling container images and createcontainers after PodHasNetwork condition has been set to True. For a Pod with init containers, the kubelet sets the Initialized condition toTrue after the init containers have successfully completed (which happensafter successful sandbox creation and network configuration by the runtimeplugin). For a Pod without init containers, the kubelet sets the Initializedcondition to True before sandbox creation and network configuration starts.",375
5.1.1 - Pod Lifecycle,Check mechanisms,"Check mechanisms There are four different ways to check a container using a probe.Each probe must define exactly one of these four mechanisms: execExecutes a specified command inside the container. The diagnosticis considered successful if the command exits with a status code of 0.grpcPerforms a remote procedure call using gRPC.The target should implementgRPC health checks.The diagnostic is considered successful if the statusof the response is SERVING.gRPC probes are an alpha feature and are only available if youenable the GRPCContainerProbefeature gate.httpGetPerforms an HTTP GET request against the Pod's IPaddress on a specified port and path. The diagnostic isconsidered successful if the response has a status codegreater than or equal to 200 and less than 400.tcpSocketPerforms a TCP check against the Pod's IP address ona specified port. The diagnostic is considered successful ifthe port is open. If the remote system (the container) closesthe connection immediately after it opens, this counts as healthy.",214
5.1.1 - Pod Lifecycle,Probe outcome,"Probe outcome Each probe has one of three results: SuccessThe container passed the diagnostic.FailureThe container failed the diagnostic.UnknownThe diagnostic failed (no action should be taken, and the kubeletwill make further checks).",46
5.1.1 - Pod Lifecycle,Types of probe,"Types of probe The kubelet can optionally perform and react to three kinds of probes on runningcontainers: livenessProbeIndicates whether the container is running. Ifthe liveness probe fails, the kubelet kills the container, and the containeris subjected to its restart policy. If a container does notprovide a liveness probe, the default state is Success.readinessProbeIndicates whether the container is ready to respond to requests.If the readiness probe fails, the endpoints controller removes the Pod's IPaddress from the endpoints of all Services that match the Pod. The defaultstate of readiness before the initial delay is Failure. If a container doesnot provide a readiness probe, the default state is Success.startupProbeIndicates whether the application within the container is started.All other probes are disabled if a startup probe is provided, until it succeeds.If the startup probe fails, the kubelet kills the container, and the containeris subjected to its restart policy. If a container does notprovide a startup probe, the default state is Success. For more information about how to set up a liveness, readiness, or startup probe,see Configure Liveness, Readiness and Startup Probes.",253
5.1.1 - Pod Lifecycle,When should you use a liveness probe?,"When should you use a liveness probe? FEATURE STATE: Kubernetes v1.0 [stable] If the process in your container is able to crash on its own whenever itencounters an issue or becomes unhealthy, you do not necessarily need a livenessprobe; the kubelet will automatically perform the correct action in accordancewith the Pod's restartPolicy. If you'd like your container to be killed and restarted if a probe fails, thenspecify a liveness probe, and specify a restartPolicy of Always or OnFailure.",113
5.1.1 - Pod Lifecycle,When should you use a readiness probe?,"When should you use a readiness probe? FEATURE STATE: Kubernetes v1.0 [stable] If you'd like to start sending traffic to a Pod only when a probe succeeds,specify a readiness probe. In this case, the readiness probe might be the sameas the liveness probe, but the existence of the readiness probe in the spec meansthat the Pod will start without receiving any traffic and only start receivingtraffic after the probe starts succeeding. If you want your container to be able to take itself down for maintenance, youcan specify a readiness probe that checks an endpoint specific to readiness thatis different from the liveness probe. If your app has a strict dependency on back-end services, you can implement botha liveness and a readiness probe. The liveness probe passes when the app itselfis healthy, but the readiness probe additionally checks that each requiredback-end service is available. This helps you avoid directing traffic to Podsthat can only respond with error messages. If your container needs to work on loading large data, configuration files, ormigrations during startup, you can use astartup probe. However, if you want todetect the difference between an app that has failed and an app that is stillprocessing its startup data, you might prefer a readiness probe. Note: If you want to be able to drain requests when the Pod is deleted, you do notnecessarily need a readiness probe; on deletion, the Pod automatically puts itselfinto an unready state regardless of whether the readiness probe exists.The Pod remains in the unready state while it waits for the containers in the Podto stop.",334
5.1.1 - Pod Lifecycle,When should you use a startup probe?,"When should you use a startup probe? FEATURE STATE: Kubernetes v1.20 [stable] Startup probes are useful for Pods that have containers that take a long time tocome into service. Rather than set a long liveness interval, you can configurea separate configuration for probing the container as it starts up, allowinga time longer than the liveness interval would allow. If your container usually starts in more thaninitialDelaySeconds + failureThreshold × periodSeconds, you should specify astartup probe that checks the same endpoint as the liveness probe. The default forperiodSeconds is 10s. You should then set its failureThreshold high enough toallow the container to start, without changing the default values of the livenessprobe. This helps to protect against deadlocks.",166
5.1.1 - Pod Lifecycle,Termination of Pods,"Termination of Pods Because Pods represent processes running on nodes in the cluster, it is important toallow those processes to gracefully terminate when they are no longer needed (ratherthan being abruptly stopped with a KILL signal and having no chance to clean up). The design aim is for you to be able to request deletion and know when processesterminate, but also be able to ensure that deletes eventually complete.When you request deletion of a Pod, the cluster records and tracks the intended grace periodbefore the Pod is allowed to be forcefully killed. With that forceful shutdown tracking inplace, the kubelet attempts gracefulshutdown. Typically, the container runtime sends a TERM signal to the main process in eachcontainer. Many container runtimes respect the STOPSIGNAL value defined in the containerimage and send this instead of TERM.Once the grace period has expired, the KILL signal is sent to any remainingprocesses, and the Pod is then deleted from theAPI Server. If the kubelet or thecontainer runtime's management service is restarted while waiting for processes to terminate, thecluster retries from the start including the full original grace period. An example flow: You use the kubectl tool to manually delete a specific Pod, with the default grace period(30 seconds).The Pod in the API server is updated with the time beyond which the Pod is considered ""dead""along with the grace period.If you use kubectl describe to check on the Pod you're deleting, that Pod shows up as""Terminating"".On the node where the Pod is running: as soon as the kubelet sees that a Pod has been markedas terminating (a graceful shutdown duration has been set), the kubelet begins the local Podshutdown process.If one of the Pod's containers has defined a preStophook, the kubeletruns that hook inside of the container. If the preStop hook is still running after thegrace period expires, the kubelet requests a small, one-off grace period extension of 2seconds.Note: If the preStop hook needs longer to complete than the default grace period allows,you must modify terminationGracePeriodSeconds to suit this.The kubelet triggers the container runtime to send a TERM signal to process 1 inside eachcontainer.Note: The containers in the Pod receive the TERM signal at different times and in an arbitraryorder. If the order of shutdowns matters, consider using a preStop hook to synchronize.At the same time as the kubelet is starting graceful shutdown, the control plane removes thatshutting-down Pod from EndpointSlice (and Endpoints) objects where these representa Service with a configuredselector.ReplicaSets and other workload resourcesno longer treat the shutting-down Pod as a valid, in-service replica. Pods that shut down slowlycannot continue to serve traffic as load balancers (like the service proxy) remove the Pod fromthe list of endpoints as soon as the termination grace period begins.When the grace period expires, the kubelet triggers forcible shutdown. The container runtime sendsSIGKILL to any processes still running in any container in the Pod.The kubelet also cleans up a hidden pause container if that container runtime uses one.The kubelet triggers forcible removal of Pod object from the API server, by setting grace periodto 0 (immediate deletion).The API server deletes the Pod's API object, which is then no longer visible from any client.",725
5.1.1 - Pod Lifecycle,Forced Pod termination,"Forced Pod termination Caution: Forced deletions can be potentially disruptive for some workloads and their Pods. By default, all deletes are graceful within 30 seconds. The kubectl delete command supportsthe --grace-period=<seconds> option which allows you to override the default and specify yourown value. Setting the grace period to 0 forcibly and immediately deletes the Pod from the APIserver. If the pod was still running on a node, that forcible deletion triggers the kubelet tobegin immediate cleanup. Note: You must specify an additional flag --force along with --grace-period=0 in order to perform force deletions. When a force deletion is performed, the API server does not wait for confirmationfrom the kubelet that the Pod has been terminated on the node it was running on. Itremoves the Pod in the API immediately so a new Pod can be created with the samename. On the node, Pods that are set to terminate immediately will still be givena small grace period before being force killed. Caution: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. If you need to force-delete Pods that are part of a StatefulSet, refer to the taskdocumentation fordeleting Pods from a StatefulSet.",283
5.1.1 - Pod Lifecycle,Garbage collection of Pods,"Garbage collection of Pods For failed Pods, the API objects remain in the cluster's API until a human orcontroller processexplicitly removes them. The Pod garbage collector (PodGC), which is a controller in the control plane, cleans up terminated Pods (with a phase of Succeeded orFailed), when the number of Pods exceeds the configured threshold(determined by terminated-pod-gc-threshold in the kube-controller-manager).This avoids a resource leak as Pods are created and terminated over time. Additionally, PodGC cleans up any Pods which satisfy any of the following conditions: are orphan pods - bound to a node which no longer exists,are unscheduled terminating pods,are terminating pods, bound to a non-ready node tainted with node.kubernetes.io/out-of-service, when the NodeOutOfServiceVolumeDetach feature gate is enabled. When the PodDisruptionConditions feature gate is enabled, along withcleaning up the pods, PodGC will also mark them as failed if they are in a non-terminalphase. Also, PodGC adds a pod disruption condition when cleaning up an orphanpod (see also:Pod disruption conditions). Get hands-on experienceattaching handlers to container lifecycle events.Get hands-on experienceconfiguring Liveness, Readiness and Startup Probes.Learn more about container lifecycle hooks.For detailed information about Pod and container status in the API, seethe API reference documentation covering.status for Pod.",317
5.1.2 - Init Containers,default,This page provides an overview of init containers: specialized containers that runbefore app containers in a Pod.Init containers can contain utilities or setup scripts not present in an app image. You can specify init containers in the Pod specification alongside the containersarray (which describes app containers).,54
5.1.2 - Init Containers,Understanding init containers,"Understanding init containers A Pod can have multiple containersrunning apps within it, but it can also have one or more init containers, which are runbefore the app containers are started. Init containers are exactly like regular containers, except: Init containers always run to completion.Each init container must complete successfully before the next one starts. If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.However, if the Pod has a restartPolicy of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed. To specify an init container for a Pod, add the initContainers field intothe Pod specification,as an array of container items (similar to the app containers field and its contents).See Container in theAPI reference for more details. The status of the init containers is returned in .status.initContainerStatusesfield as an array of the container statuses (similar to the .status.containerStatusesfield).",204
5.1.2 - Init Containers,Differences from regular containers,"Differences from regular containers Init containers support all the fields and features of app containers,including resource limits, volumes, and security settings. However, theresource requests and limits for an init container are handled differently,as documented in Resources. Also, init containers do not support lifecycle, livenessProbe, readinessProbe, orstartupProbe because they must run to completion before the Pod can be ready. If you specify multiple init containers for a Pod, kubelet runs each initcontainer sequentially. Each init container must succeed before the next can run.When all of the init containers have run to completion, kubelet initializesthe application containers for the Pod and runs them as usual.",144
5.1.2 - Init Containers,Using init containers,"Using init containers Because init containers have separate images from app containers, theyhave some advantages for start-up related code: Init containers can contain utilities or custom code for setup that are not present in an appimage. For example, there is no need to make an image FROM another image just to use a tool likesed, awk, python, or dig during setup.The application image builder and deployer roles can work independently withoutthe need to jointly build a single app image.Init containers can run with a different view of the filesystem than app containers in thesame Pod. Consequently, they can be given access toSecrets that app containers cannot access.Because init containers run to completion before any app containers start, init containers offera mechanism to block or delay app container startup until a set of preconditions are met. Oncepreconditions are met, all of the app containers in a Pod can start in parallel.Init containers can securely run utilities or custom code that would otherwise make an appcontainer image less secure. By keeping unnecessary tools separate you can limit the attacksurface of your app container image.",224
5.1.2 - Init Containers,Examples,"Examples Here are some ideas for how to use init containers: Wait for a Service tobe created, using a shell one-line command like:for i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1Register this Pod with a remote server from the downward API with a command like:curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$(<POD_NAME>)&ip=$(<POD_IP>)'Wait for some time before starting the app container with a command likesleep 60Clone a Git repository into a VolumePlace values into a configuration file and run a template tool to dynamicallygenerate a configuration file for the main app container. For example,place the POD_IP value in a configuration and generate the main appconfiguration file using Jinja.",196
5.1.2 - Init Containers,Init containers in use,"Init containers in use This example defines a simple Pod that has two init containers.The first waits for myservice, and the second waits for mydb. Once bothinit containers complete, the Pod runs the app container from its spec section. apiVersion: v1kind: Podmetadata:  name: myapp-pod  labels:    app.kubernetes.io/name: MyAppspec:  containers:  - name: myapp-container    image: busybox:1.28    command: ['sh', '-c', 'echo The app is running! && sleep 3600']  initContainers:  - name: init-myservice    image: busybox:1.28    command: ['sh', '-c', ""until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done""]  - name: init-mydb    image: busybox:1.28    command: ['sh', '-c', ""until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done""] You can start this Pod by running: kubectl apply -f myapp.yaml The output is similar to this: pod/myapp-pod created And check on its status with: kubectl get -f myapp.yaml The output is similar to this: NAME        READY     STATUS     RESTARTS   AGEmyapp-pod   0/1       Init:0/2   0          6m or for more details: kubectl describe -f myapp.yaml The output is similar to this: Name:          myapp-podNamespace:     default[...]Labels:        app.kubernetes.io/name=MyAppStatus:        Pending[...]Init Containers:  init-myservice:[...]    State:         Running[...]  init-mydb:[...]    State:         Waiting      Reason:      PodInitializing    Ready:         False[...]Containers:  myapp-container:[...]    State:         Waiting      Reason:      PodInitializing    Ready:         False[...]Events:  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message  ---------    --------    -----    ----                      -------------                           --------      ------        -------  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image ""busybox""  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image ""busybox""  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container init-myservice  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container init-myservice To see logs for the init containers in this Pod, run: kubectl logs myapp-pod -c init-myservice # Inspect the first init containerkubectl logs myapp-pod -c init-mydb      # Inspect the second init container At this point, those init containers will be waiting to discover Services namedmydb and myservice. Here's a configuration you can use to make those Services appear: ---apiVersion: v1kind: Servicemetadata:  name: myservicespec:  ports:  - protocol: TCP    port: 80    targetPort: 9376---apiVersion: v1kind: Servicemetadata:  name: mydbspec:  ports:  - protocol: TCP    port: 80    targetPort: 9377 To create the mydb and myservice services: kubectl apply -f services.yaml The output is similar to this: service/myservice createdservice/mydb created You'll then see that those init containers complete, and that the myapp-podPod moves into the Running state: kubectl get -f myapp.yaml The output is similar to this: NAME        READY     STATUS    RESTARTS   AGEmyapp-pod   1/1       Running   0          9m This simple example should provide some inspiration for you to create your owninit containers. What's next contains a link to a more detailed example.",1093
5.1.2 - Init Containers,Detailed behavior,"Detailed behavior During Pod startup, the kubelet delays running init containers until the networkingand storage are ready. Then the kubelet runs the Pod's init containers in the orderthey appear in the Pod's spec. Each init container must exit successfully beforethe next container starts. If a container fails to start due to the runtime orexits with failure, it is retried according to the Pod restartPolicy. However,if the Pod restartPolicy is set to Always, the init containers userestartPolicy OnFailure. A Pod cannot be Ready until all init containers have succeeded. The ports on aninit container are not aggregated under a Service. A Pod that is initializingis in the Pending state but should have a condition Initialized set to false. If the Pod restarts, or is restarted, all init containersmust execute again. Changes to the init container spec are limited to the container image field.Altering an init container image field is equivalent to restarting the Pod. Because init containers can be restarted, retried, or re-executed, init containercode should be idempotent. In particular, code that writes to files on EmptyDirsshould be prepared for the possibility that an output file already exists. Init containers have all of the fields of an app container. However, Kubernetesprohibits readinessProbe from being used because init containers cannotdefine readiness distinct from completion. This is enforced during validation. Use activeDeadlineSeconds on the Pod to prevent init containers from failing forever.The active deadline includes init containers.However it is recommended to use activeDeadlineSeconds only if teams deploy their applicationas a Job, because activeDeadlineSeconds has an effect even after initContainer finished.The Pod which is already running correctly would be killed by activeDeadlineSeconds if you set. The name of each app and init container in a Pod must be unique; avalidation error is thrown for any container sharing a name with another.",408
5.1.2 - Init Containers,Resources,"Resources Given the ordering and execution for init containers, the following rulesfor resource usage apply: The highest of any particular resource request or limit defined on all initcontainers is the effective init request/limit. If any resource has noresource limit specified this is considered as the highest limit.The Pod's effective request/limit for a resource is the higher of:the sum of all app containers request/limit for a resourcethe effective init request/limit for a resourceScheduling is done based on effective requests/limits, which meansinit containers can reserve resources for initialization that are not usedduring the life of the Pod.The QoS (quality of service) tier of the Pod's effective QoS tier is theQoS tier for init containers and app containers alike. Quota and limits are applied based on the effective Pod request andlimit. Pod level control groups (cgroups) are based on the effective Pod request andlimit, the same as the scheduler.",198
5.1.2 - Init Containers,Pod restart reasons,"Pod restart reasons A Pod can restart, causing re-execution of init containers, for the followingreasons: The Pod infrastructure container is restarted. This is uncommon and wouldhave to be done by someone with root access to nodes.All containers in a Pod are terminated while restartPolicy is set to Always,forcing a restart, and the init container completion record has been lost dueto garbage collection. The Pod will not be restarted when the init container image is changed, or theinit container completion record has been lost due to garbage collection. Thisapplies for Kubernetes v1.20 and later. If you are using an earlier version ofKubernetes, consult the documentation for the version you are using. Read about creating a Pod that has an init containerLearn how to debug init containers",164
5.1.3 - Disruptions,default,"This guide is for application owners who want to buildhighly available applications, and thus need to understandwhat types of disruptions can happen to Pods. It is also for cluster administrators who want to perform automatedcluster actions, like upgrading and autoscaling clusters.",52
5.1.3 - Disruptions,Voluntary and involuntary disruptions,"Voluntary and involuntary disruptions Pods do not disappear until someone (a person or a controller) destroys them, orthere is an unavoidable hardware or system software error. We call these unavoidable cases involuntary disruptions toan application. Examples are: a hardware failure of the physical machine backing the nodecluster administrator deletes VM (instance) by mistakecloud provider or hypervisor failure makes VM disappeara kernel panicthe node disappears from the cluster due to cluster network partitioneviction of a pod due to the node being out-of-resources. Except for the out-of-resources condition, all these conditionsshould be familiar to most users; they are not specificto Kubernetes. We call other cases voluntary disruptions. These include bothactions initiated by the application owner and those initiated by a ClusterAdministrator. Typical application owner actions include: deleting the deployment or other controller that manages the podupdating a deployment's pod template causing a restartdirectly deleting a pod (e.g. by accident) Cluster administrator actions include: Draining a node for repair or upgrade.Draining a node from a cluster to scale the cluster down (learn aboutCluster Autoscaling).Removing a pod from a node to permit something else to fit on that node. These actions might be taken directly by the cluster administrator, or by automationrun by the cluster administrator, or by your cluster hosting provider. Ask your cluster administrator or consult your cloud provider or distribution documentationto determine if any sources of voluntary disruptions are enabled for your cluster.If none are enabled, you can skip creating Pod Disruption Budgets. Caution: Not all voluntary disruptions are constrained by Pod Disruption Budgets. For example,deleting deployments or pods bypasses Pod Disruption Budgets.",357
5.1.3 - Disruptions,Dealing with disruptions,"Dealing with disruptions Here are some ways to mitigate involuntary disruptions: Ensure your pod requests the resources it needs.Replicate your application if you need higher availability. (Learn about running replicatedstatelessand stateful applications.)For even higher availability when running replicated applications,spread applications across racks (usinganti-affinity)or across zones (if using amulti-zone cluster.) The frequency of voluntary disruptions varies. On a basic Kubernetes cluster, there areno automated voluntary disruptions (only user-triggered ones). However, your cluster administrator or hosting providermay run some additional services which cause voluntary disruptions. For example,rolling out node software updates can cause voluntary disruptions. Also, some implementationsof cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.Your cluster administrator or hosting provider should have documented what level of voluntarydisruptions, if any, to expect. Certain configuration options, such asusing PriorityClassesin your pod spec can also cause voluntary (and involuntary) disruptions.",211
5.1.3 - Disruptions,Pod disruption budgets,"Pod disruption budgets FEATURE STATE: Kubernetes v1.21 [stable] Kubernetes offers features to help you run highly available applications even when youintroduce frequent voluntary disruptions. As an application owner, you can create a PodDisruptionBudget (PDB) for each application.A PDB limits the number of Pods of a replicated application that are down simultaneously fromvoluntary disruptions. For example, a quorum-based application wouldlike to ensure that the number of replicas running is never brought below thenumber needed for a quorum. A web front end might want toensure that the number of replicas serving load never falls below a certainpercentage of the total. Cluster managers and hosting providers should use tools whichrespect PodDisruptionBudgets by calling the Eviction APIinstead of directly deleting pods or deployments. For example, the kubectl drain subcommand lets you mark a node as going out ofservice. When you run kubectl drain, the tool tries to evict all of the Pods onthe Node you're taking out of service. The eviction request that kubectl submits onyour behalf may be temporarily rejected, so the tool periodically retries all failedrequests until all Pods on the target node are terminated, or until a configurable timeoutis reached. A PDB specifies the number of replicas that an application can tolerate having, relative to howmany it is intended to have. For example, a Deployment which has a .spec.replicas: 5 issupposed to have 5 pods at any given time. If its PDB allows for there to be 4 at a time,then the Eviction API will allow voluntary disruption of one (but not two) pods at a time. The group of pods that comprise the application is specified using a label selector, the sameas the one used by the application's controller (deployment, stateful-set, etc). The ""intended"" number of pods is computed from the .spec.replicas of the workload resourcethat is managing those pods. The control plane discovers the owning workload resource byexamining the .metadata.ownerReferences of the Pod. Involuntary disruptions cannot be prevented by PDBs; however theydo count against the budget. Pods which are deleted or unavailable due to a rolling upgrade to an application do countagainst the disruption budget, but workload resources (such as Deployment and StatefulSet)are not limited by PDBs when doing rolling upgrades. Instead, the handling of failuresduring application updates is configured in the spec for the specific workload resource. When a pod is evicted using the eviction API, it is gracefullyterminated, honoring theterminationGracePeriodSeconds setting in its PodSpec.",572
5.1.3 - Disruptions,PodDisruptionBudget example,"PodDisruptionBudget example Consider a cluster with 3 nodes, node-1 through node-3.The cluster is running several applications. One of them has 3 replicas initially calledpod-a, pod-b, and pod-c. Another, unrelated pod without a PDB, called pod-x, is also shown.Initially, the pods are laid out as follows: node-1node-2node-3pod-a availablepod-b availablepod-c availablepod-x available All 3 pods are part of a deployment, and they collectively have a PDB which requiresthere be at least 2 of the 3 pods to be available at all times. For example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.The cluster administrator first tries to drain node-1 using the kubectl drain command.That tool tries to evict pod-a and pod-x. This succeeds immediately.Both pods go into the terminating state at the same time.This puts the cluster in this state: node-1 drainingnode-2node-3pod-a terminatingpod-b availablepod-c availablepod-x terminating The deployment notices that one of the pods is terminating, so it creates a replacementcalled pod-d. Since node-1 is cordoned, it lands on another node. Something hasalso created pod-y as a replacement for pod-x. (Note: for a StatefulSet, pod-a, which would be called something like pod-0, would needto terminate completely before its replacement, which is also called pod-0 but has adifferent UID, could be created. Otherwise, the example applies to a StatefulSet as well.) Now the cluster is in this state: node-1 drainingnode-2node-3pod-a terminatingpod-b availablepod-c availablepod-x terminatingpod-d startingpod-y At some point, the pods terminate, and the cluster looks like this: node-1 drainednode-2node-3pod-b availablepod-c availablepod-d startingpod-y At this point, if an impatient cluster administrator tries to drain node-2 ornode-3, the drain command will block, because there are only 2 availablepods for the deployment, and its PDB requires at least 2. After some time passes, pod-d becomes available. The cluster state now looks like this: node-1 drainednode-2node-3pod-b availablepod-c availablepod-d availablepod-y Now, the cluster administrator tries to drain node-2.The drain command will try to evict the two pods in some order, saypod-b first and then pod-d. It will succeed at evicting pod-b.But, when it tries to evict pod-d, it will be refused because that would leave onlyone pod available for the deployment. The deployment creates a replacement for pod-b called pod-e.Because there are not enough resources in the cluster to schedulepod-e the drain will again block. The cluster may end up in thisstate: node-1 drainednode-2node-3no nodepod-b terminatingpod-c availablepod-e pendingpod-d availablepod-y At this point, the cluster administrator needs toadd a node back to the cluster to proceed with the upgrade. You can see how Kubernetes varies the rate at which disruptionscan happen, according to: how many replicas an application needshow long it takes to gracefully shutdown an instancehow long it takes a new instance to start upthe type of controllerthe cluster's resource capacity",757
5.1.3 - Disruptions,Pod disruption conditions,"Pod disruption conditions FEATURE STATE: Kubernetes v1.26 [beta] Note: If you are using an older version of Kubernetes than 1.26please refer to the corresponding version of the documentation. Note: In order to use this behavior, you must have the PodDisruptionConditionsfeature gateenabled in your cluster. When enabled, a dedicated Pod DisruptionTarget condition is added to indicatethat the Pod is about to be deleted due to a disruption.The reason field of the condition additionallyindicates one of the following reasons for the Pod termination: PreemptionByKubeSchedulerPod is due to be preempted by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see Pod priority preemption.DeletionByTaintManagerPod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within kube-controller-manager) due to a NoExecute taint that the Pod does not tolerate; see taint-based evictions.EvictionByEvictionAPIPod has been marked for eviction using the Kubernetes API .DeletionByPodGCPod, that is bound to a no longer existing Node, is due to be deleted by Pod garbage collection.TerminationByKubeletPod has been terminated by the kubelet, because of either node pressure eviction or the graceful node shutdown. Note: A Pod disruption might be interrupted. The control plane might re-attempt tocontinue the disruption of the same Pod, but it is not guaranteed. As a result,the DisruptionTarget condition might be added to a Pod, but that Pod might then not actually bedeleted. In such a situation, after some time, thePod disruption condition will be cleared. When the PodDisruptionConditions feature gate is enabled,along with cleaning up the pods, the Pod garbage collector (PodGC) will also mark them as failed if they are in a non-terminalphase (see also Pod garbage collection). When using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job'sPod failure policy.",450
5.1.3 - Disruptions,Separating Cluster Owner and Application Owner Roles,"Separating Cluster Owner and Application Owner Roles Often, it is useful to think of the Cluster Managerand Application Owner as separate roles with limited knowledgeof each other. This separation of responsibilitiesmay make sense in these scenarios: when there are many application teams sharing a Kubernetes cluster, andthere is natural specialization of roleswhen third-party tools or services are used to automate cluster management Pod Disruption Budgets support this separation of roles by providing aninterface between the roles. If you do not have such a separation of responsibilities in your organization,you may not need to use Pod Disruption Budgets.",123
5.1.3 - Disruptions,How to perform Disruptive Actions on your Cluster,"How to perform Disruptive Actions on your Cluster If you are a Cluster Administrator, and you need to perform a disruptive action on allthe nodes in your cluster, such as a node or system software upgrade, here are some options: Accept downtime during the upgrade.Failover to another complete replica cluster.No downtime, but may be costly both for the duplicated nodesand for human effort to orchestrate the switchover.Write disruption tolerant applications and use PDBs.No downtime.Minimal resource duplication.Allows more automation of cluster administration.Writing disruption-tolerant applications is tricky, but the work to tolerate voluntarydisruptions largely overlaps with work to support autoscaling and toleratinginvoluntary disruptions. Follow steps to protect your application by configuring a Pod Disruption Budget.Learn more about draining nodesLearn about updating a deploymentincluding steps to maintain its availability during the rollout.",182
5.1.4 - Ephemeral Containers,default,FEATURE STATE: Kubernetes v1.25 [stable] This page provides an overview of ephemeral containers: a special type of containerthat runs temporarily in an existing Pod toaccomplish user-initiated actions such as troubleshooting. You use ephemeralcontainers to inspect services rather than to build applications.,69
5.1.4 - Ephemeral Containers,Understanding ephemeral containers,"Understanding ephemeral containers Pods are the fundamental buildingblock of Kubernetes applications. Since Pods are intended to be disposable andreplaceable, you cannot add a container to a Pod once it has been created.Instead, you usually delete and replace Pods in a controlled fashion usingdeployments. Sometimes it's necessary to inspect the state of an existing Pod, however, forexample to troubleshoot a hard-to-reproduce bug. In these cases you can runan ephemeral container in an existing Pod to inspect its state and runarbitrary commands.",121
5.1.4 - Ephemeral Containers,What is an ephemeral container?,"What is an ephemeral container? Ephemeral containers differ from other containers in that they lack guaranteesfor resources or execution, and they will never be automatically restarted, sothey are not appropriate for building applications. Ephemeral containers aredescribed using the same ContainerSpec as regular containers, but many fieldsare incompatible and disallowed for ephemeral containers. Ephemeral containers may not have ports, so fields such as ports,livenessProbe, readinessProbe are disallowed.Pod resource allocations are immutable, so setting resources is disallowed.For a complete list of allowed fields, see the EphemeralContainer referencedocumentation. Ephemeral containers are created using a special ephemeralcontainers handlerin the API rather than by adding them directly to pod.spec, so it's notpossible to add an ephemeral container using kubectl edit. Like regular containers, you may not change or remove an ephemeral containerafter you have added it to a Pod. Note: Ephemeral containers are not supported by static pods.",218
5.1.4 - Ephemeral Containers,Uses for ephemeral containers,"Uses for ephemeral containers Ephemeral containers are useful for interactive troubleshooting when kubectl exec is insufficient because a container has crashed or a container imagedoesn't include debugging utilities. In particular, distroless imagesenable you to deploy minimal container images that reduce attack surfaceand exposure to bugs and vulnerabilities. Since distroless images do not include ashell or any debugging utilities, it's difficult to troubleshoot distrolessimages using kubectl exec alone. When using ephemeral containers, it's helpful to enable process namespacesharing soyou can view processes in other containers. Learn how to debug pods using ephemeral containers.",138
5.1.5 - Pod Quality of Service Classes,default,"This page introduces Quality of Service (QoS) classes in Kubernetes, and explainshow Kubernetes assigns a QoS class to each Pods as a consequence of the resourceconstraints that you specify for the containers in that Pod. Kubernetes relies on thisclassification to make decisions about which Pods to evict when there are not enoughavailable resources on a Node.",81
5.1.5 - Pod Quality of Service Classes,Quality of Service classes,"Quality of Service classes Kubernetes classifies the Pods that you run and allocates each Pod into a specificquality of service (QoS) class. Kubernetes uses that classification to influence how differentpods are handled. Kubernetes does this classification based on theresource requestsof the Containers in that Pod, along withhow those requests relate to resource limits.This is known as Quality of Service(QoS) class. Kubernetes assigns every Pod a QoS class based on the resource requestsand limits of its component Containers. QoS classes are used by Kubernetes to decidewhich Pods to evict from a Node experiencingNode Pressure. The possibleQoS classes are Guaranteed, Burstable, and BestEffort. When a Node runs out of resources,Kubernetes will first evict BestEffort Pods running on that Node, followed by Burstable andfinally Guaranteed Pods. When this eviction is due to resource pressure, only Pods exceedingresource requests are candidates for eviction.",220
5.1.5 - Pod Quality of Service Classes,Guaranteed,Guaranteed Pods that are Guaranteed have the strictest resource limits and are least likelyto face eviction. They are guaranteed not to be killed until they exceed their limitsor there are no lower-priority Pods that can be preempted from the Node. They maynot acquire resources beyond their specified limits. These Pods can also makeuse of exclusive CPUs using thestatic CPU management policy.,80
5.1.5 - Pod Quality of Service Classes,Criteria,"Criteria For a Pod to be given a QoS class of Guaranteed: Every Container in the Pod must have a memory limit and a memory request.For every Container in the Pod, the memory limit must equal the memory request.Every Container in the Pod must have a CPU limit and a CPU request.For every Container in the Pod, the CPU limit must equal the CPU request. Criteria A Pod is given a QoS class of Burstable if: The Pod does not meet the criteria for QoS class Guaranteed.At least one Container in the Pod has a memory or CPU request or limit. Criteria A Pod has a QoS class of BestEffort if it doesn't meet the criteria for either Guaranteedor Burstable. In other words, a Pod is BestEffort only if none of the Containers in the Pod have amemory limit or a memory request, and none of the Containers in the Pod have aCPU limit or a CPU request.Containers in a Pod can request other resources (not CPU or memory) and still be classified asBestEffort.",226
5.1.5 - Pod Quality of Service Classes,Burstable,"Burstable Pods that are Burstable have some lower-bound resource guarantees based on the request, butdo not require a specific limit. If a limit is not specified, it defaults to alimit equivalent to the capacity of the Node, which allows the Pods to flexibly increasetheir resources if resources are available. In the event of Pod eviction due to Noderesource pressure, these Pods are evicted only after all BestEffort Pods are evicted.Because a Burstable Pod can include a Container that has no resource limits or requests, a Podthat is Burstable can try to use any amount of node resources.",131
5.1.5 - Pod Quality of Service Classes,BestEffort,"BestEffort Pods in the BestEffort QoS class can use node resources that aren't specifically assignedto Pods in other QoS classes. For example, if you have a node with 16 CPU cores available to thekubelet, and you assign assign 4 CPU cores to a Guaranteed Pod, then a Pod in the BestEffortQoS class can try to use any amount of the remaining 12 CPU cores. The kubelet prefers to evict BestEffort Pods if the node comes under resource pressure.",111
5.1.5 - Pod Quality of Service Classes,Some behavior is independent of QoS class,"Some behavior is independent of QoS class Certain behavior is independent of the QoS class assigned by Kubernetes. For example: Any Container exceeding a resource limit will be killed and restarted by the kubelet withoutaffecting other Containers in that Pod.If a Container exceeds its resource request and the node it runs on facesresource pressure, the Pod it is in becomes a candidate for eviction.If this occurs, all Containers in the Pod will be terminated. Kubernetes may create areplacement Pod, usually on a different node.The resource request of a Pod is equal to the sum of the resource requests ofits component Containers, and the resource limit of a Pod is equal to the sum ofthe resource limits of its component Containers.The kube-scheduler does not consider QoS class when selecting which Pods topreempt.Preemption can occur when a cluster does not have enough resources to run all the Podsyou defined. Learn about resource management for Pods and Containers.Learn about Node-pressure eviction.Learn about Pod priority and preemption.Learn about Pod disruptions.Learn how to assign memory resources to containers and pods.Learn how to assign CPU resources to containers and pods.Learn how to configure Quality of Service for Pods.",264
5.1.6 - User Namespaces,default,"FEATURE STATE: Kubernetes v1.25 [alpha] This page explains how user namespaces are used in Kubernetes pods. A usernamespace allows to isolate the user running inside the container from the onein the host. A process running as root in a container can run as a different (non-root) userin the host; in other words, the process has full privileges for operationsinside the user namespace, but is unprivileged for operations outside thenamespace. You can use this feature to reduce the damage a compromised container can do tothe host or other pods in the same node. There are several securityvulnerabilities rated either HIGH or CRITICAL that were notexploitable when user namespaces is active. It is expected user namespace willmitigate some future vulnerabilities too.",168
5.1.6 - User Namespaces,Before you begin,"Before you begin 🛇 This item links to a third party project or product that is not part of Kubernetes itself. More information This is a Linux only feature. In addition, support is needed in thecontainer runtimeto use this feature with Kubernetes stateless pods: CRI-O: v1.25 has support for user namespaces.containerd: support is planned for the 1.7 release. See containerdissue #7063 for more details. Support for this in cri-dockerd is not planned yet.",114
5.1.6 - User Namespaces,Introduction,"Introduction User namespaces is a Linux feature that allows to map users in the container todifferent users in the host. Furthermore, the capabilities granted to a pod ina user namespace are valid only in the namespace and void outside of it. A pod can opt-in to use user namespaces by setting the pod.spec.hostUsers fieldto false. The kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a wayto guarantee that no two stateless pods on the same node use the same mapping. The runAsUser, runAsGroup, fsGroup, etc. fields in the pod.spec alwaysrefer to the user inside the container. The valid UIDs/GIDs when this feature is enabled is the range 0-65535. Thisapplies to files and processes (runAsUser, runAsGroup, etc.). Files using a UID/GID outside this range will be seen as belonging to theoverflow ID, usually 65534 (configured in /proc/sys/kernel/overflowuid and/proc/sys/kernel/overflowgid). However, it is not possible to modify thosefiles, even by running as the 65534 user/group. Most applications that need to run as root but don't access other hostnamespaces or resources, should continue to run fine without any changes neededif user namespaces is activated.",289
5.1.6 - User Namespaces,Understanding user namespaces for stateless pods,"Understanding user namespaces for stateless pods Several container runtimes with their default configuration (like Docker Engine,containerd, CRI-O) use Linux namespaces for isolation. Other technologies existand can be used with those runtimes too (e.g. Kata Containers uses VMs instead ofLinux namespaces). This page is applicable for container runtimes using Linuxnamespaces for isolation. When creating a pod, by default, several new namespaces are used for isolation:a network namespace to isolate the network of the container, a PID namespace toisolate the view of processes, etc. If a user namespace is used, this willisolate the users in the container from the users in the node. This means containers can run as root and be mapped to a non-root user on thehost. Inside the container the process will think it is running as root (andtherefore tools like apt, yum, etc. work fine), while in reality the processdoesn't have privileges on the host. You can verify this, for example, if youcheck which user the container process is running by executing ps aux fromthe host. The user ps shows is not the same as the user you see if youexecute inside the container the command id. This abstraction limits what can happen, for example, if the container managesto escape to the host. Given that the container is running as a non-privilegeduser on the host, it is limited what it can do to the host. Furthermore, as users on each pod will be mapped to different non-overlappingusers in the host, it is limited what they can do to other pods too. Capabilities granted to a pod are also limited to the pod user namespace andmostly invalid out of it, some are even completely void. Here are two examples: CAP_SYS_MODULE does not have any effect if granted to a pod using usernamespaces, the pod isn't able to load kernel modules.CAP_SYS_ADMIN is limited to the pod's user namespace and invalid outsideof it. Without using a user namespace a container running as root, in the case of acontainer breakout, has root privileges on the node. And if some capability weregranted to the container, the capabilities are valid on the host too. None ofthis is true when we use user namespaces. If you want to know more details about what changes when user namespaces are inuse, see man 7 user_namespaces.",509
5.1.6 - User Namespaces,Set up a node to support user namespaces,"Set up a node to support user namespaces It is recommended that the host's files and host's processes use UIDs/GIDs inthe range of 0-65535. The kubelet will assign UIDs/GIDs higher than that to pods. Therefore, toguarantee as much isolation as possible, the UIDs/GIDs used by the host's filesand host's processes should be in the range 0-65535. Note that this recommendation is important to mitigate the impact of CVEs likeCVE-2021-25741, where a pod can potentially read arbitraryfiles in the hosts. If the UIDs/GIDs of the pod and the host don't overlap, itis limited what a pod would be able to do: the pod UID/GID won't match thehost's file owner/group.",171
5.1.6 - User Namespaces,Limitations,"Limitations When using a user namespace for the pod, it is disallowed to use other hostnamespaces. In particular, if you set hostUsers: false then you are notallowed to set any of: hostNetwork: truehostIPC: truehostPID: true The pod is allowed to use no volumes at all or, if using volumes, only thesevolume types are allowed: configmapsecretprojecteddownwardAPIemptyDir To guarantee that the pod can read the files of such volumes, volumes arecreated as if you specified .spec.securityContext.fsGroup as 0 for the Pod.If it is specified to a different value, this other value will of course behonored instead. As a by-product of this, folders and files for these volumes will havepermissions for the group, even if defaultMode or mode to specific items ofthe volumes were specified without permissions to groups. For example, it is notpossible to mount these volumes in a way that its files have permissions onlyfor the owner.",209
5.1.7 - Downward API,default,"There are two ways to expose Pod and container fields to a running container: environment variables, and as files that are populated by a special volume type. Together, these two ways of exposing Pod and container fields are called the downward API. It is sometimes useful for a container to have information about itself, withoutbeing overly coupled to Kubernetes. The downward API allows containers to consumeinformation about themselves or the cluster without using the Kubernetes clientor API server. An example is an existing application that assumes a particular well-knownenvironment variable holds a unique identifier. One possibility is to wrap theapplication, but that is tedious and error-prone, and it violates the goal of lowcoupling. A better option would be to use the Pod's name as an identifier, andinject the Pod's name into the well-known environment variable. In Kubernetes, there are two ways to expose Pod and container fields to a running container: as environment variablesas files in a downwardAPI volume Together, these two ways of exposing Pod and container fields are called thedownward API.",222
5.1.7 - Downward API,Available fields,"Available fields Only some Kubernetes API fields are available through the downward API. Thissection lists which fields you can make available. You can pass information from available Pod-level fields using fieldRef.At the API level, the spec for a Pod always defines at least oneContainer.You can pass information from available Container-level fields usingresourceFieldRef.",73
5.1.7 - Downward API,Information available via fieldRef,"Information available via fieldRef For most Pod-level fields, you can provide them to a container either asan environment variable or using a downwardAPI volume. The fields availablevia either mechanism are: metadata.namethe pod's namemetadata.namespacethe pod's namespacemetadata.uidthe pod's unique IDmetadata.annotations['<KEY>']the value of the pod's annotation named <KEY> (for example, metadata.annotations['myannotation'])metadata.labels['<KEY>']the text value of the pod's label named <KEY> (for example, metadata.labels['mylabel']) The following information is available through environment variablesbut not as a downwardAPI volume fieldRef: spec.serviceAccountNamethe name of the pod's service accountspec.nodeNamethe name of the node where the Pod is executingstatus.hostIPthe primary IP address of the node to which the Pod is assignedstatus.podIPthe pod's primary IP address (usually, its IPv4 address) The following information is available through a downwardAPI volumefieldRef, but not as environment variables: metadata.labelsall of the pod's labels, formatted as label-key=""escaped-label-value"" with one label per linemetadata.annotationsall of the pod's annotations, formatted as annotation-key=""escaped-annotation-value"" with one annotation per line",297
5.1.7 - Downward API,Information available via resourceFieldRef,Information available via resourceFieldRef These container-level fields allow you to provide information aboutrequests and limitsfor resources such as CPU and memory. resource: limits.cpuA container's CPU limitresource: requests.cpuA container's CPU requestresource: limits.memoryA container's memory limitresource: requests.memoryA container's memory requestresource: limits.hugepages-*A container's hugepages limit (provided that the DownwardAPIHugePages feature gate is enabled)resource: requests.hugepages-*A container's hugepages request (provided that the DownwardAPIHugePages feature gate is enabled)resource: limits.ephemeral-storageA container's ephemeral-storage limitresource: requests.ephemeral-storageA container's ephemeral-storage request,163
5.1.7 - Downward API,Fallback information for resource limits,"Fallback information for resource limits If CPU and memory limits are not specified for a container, and you use thedownward API to try to expose that information, then thekubelet defaults to exposing the maximum allocatable value for CPU and memorybased on the node allocatablecalculation. You can read about downwardAPI volumes. You can try using the downward API to expose container- or Pod-level information: as environment variablesas files in downwardAPI volume",92
5.2.1 - Deployments,default,"A Deployment provides declarative updates for Pods andReplicaSets. You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Note: Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.",110
5.2.1 - Deployments,Use Case,Use Case The following are typical use cases for Deployments: Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.Rollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.Scale up the Deployment to facilitate more load.Pause the rollout of a Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.Use the status of the Deployment as an indicator that a rollout has stuck.Clean up older ReplicaSets that you don't need anymore.,209
5.2.1 - Deployments,Creating a Deployment,"Creating a Deployment The following is an example of a Deployment. It creates a ReplicaSet to bring up three nginx Pods: controllers/nginx-deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deployment  labels:    app: nginxspec:  replicas: 3  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:1.14.2        ports:        - containerPort: 80 In this example: A Deployment named nginx-deployment is created, indicated by the.metadata.name field. This name will become the basis for the ReplicaSetsand Pods which are created later. See Writing a Deployment Specfor more details.The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the .spec.replicas field.The .spec.selector field defines how the created ReplicaSet finds which Pods to manage.In this case, you select a label that is defined in the Pod template (app: nginx).However, more sophisticated selection rules are possible,as long as the Pod template itself satisfies the rule.Note: The .spec.selector.matchLabels field is a map of {key,value} pairs.A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions,whose key field is ""key"", the operator is ""In"", and the values array contains only ""value"".All of the requirements, from both matchLabels and matchExpressions, must be satisfied in order to match.The template field contains the following sub-fields:The Pods are labeled app: nginxusing the .metadata.labels field.The Pod template's specification, or .template.spec field, indicates thatthe Pods run one container, nginx, which runs the nginxDocker Hub image at version 1.14.2.Create one container and name it nginx using the .spec.template.spec.containers[0].name field. Before you begin, make sure your Kubernetes cluster is up and running.Follow the steps given below to create the above Deployment: Create the Deployment by running the following command:kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yamlRun kubectl get deployments to check if the Deployment was created.If the Deployment is still being created, the output is similar to the following:NAME               READY   UP-TO-DATE   AVAILABLE   AGEnginx-deployment   0/3     0            0           1sWhen you inspect the Deployments in your cluster, the following fields are displayed:NAME lists the names of the Deployments in the namespace.READY displays how many replicas of the application are available to your users. It follows the pattern ready/desired.UP-TO-DATE displays the number of replicas that have been updated to achieve the desired state.AVAILABLE displays how many replicas of the application are available to your users.AGE displays the amount of time that the application has been running.Notice how the number of desired replicas is 3 according to .spec.replicas field.To see the Deployment rollout status, run kubectl rollout status deployment/nginx-deployment.The output is similar to:Waiting for rollout to finish: 2 out of 3 new replicas have been updated...deployment ""nginx-deployment"" successfully rolled outRun the kubectl get deployments again a few seconds later.The output is similar to this:NAME               READY   UP-TO-DATE   AVAILABLE   AGEnginx-deployment   3/3     3            3           18sNotice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.To see the ReplicaSet (rs) created by the Deployment, run kubectl get rs. The output is similar to this:NAME                          DESIRED   CURRENT   READY   AGEnginx-deployment-75675f5897   3         3         3       18sReplicaSet output shows the following fields:NAME lists the names of the ReplicaSets in the namespace.DESIRED displays the desired number of replicas of the application, which you define when you create the Deployment. This is the desired state.CURRENT displays how many replicas are currently running.READY displays how many replicas of the application are available to your users.AGE displays the amount of time that the application has been running.Notice that the name of the ReplicaSet is always formatted as[DEPLOYMENT-NAME]-[HASH]. This name will become the basis for the Podswhich are created.The HASH string is the same as the pod-template-hash label on the ReplicaSet.To see the labels automatically generated for each Pod, run kubectl get pods --show-labels.The output is similar to:NAME                                READY     STATUS    RESTARTS   AGE       LABELSnginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453The created ReplicaSet ensures that there are three nginx Pods. Note:You must specify an appropriate selector and Pod template labels in a Deployment(in this case, app: nginx).Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.",1366
5.2.1 - Deployments,Pod-template-hash label,"Pod-template-hash label Caution: Do not change this label. The pod-template-hash label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts. This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the PodTemplate of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,and in any existing Pods that the ReplicaSet might have.",110
5.2.1 - Deployments,Updating a Deployment,"Updating a Deployment Note: A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, .spec.template)is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout. Follow the steps given below to update your Deployment: Let's update the nginx Pods to use the nginx:1.16.1 image instead of the nginx:1.14.2 image.kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1or use the following command:kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1The output is similar to:deployment.apps/nginx-deployment image updatedAlternatively, you can edit the Deployment and change .spec.template.spec.containers[0].image from nginx:1.14.2 to nginx:1.16.1:kubectl edit deployment/nginx-deploymentThe output is similar to:deployment.apps/nginx-deployment editedTo see the rollout status, run:kubectl rollout status deployment/nginx-deploymentThe output is similar to this:Waiting for rollout to finish: 2 out of 3 new replicas have been updated...ordeployment ""nginx-deployment"" successfully rolled out Get more details on your updated Deployment: After the rollout succeeds, you can view the Deployment by running kubectl get deployments.The output is similar to this:NAME               READY   UP-TO-DATE   AVAILABLE   AGEnginx-deployment   3/3     3            3           36sRun kubectl get rs to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling itup to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.kubectl get rsThe output is similar to this:NAME                          DESIRED   CURRENT   READY   AGEnginx-deployment-1564180365   3         3         3       6snginx-deployment-2035384211   0         0         0       36sRunning get pods should now show only the new Pods:kubectl get podsThe output is similar to this:NAME                                READY     STATUS    RESTARTS   AGEnginx-deployment-1564180365-khku8   1/1       Running   0          14snginx-deployment-1564180365-nacti   1/1       Running   0          14snginx-deployment-1564180365-z9gth   1/1       Running   0          14sNext time you want to update these Pods, you only need to update the Deployment's Pod template again.Deployment ensures that only a certain number of Pods are down while they are being updated. By default,it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).Deployment also ensures that only a certain number of Pods are created above the desired number of Pods.By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).For example, if you look at the above Deployment closely, you will see that it first creates a new Pod,then deletes an old Pod, and creates another new one. It does not kill old Pods until a sufficient number ofnew Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.It makes sure that at least 3 Pods are available and that at max 4 Pods in total are available. In case ofa Deployment with 4 replicas, the number of Pods would be between 3 and 5.Get details of your Deployment:kubectl describe deploymentsThe output is similar to this:Name:                   nginx-deploymentNamespace:              defaultCreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000Labels:                 app=nginxAnnotations:            deployment.kubernetes.io/revision=2Selector:               app=nginxReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailableStrategyType:           RollingUpdateMinReadySeconds:        0RollingUpdateStrategy:  25% max unavailable, 25% max surgePod Template:  Labels:  app=nginx   Containers:    nginx:      Image:        nginx:1.16.1      Port:         80/TCP      Environment:  <none>      Mounts:       <none>    Volumes:        <none>  Conditions:    Type           Status  Reason    ----           ------  ------    Available      True    MinimumReplicasAvailable    Progressing    True    NewReplicaSetAvailable  OldReplicaSets:  <none>  NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)  Events:    Type    Reason             Age   From                   Message    ----    ------             ----  ----                   -------    Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3    Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3    Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)and scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet(nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSetto 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times.It then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy.Finally, you'll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0. Note: Kubernetes doesn't count terminating Pods when calculating the number of availableReplicas, which must be betweenreplicas - maxUnavailable and replicas + maxSurge. As a result, you might notice that there are more Pods thanexpected during a rollout, and that the total resources consumed by the Deployment is more than replicas + maxSurgeuntil the terminationGracePeriodSeconds of the terminating Pods expires.",1666
5.2.1 - Deployments,Rollover (aka multiple updates in-flight),"Rollover (aka multiple updates in-flight) Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring upthe desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labelsmatch .spec.selector but whose template does not match .spec.template are scaled down. Eventually, the newReplicaSet is scaled to .spec.replicas and all old ReplicaSets is scaled to 0. If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSetas per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously-- it will add it to its list of old ReplicaSets and start scaling it down. For example, suppose you create a Deployment to create 5 replicas of nginx:1.14.2,but then update the Deployment to create 5 replicas of nginx:1.16.1, when only 3replicas of nginx:1.14.2 had been created. In that case, the Deployment immediately startskilling the 3 nginx:1.14.2 Pods that it had created, and starts creatingnginx:1.16.1 Pods. It does not wait for the 5 replicas of nginx:1.14.2 to be createdbefore changing course.",298
5.2.1 - Deployments,Label selector updates,"Label selector updates It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.In any case, if you need to perform a label selector update, exercise great caution and make sure you have graspedall of the implications. Note: In API version apps/v1, a Deployment's label selector is immutable after it gets created. Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too,otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector doesnot select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets andcreating a new ReplicaSet.Selector updates changes the existing value in a selector key -- result in the same behavior as additions.Selector removals removes an existing key from the Deployment selector -- do not require any changes in thePod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that theremoved label still exists in any existing Pods and ReplicaSets.",244
5.2.1 - Deployments,Rolling Back a Deployment,"Rolling Back a Deployment Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want(you can change that by modifying revision history limit). Note: A Deployment's revision is created when a Deployment's rollout is triggered. This means that thenew revision is created if and only if the Deployment's Pod template (.spec.template) is changed,for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.This means that when you roll back to an earlier revision, only the Deployment's Pod template part isrolled back. Suppose that you made a typo while updating the Deployment, by putting the image name as nginx:1.161 instead of nginx:1.16.1:kubectl set image deployment/nginx-deployment nginx=nginx:1.161 The output is similar to this:deployment.apps/nginx-deployment image updatedThe rollout gets stuck. You can verify it by checking the rollout status:kubectl rollout status deployment/nginx-deploymentThe output is similar to this:Waiting for rollout to finish: 1 out of 3 new replicas have been updated...Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,read more here.You see that the number of old replicas (nginx-deployment-1564180365 and nginx-deployment-2035384211) is 2, and new replicas (nginx-deployment-3066724191) is 1.kubectl get rsThe output is similar to this:NAME                          DESIRED   CURRENT   READY   AGEnginx-deployment-1564180365   3         3         3       25snginx-deployment-2035384211   0         0         0       36snginx-deployment-3066724191   1         1         0       6sLooking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.kubectl get podsThe output is similar to this:NAME                                READY     STATUS             RESTARTS   AGEnginx-deployment-1564180365-70iae   1/1       Running            0          25snginx-deployment-1564180365-jbqqo   1/1       Running            0          25snginx-deployment-1564180365-hysrc   1/1       Running            0          25snginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6sNote: The Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (maxUnavailable specifically) that you have specified. Kubernetes by default sets the value to 25%.Get the description of the Deployment:kubectl describe deploymentThe output is similar to this:Name:           nginx-deploymentNamespace:      defaultCreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700Labels:         app=nginxSelector:       app=nginxReplicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailableStrategyType:       RollingUpdateMinReadySeconds:    0RollingUpdateStrategy:  25% max unavailable, 25% max surgePod Template:  Labels:  app=nginx  Containers:   nginx:    Image:        nginx:1.161    Port:         80/TCP    Host Port:    0/TCP    Environment:  <none>    Mounts:       <none>  Volumes:        <none>Conditions:  Type           Status  Reason  ----           ------  ------  Available      True    MinimumReplicasAvailable  Progressing    True    ReplicaSetUpdatedOldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)Events:  FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message  --------- --------    -----   ----                    -------------   --------    ------              -------  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1To fix this, you need to rollback to a previous revision of Deployment that is stable.",1378
5.2.1 - Deployments,Checking Rollout History of a Deployment,"Checking Rollout History of a Deployment Follow the steps given below to check the rollout history: First, check the revisions of this Deployment:kubectl rollout history deployment/nginx-deploymentThe output is similar to this:deployments ""nginx-deployment""REVISION    CHANGE-CAUSE1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.13           kubectl set image deployment/nginx-deployment nginx=nginx:1.161CHANGE-CAUSE is copied from the Deployment annotation kubernetes.io/change-cause to its revisions upon creation. You can specify theCHANGE-CAUSE message by:Annotating the Deployment with kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=""image updated to 1.16.1""Manually editing the manifest of the resource.To see the details of each revision, run:kubectl rollout history deployment/nginx-deployment --revision=2The output is similar to this:deployments ""nginx-deployment"" revision 2  Labels:       app=nginx          pod-template-hash=1159050644  Annotations:  kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1  Containers:   nginx:    Image:      nginx:1.16.1    Port:       80/TCP     QoS Tier:        cpu:      BestEffort        memory:   BestEffort    Environment Variables:      <none>  No volumes.",416
5.2.1 - Deployments,Rolling Back to a Previous Revision,"Rolling Back to a Previous Revision Follow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2. Now you've decided to undo the current rollout and rollback to the previous revision:kubectl rollout undo deployment/nginx-deploymentThe output is similar to this:deployment.apps/nginx-deployment rolled backAlternatively, you can rollback to a specific revision by specifying it with --to-revision:kubectl rollout undo deployment/nginx-deployment --to-revision=2The output is similar to this:deployment.apps/nginx-deployment rolled backFor more details about rollout related commands, read kubectl rollout.The Deployment is now rolled back to a previous stable revision. As you can see, a DeploymentRollback eventfor rolling back to revision 2 is generated from Deployment controller.Check if the rollback was successful and the Deployment is running as expected, run:kubectl get deployment nginx-deploymentThe output is similar to this:NAME               READY   UP-TO-DATE   AVAILABLE   AGEnginx-deployment   3/3     3            3           30mGet the description of the Deployment:kubectl describe deployment nginx-deploymentThe output is similar to this:Name:                   nginx-deploymentNamespace:              defaultCreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500Labels:                 app=nginxAnnotations:            deployment.kubernetes.io/revision=4                        kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1Selector:               app=nginxReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailableStrategyType:           RollingUpdateMinReadySeconds:        0RollingUpdateStrategy:  25% max unavailable, 25% max surgePod Template:  Labels:  app=nginx  Containers:   nginx:    Image:        nginx:1.16.1    Port:         80/TCP    Host Port:    0/TCP    Environment:  <none>    Mounts:       <none>  Volumes:        <none>Conditions:  Type           Status  Reason  ----           ------  ------  Available      True    MinimumReplicasAvailable  Progressing    True    NewReplicaSetAvailableOldReplicaSets:  <none>NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)Events:  Type    Reason              Age   From                   Message  ----    ------              ----  ----                   -------  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment ""nginx-deployment"" to revision 2  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0",962
5.2.1 - Deployments,Scaling a Deployment,"Scaling a Deployment You can scale a Deployment by using the following command: kubectl scale deployment/nginx-deployment --replicas=10 The output is similar to this: deployment.apps/nginx-deployment scaled Assuming horizontal Pod autoscaling is enabledin your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number ofPods you want to run based on the CPU utilization of your existing Pods. kubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80 The output is similar to this: deployment.apps/nginx-deployment scaled",149
5.2.1 - Deployments,Proportional scaling,"Proportional scaling RollingUpdate Deployments support running multiple versions of an application at the same time. When youor an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progressor paused), the Deployment controller balances the additional replicas in the existing activeReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called proportional scaling. For example, you are running a Deployment with 10 replicas, maxSurge=3, and maxUnavailable=2. Ensure that the 10 replicas in your Deployment are running.kubectl get deployThe output is similar to this:NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEnginx-deployment     10        10        10           10          50sYou update to a new image which happens to be unresolvable from inside the cluster.kubectl set image deployment/nginx-deployment nginx=nginx:sometagThe output is similar to this:deployment.apps/nginx-deployment image updatedThe image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to themaxUnavailable requirement that you mentioned above. Check out the rollout status:kubectl get rsThe output is similar to this:NAME                          DESIRED   CURRENT   READY     AGEnginx-deployment-1989198191   5         5         0         9snginx-deployment-618515232    8         8         8         1mThen a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicasto 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't usingproportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, youspread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with themost replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to theReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up. In our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to thenew ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assumingthe new replicas become healthy. To confirm this, run: kubectl get deploy The output is similar to this: NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEnginx-deployment     15        18        7            8           7m The rollout status confirms how the replicas were added to each ReplicaSet. kubectl get rs The output is similar to this: NAME                          DESIRED   CURRENT   READY     AGEnginx-deployment-1989198191   7         7         0         7mnginx-deployment-618515232    11        11        11        7m",678
5.2.1 - Deployments,Pausing and Resuming a rollout of a Deployment,"Pausing and Resuming a rollout of a Deployment When you update a Deployment, or plan to, you can pause rolloutsfor that Deployment before you trigger one or more updates. Whenyou're ready to apply those changes, you resume rollouts for theDeployment. This approach allows you toapply multiple fixes in between pausing and resuming without triggering unnecessary rollouts. For example, with a Deployment that was created:Get the Deployment details:kubectl get deployThe output is similar to this:NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEnginx     3         3         3            3           1mGet the rollout status:kubectl get rsThe output is similar to this:NAME               DESIRED   CURRENT   READY     AGEnginx-2142116321   3         3         3         1mPause by running the following command:kubectl rollout pause deployment/nginx-deploymentThe output is similar to this:deployment.apps/nginx-deployment pausedThen update the image of the Deployment:kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1The output is similar to this:deployment.apps/nginx-deployment image updatedNotice that no new rollout started:kubectl rollout history deployment/nginx-deploymentThe output is similar to this:deployments ""nginx""REVISION  CHANGE-CAUSE1   <none>Get the rollout status to verify that the existing ReplicaSet has not changed:kubectl get rsThe output is similar to this:NAME               DESIRED   CURRENT   READY     AGEnginx-2142116321   3         3         3         2mYou can make as many updates as you wish, for example, update the resources that will be used:kubectl set resources deployment/nginx-deployment -c=nginx --limits=cpu=200m,memory=512MiThe output is similar to this:deployment.apps/nginx-deployment resource requirements updatedThe initial state of the Deployment prior to pausing its rollout will continue its function, but new updates tothe Deployment will not have any effect as long as the Deployment rollout is paused.Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:kubectl rollout resume deployment/nginx-deploymentThe output is similar to this:deployment.apps/nginx-deployment resumedWatch the status of the rollout until it's done.kubectl get rs -wThe output is similar to this:NAME               DESIRED   CURRENT   READY     AGEnginx-2142116321   2         2         2         2mnginx-3926361531   2         2         0         6snginx-3926361531   2         2         1         18snginx-2142116321   1         2         2         2mnginx-2142116321   1         2         2         2mnginx-3926361531   3         2         1         18snginx-3926361531   3         2         1         18snginx-2142116321   1         1         1         2mnginx-3926361531   3         3         1         18snginx-3926361531   3         3         2         19snginx-2142116321   0         1         1         2mnginx-2142116321   0         1         1         2mnginx-2142116321   0         0         0         2mnginx-3926361531   3         3         3         20sGet the status of the latest rollout:kubectl get rsThe output is similar to this:NAME               DESIRED   CURRENT   READY     AGEnginx-2142116321   0         0         0         2mnginx-3926361531   3         3         3         28s Note: You cannot rollback a paused Deployment until you resume it.",910
5.2.1 - Deployments,Deployment status,"Deployment status A Deployment enters various states during its lifecycle. It can be progressing whilerolling out a new ReplicaSet, it can be complete, or it can fail to progress.",41
5.2.1 - Deployments,Progressing Deployment,"Progressing Deployment Kubernetes marks a Deployment as progressing when one of the following tasks is performed: The Deployment creates a new ReplicaSet.The Deployment is scaling up its newest ReplicaSet.The Deployment is scaling down its older ReplicaSet(s).New Pods become ready or available (ready for at least MinReadySeconds). When the rollout becomes “progressing”, the Deployment controller adds a condition with the followingattributes to the Deployment's .status.conditions: type: Progressingstatus: ""True""reason: NewReplicaSetCreated | reason: FoundNewReplicaSet | reason: ReplicaSetUpdated You can monitor the progress for a Deployment by using kubectl rollout status.",159
5.2.1 - Deployments,Complete Deployment,"Complete Deployment Kubernetes marks a Deployment as complete when it has the following characteristics: All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning anyupdates you've requested have been completed.All of the replicas associated with the Deployment are available.No old replicas for the Deployment are running. When the rollout becomes “complete”, the Deployment controller sets a condition with the followingattributes to the Deployment's .status.conditions: type: Progressingstatus: ""True""reason: NewReplicaSetAvailable This Progressing condition will retain a status value of ""True"" until a new rolloutis initiated. The condition holds even when availability of replicas changes (whichdoes instead affect the Available condition). You can check if a Deployment has completed by using kubectl rollout status. If the rollout completedsuccessfully, kubectl rollout status returns a zero exit code. kubectl rollout status deployment/nginx-deployment The output is similar to this: Waiting for rollout to finish: 2 of 3 updated replicas are available...deployment ""nginx-deployment"" successfully rolled out and the exit status from kubectl rollout is 0 (success): echo $? 0",269
5.2.1 - Deployments,Failed Deployment,"Failed Deployment Your Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occurdue to some of the following factors: Insufficient quotaReadiness probe failuresImage pull errorsInsufficient permissionsLimit rangesApplication runtime misconfiguration One way you can detect this condition is to specify a deadline parameter in your Deployment spec:(.spec.progressDeadlineSeconds). .spec.progressDeadlineSeconds denotes thenumber of seconds the Deployment controller waits before indicating (in the Deployment status) that theDeployment progress has stalled. The following kubectl command sets the spec with progressDeadlineSeconds to make the controller reportlack of progress of a rollout for a Deployment after 10 minutes: kubectl patch deployment/nginx-deployment -p '{""spec"":{""progressDeadlineSeconds"":600}}' The output is similar to this: deployment.apps/nginx-deployment patched Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the followingattributes to the Deployment's .status.conditions: type: Progressingstatus: ""False""reason: ProgressDeadlineExceeded This condition can also fail early and is then set to status value of ""False"" due to reasons as ReplicaSetCreateError.Also, the deadline is not taken into account anymore once the Deployment rollout completes. See the Kubernetes API conventions for more information on status conditions. Note: Kubernetes takes no action on a stalled Deployment other than to report a status condition withreason: ProgressDeadlineExceeded. Higher level orchestrators can take advantage of it and act accordingly, forexample, rollback the Deployment to its previous version. Note: If you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline.You can safely pause a Deployment rollout in the middle of a rollout and resume without triggeringthe condition for exceeding the deadline. You may experience transient errors with your Deployments, either due to a low timeout that you have set ordue to any other kind of error that can be treated as transient. For example, let's suppose you haveinsufficient quota. If you describe the Deployment you will notice the following section: kubectl describe deployment nginx-deployment The output is similar to this: <...>Conditions:  Type            Status  Reason  ----            ------  ------  Available       True    MinimumReplicasAvailable  Progressing     True    ReplicaSetUpdated  ReplicaFailure  True    FailedCreate<...> If you run kubectl get deployment nginx-deployment -o yaml, the Deployment status is similar to this: status:  availableReplicas: 2  conditions:  - lastTransitionTime: 2016-10-04T12:25:39Z    lastUpdateTime: 2016-10-04T12:25:39Z    message: Replica set ""nginx-deployment-4262182780"" is progressing.    reason: ReplicaSetUpdated    status: ""True""    type: Progressing  - lastTransitionTime: 2016-10-04T12:25:42Z    lastUpdateTime: 2016-10-04T12:25:42Z    message: Deployment has minimum availability.    reason: MinimumReplicasAvailable    status: ""True""    type: Available  - lastTransitionTime: 2016-10-04T12:25:39Z    lastUpdateTime: 2016-10-04T12:25:39Z    message: 'Error creating: pods ""nginx-deployment-4262182780-"" is forbidden: exceeded quota:      object-counts, requested: pods=1, used: pods=3, limited: pods=2'    reason: FailedCreate    status: ""True""    type: ReplicaFailure  observedGeneration: 3  replicas: 2  unavailableReplicas: 2 Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and thereason for the Progressing condition: Conditions:  Type            Status  Reason  ----            ------  ------  Available       True    MinimumReplicasAvailable  Progressing     False   ProgressDeadlineExceeded  ReplicaFailure  True    FailedCreate You can address an issue of insufficient quota by scaling down your Deployment, by scaling down othercontrollers you may be running, or by increasing quota in your namespace. If you satisfy the quotaconditions and the Deployment controller then completes the Deployment rollout, you'll see theDeployment's status update with a successful condition (status: ""True"" and reason: NewReplicaSetAvailable). Conditions:  Type          Status  Reason  ----          ------  ------  Available     True    MinimumReplicasAvailable  Progressing   True    NewReplicaSetAvailable type: Available with status: ""True"" means that your Deployment has minimum availability. Minimum availability is dictatedby the parameters specified in the deployment strategy. type: Progressing with status: ""True"" means that your Deploymentis either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimumrequired new replicas are available (see the Reason of the condition for the particulars - in our casereason: NewReplicaSetAvailable means that the Deployment is complete). You can check if a Deployment has failed to progress by using kubectl rollout status. kubectl rollout statusreturns a non-zero exit code if the Deployment has exceeded the progression deadline. kubectl rollout status deployment/nginx-deployment The output is similar to this: Waiting for rollout to finish: 2 out of 3 new replicas have been updated...error: deployment ""nginx"" exceeded its progress deadline and the exit status from kubectl rollout is 1 (indicating an error): echo $? 1",1246
5.2.1 - Deployments,Operating on a failed deployment,"Operating on a failed deployment All actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll backto a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.",56
5.2.1 - Deployments,Clean up Policy,"Clean up Policy You can set .spec.revisionHistoryLimit field in a Deployment to specify how many old ReplicaSets forthis Deployment you want to retain. The rest will be garbage-collected in the background. By default,it is 10. Note: Explicitly setting this field to 0, will result in cleaning up all the history of your Deploymentthus that Deployment will not be able to roll back.",89
5.2.1 - Deployments,Canary Deployment,"Canary Deployment If you want to roll out releases to a subset of users or servers using the Deployment, youcan create multiple Deployments, one for each release, following the canary pattern described inmanaging resources.",46
5.2.1 - Deployments,Writing a Deployment Spec,"Writing a Deployment Spec As with all other Kubernetes configs, a Deployment needs .apiVersion, .kind, and .metadata fields.For general information about working with config files, seedeploying applications,configuring containers, and using kubectl to manage resources documents. When the control plane creates new Pods for a Deployment, the .metadata.name of theDeployment is part of the basis for naming those Pods. The name of a Deployment must be a validDNS subdomainvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,the name should follow the more restrictive rules for aDNS label. A Deployment also needs a .spec section.",151
5.2.1 - Deployments,Pod Template,"Pod Template The .spec.template and .spec.selector are the only required fields of the .spec. The .spec.template is a Pod template. It has exactly the same schema as a Pod, except it is nested and does not have an apiVersion or kind. In addition to required fields for a Pod, a Pod template in a Deployment must specify appropriatelabels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See selector. Only a .spec.template.spec.restartPolicy equal to Always isallowed, which is the default if not specified.",125
5.2.1 - Deployments,Replicas,"Replicas .spec.replicas is an optional field that specifies the number of desired Pods. It defaults to 1. Should you manually scale a Deployment, example via kubectl scale deployment deployment --replicas=X, and then you update that Deployment based on a manifest(for example: by running kubectl apply -f deployment.yaml),then applying that manifest overwrites the manual scaling that you previously did. If a HorizontalPodAutoscaler (or anysimilar API for horizontal scaling) is managing scaling for a Deployment, don't set .spec.replicas. Instead, allow the Kubernetescontrol plane to manage the.spec.replicas field automatically.",147
5.2.1 - Deployments,Selector,"Selector .spec.selector is a required field that specifies a label selectorfor the Pods targeted by this Deployment. .spec.selector must match .spec.template.metadata.labels, or it will be rejected by the API. In API version apps/v1, .spec.selector and .metadata.labels do not default to .spec.template.metadata.labels if not set. So they must be set explicitly. Also note that .spec.selector is immutable after creation of the Deployment in apps/v1. A Deployment may terminate Pods whose labels match the selector if their template is differentfrom .spec.template or if the total number of such Pods exceeds .spec.replicas. It brings up newPods with .spec.template if the number of Pods is less than the desired number. Note: You should not create other Pods whose labels match this selector, either directly, by creatinganother Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If youdo so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this. If you have multiple controllers that have overlapping selectors, the controllers will fight with eachother and won't behave correctly.",274
5.2.1 - Deployments,Strategy,"Strategy .spec.strategy specifies the strategy used to replace old Pods by new ones..spec.strategy.type can be ""Recreate"" or ""RollingUpdate"". ""RollingUpdate"" isthe default value.",49
5.2.1 - Deployments,Recreate Deployment,"Recreate Deployment All existing Pods are killed before new ones are created when .spec.strategy.type==Recreate. Note: This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Podsof the old revision will be terminated immediately. Successful removal is awaited before any Pod of the newrevision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and thereplacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an""at most"" guarantee for your Pods, you should consider using aStatefulSet.",140
5.2.1 - Deployments,Rolling Update Deployment,"Rolling Update Deployment The Deployment updates Pods in a rolling updatefashion when .spec.strategy.type==RollingUpdate. You can specify maxUnavailable and maxSurge to controlthe rolling update process. Max Unavailable .spec.strategy.rollingUpdate.maxUnavailable is an optional field that specifies the maximum numberof Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage byrounding down. The value cannot be 0 if .spec.strategy.rollingUpdate.maxSurge is 0. The default value is 25%. For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desiredPods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaleddown further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods availableat all times during the update is at least 70% of the desired Pods. Max Surge .spec.strategy.rollingUpdate.maxSurge is an optional field that specifies the maximum number of Podsthat can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or apercentage of desired Pods (for example, 10%). The value cannot be 0 if MaxUnavailable is 0. The absolute numberis calculated from the percentage by rounding up. The default value is 25%. For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when therolling update starts, such that the total number of old and new Pods does not exceed 130% of desiredPods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that thetotal number of Pods running at any time during the update is at most 130% of desired Pods.",421
5.2.1 - Deployments,Progress Deadline Seconds,"Progress Deadline Seconds .spec.progressDeadlineSeconds is an optional field that specifies the number of seconds you wantto wait for your Deployment to progress before the system reports back that the Deployment hasfailed progressing - surfaced as a condition with type: Progressing, status: ""False"".and reason: ProgressDeadlineExceeded in the status of the resource. The Deployment controller will keepretrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deploymentcontroller will roll back a Deployment as soon as it observes such a condition. If specified, this field needs to be greater than .spec.minReadySeconds.",141
5.2.1 - Deployments,Min Ready Seconds,"Min Ready Seconds .spec.minReadySeconds is an optional field that specifies the minimum number of seconds for which a newlycreated Pod should be ready without any of its containers crashing, for it to be considered available.This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about whena Pod is considered ready, see Container Probes.",78
5.2.1 - Deployments,Revision History Limit,"Revision History Limit A Deployment's revision history is stored in the ReplicaSets it controls. .spec.revisionHistoryLimit is an optional field that specifies the number of old ReplicaSets to retainto allow rollback. These old ReplicaSets consume resources in etcd and crowd the output of kubectl get rs. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments. More specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.",187
5.2.1 - Deployments,Paused,"Paused .spec.paused is an optional boolean field for pausing and resuming a Deployment. The only difference betweena paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the pausedDeployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default whenit is created. Learn about Pods.Run a Stateless Application Using a Deployment.Deployment is a top-level resource in the Kubernetes REST API.Read theDeploymentobject definition to understand the API for deployments.Read about PodDisruptionBudget and howyou can use it to manage application availability during disruptions.",142
5.2.2 - ReplicaSet,default,"A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is oftenused to guarantee the availability of a specified number of identical Pods.",42
5.2.2 - ReplicaSet,How a ReplicaSet works,"How a ReplicaSet works A ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a numberof replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Podsit should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creatingand deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Podtemplate. A ReplicaSet is linked to its Pods via the Pods' metadata.ownerReferencesfield, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owningReplicaSet's identifying information within their ownerReferences field. It's through this link that the ReplicaSetknows of the state of the Pods it is maintaining and plans accordingly. A ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has noOwnerReference or the OwnerReference is not a Controller and itmatches a ReplicaSet's selector, it will be immediately acquired by said ReplicaSet.",245
5.2.2 - ReplicaSet,When to use a ReplicaSet,"When to use a ReplicaSet A ReplicaSet ensures that a specified number of pod replicas are running at any giventime. However, a Deployment is a higher-level concept that manages ReplicaSets andprovides declarative updates to Pods along with a lot of other useful features.Therefore, we recommend using Deployments instead of directly using ReplicaSets, unlessyou require custom update orchestration or don't require updates at all. This actually means that you may never need to manipulate ReplicaSet objects:use a Deployment instead, and define your application in the spec section.",126
5.2.2 - ReplicaSet,Example,"Example controllers/frontend.yamlapiVersion: apps/v1kind: ReplicaSetmetadata:  name: frontend  labels:    app: guestbook    tier: frontendspec:  # modify replicas according to your case  replicas: 3  selector:    matchLabels:      tier: frontend  template:    metadata:      labels:        tier: frontend    spec:      containers:      - name: php-redis        image: gcr.io/google_samples/gb-frontend:v3 Saving this manifest into frontend.yaml and submitting it to a Kubernetes cluster willcreate the defined ReplicaSet and the Pods that it manages. kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml You can then get the current ReplicaSets deployed: kubectl get rs And see the frontend one you created: NAME       DESIRED   CURRENT   READY   AGEfrontend   3         3         3       6s You can also check on the state of the ReplicaSet: kubectl describe rs/frontend And you will see output similar to: Name:         frontendNamespace:    defaultSelector:     tier=frontendLabels:       app=guestbook              tier=frontendAnnotations:  kubectl.kubernetes.io/last-applied-configuration:                {""apiVersion"":""apps/v1"",""kind"":""ReplicaSet"",""metadata"":{""annotations"":{},""labels"":{""app"":""guestbook"",""tier"":""frontend""},""name"":""frontend"",...Replicas:     3 current / 3 desiredPods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 FailedPod Template:  Labels:  tier=frontend  Containers:   php-redis:    Image:        gcr.io/google_samples/gb-frontend:v3    Port:         <none>    Host Port:    <none>    Environment:  <none>    Mounts:       <none>  Volumes:        <none>Events:  Type    Reason            Age   From                   Message  ----    ------            ----  ----                   -------  Normal  SuccessfulCreate  117s  replicaset-controller  Created pod: frontend-wtsmm  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-b2zdv  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-vcmts And lastly you can check for the Pods brought up: kubectl get pods You should see Pod information similar to: NAME             READY   STATUS    RESTARTS   AGEfrontend-b2zdv   1/1     Running   0          6m36sfrontend-vcmts   1/1     Running   0          6m36sfrontend-wtsmm   1/1     Running   0          6m36s You can also verify that the owner reference of these pods is set to the frontend ReplicaSet.To do this, get the yaml of one of the Pods running: kubectl get pods frontend-b2zdv -o yaml The output will look similar to this, with the frontend ReplicaSet's info set in the metadata's ownerReferences field: apiVersion: v1kind: Podmetadata:  creationTimestamp: ""2020-02-12T07:06:16Z""  generateName: frontend-  labels:    tier: frontend  name: frontend-b2zdv  namespace: default  ownerReferences:  - apiVersion: apps/v1    blockOwnerDeletion: true    controller: true    kind: ReplicaSet    name: frontend    uid: f391f6db-bb9b-4c09-ae74-6a1f77f3d5cf...",873
5.2.2 - ReplicaSet,Non-Template Pod acquisitions,"Non-Template Pod acquisitions While you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not havelabels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limitedto owning Pods specified by its template-- it can acquire other Pods in the manner specified in the previous sections. Take the previous frontend ReplicaSet example, and the Pods specified in the following manifest: pods/pod-rs.yamlapiVersion: v1kind: Podmetadata:  name: pod1  labels:    tier: frontendspec:  containers:  - name: hello1    image: gcr.io/google-samples/hello-app:2.0---apiVersion: v1kind: Podmetadata:  name: pod2  labels:    tier: frontendspec:  containers:  - name: hello2    image: gcr.io/google-samples/hello-app:1.0 As those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontendReplicaSet, they will immediately be acquired by it. Suppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas tofulfill its replica count requirement: kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml The new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be overits desired count. Fetching the Pods: kubectl get pods The output shows that the new Pods are either already terminated, or in the process of being terminated: NAME             READY   STATUS        RESTARTS   AGEfrontend-b2zdv   1/1     Running       0          10mfrontend-vcmts   1/1     Running       0          10mfrontend-wtsmm   1/1     Running       0          10mpod1             0/1     Terminating   0          1spod2             0/1     Terminating   0          1s If you create the Pods first: kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml And then create the ReplicaSet however: kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml You shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until thenumber of its new Pods and the original matches its desired count. As fetching the Pods: kubectl get pods Will reveal in its output: NAME             READY   STATUS    RESTARTS   AGEfrontend-hmmj2   1/1     Running   0          9spod1             1/1     Running   0          36spod2             1/1     Running   0          36s In this manner, a ReplicaSet can own a non-homogenous set of Pods",684
5.2.2 - ReplicaSet,Writing a ReplicaSet manifest,"Writing a ReplicaSet manifest As with all other Kubernetes API objects, a ReplicaSet needs the apiVersion, kind, and metadata fields.For ReplicaSets, the kind is always a ReplicaSet. When the control plane creates new Pods for a ReplicaSet, the .metadata.name of theReplicaSet is part of the basis for naming those Pods. The name of a ReplicaSet must be a validDNS subdomainvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,the name should follow the more restrictive rules for aDNS label. A ReplicaSet also needs a .spec section.",140
5.2.2 - ReplicaSet,Pod Template,"Pod Template The .spec.template is a pod template which is alsorequired to have labels in place. In our frontend.yaml example we had one label: tier: frontend.Be careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod. For the template's restart policy field,.spec.template.spec.restartPolicy, the only allowed value is Always, which is the default.",93
5.2.2 - ReplicaSet,Pod Selector,"Pod Selector The .spec.selector field is a label selector. As discussedearlier these are the labels used to identify potential Pods to acquire. In ourfrontend.yaml example, the selector was: matchLabels:  tier: frontend In the ReplicaSet, .spec.template.metadata.labels must match spec.selector, or it willbe rejected by the API. Note: For 2 ReplicaSets specifying the same .spec.selector but different.spec.template.metadata.labels and .spec.template.spec fields, each ReplicaSet ignores thePods created by the other ReplicaSet.",137
5.2.2 - ReplicaSet,Replicas,"Replicas You can specify how many Pods should run concurrently by setting .spec.replicas. The ReplicaSet will create/deleteits Pods to match this number. If you do not specify .spec.replicas, then it defaults to 1.",53
5.2.2 - ReplicaSet,Deleting a ReplicaSet and its Pods,"Deleting a ReplicaSet and its Pods To delete a ReplicaSet and all of its Pods, usekubectl delete. TheGarbage collector automatically deletes all ofthe dependent Pods by default. When using the REST API or the client-go library, you must set propagationPolicy toBackground or Foreground in the -d option. For example: kubectl proxy --port=8080curl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \  -d '{""kind"":""DeleteOptions"",""apiVersion"":""v1"",""propagationPolicy"":""Foreground""}' \  -H ""Content-Type: application/json""",161
5.2.2 - ReplicaSet,Deleting just a ReplicaSet,"Deleting just a ReplicaSet You can delete a ReplicaSet without affecting any of its Pods usingkubectl deletewith the --cascade=orphan option.When using the REST API or the client-go library, you must set propagationPolicy to Orphan.For example: kubectl proxy --port=8080curl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \  -d '{""kind"":""DeleteOptions"",""apiVersion"":""v1"",""propagationPolicy"":""Orphan""}' \  -H ""Content-Type: application/json"" Once the original is deleted, you can create a new ReplicaSet to replace it. As longas the old and new .spec.selector are the same, then the new one will adopt the old Pods.However, it will not make any effort to make existing Pods match a new, different pod template.To update Pods to a new spec in a controlled way, use aDeployment, asReplicaSets do not support a rolling update directly.",243
5.2.2 - ReplicaSet,Isolating Pods from a ReplicaSet,"Isolating Pods from a ReplicaSet You can remove Pods from a ReplicaSet by changing their labels. This technique may be used to remove Podsfrom service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).",69
5.2.2 - ReplicaSet,Scaling a ReplicaSet,"Scaling a ReplicaSet A ReplicaSet can be easily scaled up or down by simply updating the .spec.replicas field. The ReplicaSet controllerensures that a desired number of Pods with a matching label selector are available and operational. When scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods toprioritize scaling down pods based on the following general algorithm: Pending (and unschedulable) pods are scaled down firstIf controller.kubernetes.io/pod-deletion-cost annotation is set, thenthe pod with the lower value will come first.Pods on nodes with more replicas come before pods on nodes with fewer replicas.If the pods' creation times differ, the pod that was created more recentlycomes before the older pod (the creation times are bucketed on an integer log scalewhen the LogarithmicScaleDown feature gate is enabled) If all of the above match, then selection is random.",210
5.2.2 - ReplicaSet,Pod deletion cost,"Pod deletion cost FEATURE STATE: Kubernetes v1.22 [beta] Using the controller.kubernetes.io/pod-deletion-costannotation, users can set a preference regarding which pods to remove first when downscaling a ReplicaSet. The annotation should be set on the pod, the range is [-2147483647, 2147483647]. It represents the cost ofdeleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletioncost are preferred to be deleted before pods with higher deletion cost. The implicit value for this annotation for pods that don't set it is 0; negative values are permitted.Invalid values will be rejected by the API server. This feature is beta and enabled by default. You can disable it using thefeature gatePodDeletionCost in both kube-apiserver and kube-controller-manager. Note:This is honored on a best-effort basis, so it does not offer any guarantees on pod deletion order.Users should avoid updating the annotation frequently, such as updating it based on a metric value,because doing so will generate a significant number of pod updates on the apiserver.",253
5.2.2 - ReplicaSet,Example Use Case,"Example Use Case The different pods of an application could have different utilization levels. On scale down, the applicationmay prefer to remove the pods with lower utilization. To avoid frequently updating the pods, the applicationshould update controller.kubernetes.io/pod-deletion-cost once before issuing a scale down (setting theannotation to a value proportional to pod utilization level). This works if the application itself controlsthe down scaling; for example, the driver pod of a Spark deployment.",102
5.2.2 - ReplicaSet,ReplicaSet as a Horizontal Pod Autoscaler Target,"ReplicaSet as a Horizontal Pod Autoscaler Target A ReplicaSet can also be a target forHorizontal Pod Autoscalers (HPA). That is,a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targetingthe ReplicaSet we created in the previous example. controllers/hpa-rs.yamlapiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata:  name: frontend-scalerspec:  scaleTargetRef:    kind: ReplicaSet    name: frontend  minReplicas: 3  maxReplicas: 10  targetCPUUtilizationPercentage: 50 Saving this manifest into hpa-rs.yaml and submitting it to a Kubernetes cluster shouldcreate the defined HPA that autoscales the target ReplicaSet depending on the CPU usageof the replicated Pods. kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml Alternatively, you can use the kubectl autoscale command to accomplish the same(and it's easier!) kubectl autoscale rs frontend --max=10 --min=3 --cpu-percent=50",271
5.2.2 - ReplicaSet,Deployment (recommended),"Deployment (recommended) Deployment is an object which can own ReplicaSets and updatethem and their Pods via declarative, server-side rolling updates.While ReplicaSets can be used independently, today they're mainly used by Deployments as a mechanism to orchestrate Podcreation, deletion and updates. When you use Deployments you don't have to worry about managing the ReplicaSets thatthey create. Deployments own and manage their ReplicaSets.As such, it is recommended to use Deployments when you want ReplicaSets.",120
5.2.2 - ReplicaSet,Bare Pods,"Bare Pods Unlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted orterminated for any reason, such as in the case of node failure or disruptive node maintenance,such as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if yourapplication requires only a single Pod. Think of it similarly to a process supervisor, only itsupervises multiple Pods across multiple nodes instead of individual processes on a single node. AReplicaSet delegates local container restarts to some agent on the node such as Kubelet.",126
5.2.2 - ReplicaSet,DaemonSet,"DaemonSet Use a DaemonSet instead of a ReplicaSet for Pods that provide amachine-level function, such as machine monitoring or machine logging. These Pods have a lifetime that is tiedto a machine lifetime: the Pod needs to be running on the machine before other Pods start, and aresafe to terminate when the machine is otherwise ready to be rebooted/shutdown.",81
5.2.2 - ReplicaSet,ReplicationController,"ReplicationController ReplicaSets are the successors to ReplicationControllers.The two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-basedselector requirements as described in the labels user guide.As such, ReplicaSets are preferred over ReplicationControllers Learn about Pods.Learn about Deployments.Run a Stateless Application Using a Deployment,which relies on ReplicaSets to work.ReplicaSet is a top-level resource in the Kubernetes REST API.Read theReplicaSetobject definition to understand the API for replica sets.Read about PodDisruptionBudget and howyou can use it to manage application availability during disruptions.",147
5.2.3 - StatefulSets,default,"StatefulSet is the workload API object used to manage stateful applications. Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods. Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling. If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.",174
5.2.3 - StatefulSets,Using StatefulSets,"Using StatefulSets StatefulSets are valuable for applications that require one or more of thefollowing. Stable, unique network identifiers.Stable, persistent storage.Ordered, graceful deployment and scaling.Ordered, automated rolling updates. In the above, stable is synonymous with persistence across Pod (re)scheduling.If an application doesn't require any stable identifiers or ordered deployment,deletion, or scaling, you should deploy your application using a workload objectthat provides a set of stateless replicas.Deployment orReplicaSet may be better suited to your stateless needs.",124
5.2.3 - StatefulSets,Limitations,"Limitations The storage for a given Pod must either be provisioned by aPersistentVolume Provisionerbased on the requested storage class, or pre-provisioned by an admin.Deleting and/or scaling a StatefulSet down will not delete the volumes associated with theStatefulSet. This is done to ensure data safety, which is generally more valuable than anautomatic purge of all related StatefulSet resources.StatefulSets currently require a Headless Serviceto be responsible for the network identity of the Pods. You are responsible for creating thisService.StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet isdeleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it ispossible to scale the StatefulSet down to 0 prior to deletion.When using Rolling Updates with the defaultPod Management Policy (OrderedReady),it's possible to get into a broken state that requiresmanual intervention to repair.",204
5.2.3 - StatefulSets,Components,"Components The example below demonstrates the components of a StatefulSet. apiVersion: v1kind: Servicemetadata:  name: nginx  labels:    app: nginxspec:  ports:  - port: 80    name: web  clusterIP: None  selector:    app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata:  name: webspec:  selector:    matchLabels:      app: nginx # has to match .spec.template.metadata.labels  serviceName: ""nginx""  replicas: 3 # by default is 1  minReadySeconds: 10 # by default is 0  template:    metadata:      labels:        app: nginx # has to match .spec.selector.matchLabels    spec:      terminationGracePeriodSeconds: 10      containers:      - name: nginx        image: registry.k8s.io/nginx-slim:0.8        ports:        - containerPort: 80          name: web        volumeMounts:        - name: www          mountPath: /usr/share/nginx/html  volumeClaimTemplates:  - metadata:      name: www    spec:      accessModes: [ ""ReadWriteOnce"" ]      storageClassName: ""my-storage-class""      resources:        requests:          storage: 1Gi In the above example: A Headless Service, named nginx, is used to control the network domain.The StatefulSet, named web, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.The volumeClaimTemplates will provide stable storage usingPersistentVolumes provisioned by aPersistentVolume Provisioner. The name of a StatefulSet object must be a validDNS label.",392
5.2.3 - StatefulSets,Pod Selector,Pod Selector You must set the .spec.selector field of a StatefulSet to match the labels of its.spec.template.metadata.labels. Failing to specify a matching Pod Selector will result in avalidation error during StatefulSet creation.,56
5.2.3 - StatefulSets,Minimum ready seconds,"Minimum ready seconds FEATURE STATE: Kubernetes v1.25 [stable] .spec.minReadySeconds is an optional field that specifies the minimum number of seconds for which a newlycreated Pod should be running and ready without any of its containers crashing, for it to be considered available.This is used to check progression of a rollout when using a Rolling Update strategy.This field defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about whena Pod is considered ready, see Container Probes.",112
5.2.3 - StatefulSets,Pod Identity,"Pod Identity StatefulSet Pods have a unique identity that consists of an ordinal, astable network identity, and stable storage. The identity sticks to the Pod,regardless of which node it's (re)scheduled on.",49
5.2.3 - StatefulSets,Ordinal Index,"Ordinal Index For a StatefulSet with N replicas, each Pod in the StatefulSetwill be assigned an integer ordinal, that is unique over the Set. By default,pods will be assigned ordinals from 0 up through N-1.",53
5.2.3 - StatefulSets,Start ordinal,"Start ordinal FEATURE STATE: Kubernetes v1.26 [alpha] .spec.ordinals is an optional field that allows you to configure the integerordinals assigned to each Pod. It defaults to nil. You must enable theStatefulSetStartOrdinalfeature gate touse this field. Once enabled, you can configure the following options: .spec.ordinals.start: If the .spec.ordinals.start field is set, Pods willbe assigned ordinals from .spec.ordinals.start up through.spec.ordinals.start + .spec.replicas - 1.",127
5.2.3 - StatefulSets,Stable Network ID,"Stable Network ID Each Pod in a StatefulSet derives its hostname from the name of the StatefulSetand the ordinal of the Pod. The pattern for the constructed hostnameis $(statefulset name)-$(ordinal). The example above will create three Podsnamed web-0,web-1,web-2.A StatefulSet can use a Headless Serviceto control the domain of its Pods. The domain managed by this Service takes the form:$(service name).$(namespace).svc.cluster.local, where ""cluster.local"" is thecluster domain.As each Pod is created, it gets a matching DNS subdomain, taking the form:$(podname).$(governing service domain), where the governing service is definedby the serviceName field on the StatefulSet. Depending on how DNS is configured in your cluster, you may not be able to look up the DNSname for a newly-run Pod immediately. This behavior can occur when other clients in thecluster have already sent queries for the hostname of the Pod before it was created.Negative caching (normal in DNS) means that the results of previous failed lookups areremembered and reused, even after the Pod is running, for at least a few seconds. If you need to discover Pods promptly after they are created, you have a few options: Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.Decrease the time of caching in your Kubernetes DNS provider (typically this means editing theconfig map for CoreDNS, which currently caches for 30 seconds). As mentioned in the limitations section, you are responsible forcreating the Headless Serviceresponsible for the network identity of the pods. Here are some examples of choices for Cluster Domain, Service name,StatefulSet name, and how that affects the DNS names for the StatefulSet's Pods. Cluster DomainService (ns/name)StatefulSet (ns/name)StatefulSet DomainPod DNSPod Hostnamecluster.localdefault/nginxdefault/webnginx.default.svc.cluster.localweb-{0..N-1}.nginx.default.svc.cluster.localweb-{0..N-1}cluster.localfoo/nginxfoo/webnginx.foo.svc.cluster.localweb-{0..N-1}.nginx.foo.svc.cluster.localweb-{0..N-1}kube.localfoo/nginxfoo/webnginx.foo.svc.kube.localweb-{0..N-1}.nginx.foo.svc.kube.localweb-{0..N-1} Note: Cluster Domain will be set to cluster.local unlessotherwise configured.",615
5.2.3 - StatefulSets,Stable Storage,"Stable Storage For each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives onePersistentVolumeClaim. In the nginx example above, each Pod receives a single PersistentVolumewith a StorageClass of my-storage-class and 1 Gib of provisioned storage. If no StorageClassis specified, then the default StorageClass will be used. When a Pod is (re)scheduledonto a node, its volumeMounts mount the PersistentVolumes associated with itsPersistentVolume Claims. Note that, the PersistentVolumes associated with thePods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.This must be done manually.",147
5.2.3 - StatefulSets,Pod Name Label,"Pod Name Label When the StatefulSet controller creates a Pod,it adds a label, statefulset.kubernetes.io/pod-name, that is set to the name ofthe Pod. This label allows you to attach a Service to a specific Pod inthe StatefulSet.",61
5.2.3 - StatefulSets,Deployment and Scaling Guarantees,"Deployment and Scaling Guarantees For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.Before a Pod is terminated, all of its successors must be completely shutdown. The StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds of 0. This practiceis unsafe and strongly discouraged. For further explanation, please refer toforce deleting StatefulSet Pods. When the nginx example above is created, three Pods will be deployed in the orderweb-0, web-1, web-2. web-1 will not be deployed before web-0 isRunning and Ready, and web-2 will not be deployed untilweb-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but beforeweb-2 is launched, web-2 will not be launched until web-0 is successfully relaunched andbecomes Running and Ready. If a user were to scale the deployed example by patching the StatefulSet such thatreplicas=1, web-2 would be terminated first. web-1 would not be terminated until web-2is fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated andis completely shutdown, but prior to web-1's termination, web-1 would not be terminateduntil web-0 is Running and Ready.",345
5.2.3 - StatefulSets,Parallel Pod Management,"Parallel Pod Management Parallel pod management tells the StatefulSet controller to launch orterminate all Pods in parallel, and to not wait for Pods to become Runningand Ready or completely terminated prior to launching or terminating anotherPod. This option only affects the behavior for scaling operations. Updates are notaffected.",62
5.2.3 - StatefulSets,Update strategies,"Update strategies A StatefulSet's .spec.updateStrategy field allows you to configureand disable automated rolling updates for containers, labels, resource request/limits, andannotations for the Pods in a StatefulSet. There are two possible values: OnDeleteWhen a StatefulSet's .spec.updateStrategy.type is set to OnDelete,the StatefulSet controller will not automatically update the Pods in aStatefulSet. Users must manually delete Pods to cause the controller tocreate new Pods that reflect modifications made to a StatefulSet's .spec.template.RollingUpdateThe RollingUpdate update strategy implements automated, rolling updates for the Pods in aStatefulSet. This is the default update strategy.",152
5.2.3 - StatefulSets,Rolling Updates,"Rolling Updates When a StatefulSet's .spec.updateStrategy.type is set to RollingUpdate, theStatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceedin the same order as Pod termination (from the largest ordinal to the smallest), updatingeach Pod one at a time. The Kubernetes control plane waits until an updated Pod is Running and Ready priorto updating its predecessor. If you have set .spec.minReadySeconds (seeMinimum Ready Seconds), the control plane additionally waits thatamount of time after the Pod turns ready, before moving on.",127
5.2.3 - StatefulSets,Partitioned rolling updates,"Partitioned rolling updates The RollingUpdate update strategy can be partitioned, by specifying a.spec.updateStrategy.rollingUpdate.partition. If a partition is specified, all Pods with anordinal that is greater than or equal to the partition will be updated when the StatefulSet's.spec.template is updated. All Pods with an ordinal that is less than the partition will notbe updated, and, even if they are deleted, they will be recreated at the previous version. If aStatefulSet's .spec.updateStrategy.rollingUpdate.partition is greater than its .spec.replicas,updates to its .spec.template will not be propagated to its Pods.In most cases you will not need to use a partition, but they are useful if you want to stage anupdate, roll out a canary, or perform a phased roll out.",188
5.2.3 - StatefulSets,Maximum unavailable Pods,"Maximum unavailable Pods FEATURE STATE: Kubernetes v1.24 [alpha] You can control the maximum number of Pods that can be unavailable during an updateby specifying the .spec.updateStrategy.rollingUpdate.maxUnavailable field.The value can be an absolute number (for example, 5) or a percentage of desiredPods (for example, 10%). Absolute number is calculated from the percentage valueby rounding it up. This field cannot be 0. The default setting is 1. This field applies to all Pods in the range 0 to replicas - 1. If there is anyunavailable Pod in the range 0 to replicas - 1, it will be counted towardsmaxUnavailable. Note: The maxUnavailable field is in Alpha stage and it is honored only by API serversthat are running with the MaxUnavailableStatefulSetfeature gateenabled.",182
5.2.3 - StatefulSets,Forced rollback,"Forced rollback When using Rolling Updates with the defaultPod Management Policy (OrderedReady),it's possible to get into a broken state that requires manual intervention to repair. If you update the Pod template to a configuration that never becomes Running andReady (for example, due to a bad binary or application-level configuration error),StatefulSet will stop the rollout and wait. In this state, it's not enough to revert the Pod template to a good configuration.Due to a known issue,StatefulSet will continue to wait for the broken Pod to become Ready(which never happens) before it will attempt to revert it back to the workingconfiguration. After reverting the template, you must also delete any Pods that StatefulSet hadalready attempted to run with the bad configuration.StatefulSet will then begin to recreate the Pods using the reverted template.",177
5.2.3 - StatefulSets,PersistentVolumeClaim retention,"PersistentVolumeClaim retention FEATURE STATE: Kubernetes v1.23 [alpha] The optional .spec.persistentVolumeClaimRetentionPolicy field controls ifand how PVCs are deleted during the lifecycle of a StatefulSet. You must enable theStatefulSetAutoDeletePVC feature gateon the API server and the controller manager to use this field.Once enabled, there are two policies you can configure for each StatefulSet: whenDeletedconfigures the volume retention behavior that applies when the StatefulSet is deletedwhenScaledconfigures the volume retention behavior that applies when the replica count ofthe StatefulSet is reduced; for example, when scaling down the set. For each policy that you can configure, you can set the value to either Delete or Retain. DeleteThe PVCs created from the StatefulSet volumeClaimTemplate are deleted for each Podaffected by the policy. With the whenDeleted policy all PVCs from thevolumeClaimTemplate are deleted after their Pods have been deleted. With thewhenScaled policy, only PVCs corresponding to Pod replicas being scaled down aredeleted, after their Pods have been deleted.Retain (default)PVCs from the volumeClaimTemplate are not affected when their Pod isdeleted. This is the behavior before this new feature. Bear in mind that these policies only apply when Pods are being removed due to theStatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSetfails due to node failure, and the control plane creates a replacement Pod, the StatefulSetretains the existing PVC. The existing volume is unaffected, and the cluster will attach it tothe node where the new Pod is about to launch. The default for policies is Retain, matching the StatefulSet behavior before this new feature. Here is an example policy. apiVersion: apps/v1kind: StatefulSet...spec:  persistentVolumeClaimRetentionPolicy:    whenDeleted: Retain    whenScaled: Delete... The StatefulSet controller addsowner referencesto its PVCs, which are then deleted by the garbage collector after the Pod is terminated. This enables the Pod tocleanly unmount all volumes before the PVCs are deleted (and before the backing PV andvolume are deleted, depending on the retain policy). When you set the whenDeletedpolicy to Delete, an owner reference to the StatefulSet instance is placed on all PVCsassociated with that StatefulSet. The whenScaled policy must delete PVCs only when a Pod is scaled down, and not when aPod is deleted for another reason. When reconciling, the StatefulSet controller comparesits desired replica count to the actual Pods present on the cluster. Any StatefulSet Podwhose id greater than the replica count is condemned and marked for deletion. If thewhenScaled policy is Delete, the condemned Pods are first set as owners to theassociated StatefulSet template PVCs, before the Pod is deleted. This causes the PVCsto be garbage collected after only the condemned Pods have terminated. This means that if the controller crashes and restarts, no Pod will be deleted before itsowner reference has been updated appropriate to the policy. If a condemned Pod isforce-deleted while the controller is down, the owner reference may or may not have beenset up, depending on when the controller crashed. It may take several reconcile loops toupdate the owner references, so some condemned Pods may have set up owner references andothers may not. For this reason we recommend waiting for the controller to come back up,which will verify owner references before terminating Pods. If that is not possible, theoperator should verify the owner references on PVCs to ensure the expected objects aredeleted when Pods are force-deleted.",789
5.2.3 - StatefulSets,Replicas,"Replicas .spec.replicas is an optional field that specifies the number of desired Pods. It defaults to 1. Should you manually scale a deployment, example via kubectl scale statefulset statefulset --replicas=X, and then you update that StatefulSetbased on a manifest (for example: by running kubectl apply -f statefulset.yaml), then applying that manifest overwrites the manual scalingthat you previously did. If a HorizontalPodAutoscaler(or any similar API for horizontal scaling) is managing scaling for aStatefulset, don't set .spec.replicas. Instead, allow the Kubernetescontrol plane to managethe .spec.replicas field automatically. Learn about Pods.Find out how to use StatefulSetsFollow an example of deploying a stateful application.Follow an example of deploying Cassandra with Stateful Sets.Follow an example of running a replicated stateful application.Learn how to scale a StatefulSet.Learn what's involved when you delete a StatefulSet.Learn how to configure a Pod to use a volume for storage.Learn how to configure a Pod to use a PersistentVolume for storage.StatefulSet is a top-level resource in the Kubernetes REST API.Read theStatefulSetobject definition to understand the API for stateful sets.Read about PodDisruptionBudget and howyou can use it to manage application availability during disruptions.",303
5.2.4 - DaemonSet,default,"A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to thecluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbagecollected. Deleting a DaemonSet will clean up the Pods it created. Some typical uses of a DaemonSet are: running a cluster storage daemon on every noderunning a logs collection daemon on every noderunning a node monitoring daemon on every node In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.A more complex setup might use multiple DaemonSets for a single type of daemon, but withdifferent flags and/or different memory and cpu requests for different hardware types.",161
5.2.4 - DaemonSet,Create a DaemonSet,"Create a DaemonSet You can describe a DaemonSet in a YAML file. For example, the daemonset.yaml file belowdescribes a DaemonSet that runs the fluentd-elasticsearch Docker image: controllers/daemonset.yamlapiVersion: apps/v1kind: DaemonSetmetadata:  name: fluentd-elasticsearch  namespace: kube-system  labels:    k8s-app: fluentd-loggingspec:  selector:    matchLabels:      name: fluentd-elasticsearch  template:    metadata:      labels:        name: fluentd-elasticsearch    spec:      tolerations:      # these tolerations are to have the daemonset runnable on control plane nodes      # remove them if your control plane nodes should not run pods      - key: node-role.kubernetes.io/control-plane        operator: Exists        effect: NoSchedule      - key: node-role.kubernetes.io/master        operator: Exists        effect: NoSchedule      containers:      - name: fluentd-elasticsearch        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2        resources:          limits:            memory: 200Mi          requests:            cpu: 100m            memory: 200Mi        volumeMounts:        - name: varlog          mountPath: /var/log      terminationGracePeriodSeconds: 30      volumes:      - name: varlog        hostPath:          path: /var/log Create a DaemonSet based on the YAML file: kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml",391
5.2.4 - DaemonSet,Required Fields,"Required Fields As with all other Kubernetes config, a DaemonSet needs apiVersion, kind, and metadata fields. Forgeneral information about working with config files, seerunning stateless applicationsand object management using kubectl. The name of a DaemonSet object must be a validDNS subdomain name. A DaemonSet also needs a.specsection.",79
5.2.4 - DaemonSet,Pod Template,"Pod Template The .spec.template is one of the required fields in .spec. The .spec.template is a pod template.It has exactly the same schema as a Pod,except it is nested and does not have an apiVersion or kind. In addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriatelabels (see pod selector). A Pod Template in a DaemonSet must have a RestartPolicyequal to Always, or be unspecified, which defaults to Always.",106
5.2.4 - DaemonSet,Pod Selector,"Pod Selector The .spec.selector field is a pod selector. It works the same as the .spec.selector ofa Job. You must specify a pod selector that matches the labels of the.spec.template.Also, once a DaemonSet is created,its .spec.selector can not be mutated. Mutating the pod selector can lead to theunintentional orphaning of Pods, and it was found to be confusing to users. The .spec.selector is an object consisting of two fields: matchLabels - works the same as the .spec.selector of aReplicationController.matchExpressions - allows to build more sophisticated selectors by specifying key,list of values and an operator that relates the key and values. When the two are specified the result is ANDed. The .spec.selector must match the .spec.template.metadata.labels.Config with these two not matching will be rejected by the API.",202
5.2.4 - DaemonSet,Running Pods on select Nodes,"Running Pods on select Nodes If you specify a .spec.template.spec.nodeSelector, then the DaemonSet controller willcreate Pods on nodes which match that node selector.Likewise if you specify a .spec.template.spec.affinity,then DaemonSet controller will create Pods on nodes which match thatnode affinity.If you do not specify either, then the DaemonSet controller will create Pods on all nodes.",93
5.2.4 - DaemonSet,How Daemon Pods are scheduled,"How Daemon Pods are scheduled A DaemonSet ensures that all eligible nodes run a copy of a Pod. The DaemonSetcontroller creates a Pod for each eligible node and adds thespec.affinity.nodeAffinity field of the Pod to match the target host. Afterthe Pod is created, the default scheduler typically takes over and then bindsthe Pod to the target host by setting the .spec.nodeName field. If the newPod cannot fit on the node, the default scheduler may preempt (evict) some ofthe existing Pods based on thepriorityof the new Pod. The user can specify a different scheduler for the Pods of the DamonSet, bysetting the .spec.template.spec.schedulerName field of the DaemonSet. The original node affinity specified at the.spec.template.spec.affinity.nodeAffinity field (if specified) is taken intoconsideration by the DaemonSet controller when evaluating the eligible nodes,but is replaced on the created Pod with the node affinity that matches the nameof the eligible node. nodeAffinity:  requiredDuringSchedulingIgnoredDuringExecution:    nodeSelectorTerms:    - matchFields:      - key: metadata.name        operator: In        values:        - target-host-name",276
5.2.4 - DaemonSet,Taints and tolerations,"Taints and tolerations The DaemonSet controller automatically adds a set of tolerations to DaemonSet Pods: Tolerations for DaemonSet podsToleration keyEffectDetailsnode.kubernetes.io/not-readyNoExecuteDaemonSet Pods can be scheduled onto nodes that are not healthy or ready to accept Pods. Any DaemonSet Pods running on such nodes will not be evicted.node.kubernetes.io/unreachableNoExecuteDaemonSet Pods can be scheduled onto nodes that are unreachable from the node controller. Any DaemonSet Pods running on such nodes will not be evicted.node.kubernetes.io/disk-pressureNoScheduleDaemonSet Pods can be scheduled onto nodes with disk pressure issues.node.kubernetes.io/memory-pressureNoScheduleDaemonSet Pods can be scheduled onto nodes with memory pressure issues.node.kubernetes.io/pid-pressureNoScheduleDaemonSet Pods can be scheduled onto nodes with process pressure issues.node.kubernetes.io/unschedulableNoScheduleDaemonSet Pods can be scheduled onto nodes that are unschedulable.node.kubernetes.io/network-unavailableNoScheduleOnly added for DaemonSet Pods that request host networking, i.e., Pods having spec.hostNetwork: true. Such DaemonSet Pods can be scheduled onto nodes with unavailable network. You can add your own tolerations to the Pods of a Daemonset as well, bydefining these in the Pod template of the DaemonSet. Because the DaemonSet controller sets thenode.kubernetes.io/unschedulable:NoSchedule toleration automatically,Kubernetes can run DaemonSet Pods on nodes that are marked as unschedulable. If you use a DaemonSet to provide an important node-level function, such ascluster networking, it ishelpful that Kubernetes places DaemonSet Pods on nodes before they are ready.For example, without that special toleration, you could end up in a deadlocksituation where the node is not marked as ready because the network plugin isnot running there, and at the same time the network plugin is not running onthat node because the node is not yet ready.",510
5.2.4 - DaemonSet,Communicating with Daemon Pods,"Communicating with Daemon Pods Some possible patterns for communicating with Pods in a DaemonSet are: Push: Pods in the DaemonSet are configured to send updates to another service, suchas a stats database. They do not have clients.NodeIP and Known Port: Pods in the DaemonSet can use a hostPort, so that the podsare reachable via the node IPs.Clients know the list of node IPs somehow, and know the port by convention.DNS: Create a headless servicewith the same pod selector, and then discover DaemonSets using the endpointsresource or retrieve multiple A records from DNS.Service: Create a service with the same Pod selector, and use the service to reach adaemon on a random node. (No way to reach specific node.)",170
5.2.4 - DaemonSet,Updating a DaemonSet,"Updating a DaemonSet If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and deletePods from newly not-matching nodes. You can modify the Pods that a DaemonSet creates. However, Pods do not allow allfields to be updated. Also, the DaemonSet controller will use the original template the nexttime a node (even with the same name) is created. You can delete a DaemonSet. If you specify --cascade=orphan with kubectl, then the Podswill be left on the nodes. If you subsequently create a new DaemonSet with the same selector,the new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replacesthem according to its updateStrategy. You can perform a rolling update on a DaemonSet.",183
5.2.4 - DaemonSet,Init scripts,"Init scripts It is certainly possible to run daemon processes by directly starting them on a node (e.g. usinginit, upstartd, or systemd). This is perfectly fine. However, there are several advantages torunning such processes via a DaemonSet: Ability to monitor and manage logs for daemons in the same way as applications.Same config language and tools (e.g. Pod templates, kubectl) for daemons and applications.Running daemons in containers with resource limits increases isolation between daemons from appcontainers. However, this can also be accomplished by running the daemons in a container but not in a Pod.",137
5.2.4 - DaemonSet,Bare Pods,"Bare Pods It is possible to create Pods directly which specify a particular node to run on. However,a DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case ofnode failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you shoulduse a DaemonSet rather than creating individual Pods.",76
5.2.4 - DaemonSet,Static Pods,"Static Pods It is possible to create Pods by writing a file to a certain directory watched by Kubelet. Theseare called static pods.Unlike DaemonSet, static Pods cannot be managed with kubectlor other Kubernetes API clients. Static Pods do not depend on the apiserver, making them usefulin cluster bootstrapping cases. Also, static Pods may be deprecated in the future.",88
5.2.4 - DaemonSet,Deployments,"Deployments DaemonSets are similar to Deployments in thatthey both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,storage servers). Use a Deployment for stateless services, like frontends, where scaling up and down thenumber of replicas and rolling out updates are more important than controlling exactly which hostthe Pod runs on. Use a DaemonSet when it is important that a copy of a Pod always run onall or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that particular node. For example, network plugins often include a component that runs as a DaemonSet. The DaemonSet component makes sure that the node where it's running has working cluster networking. Learn about Pods.Learn about static Pods, which are useful for running Kubernetescontrol plane components.Find out how to use DaemonSetsPerform a rolling update on a DaemonSetPerform a rollback on a DaemonSet(for example, if a roll out didn't work how you expected).Understand how Kubernetes assigns Pods to Nodes.Learn about device plugins andadd ons, which often run as DaemonSets.DaemonSet is a top-level resource in the Kubernetes REST API.Read theDaemonSetobject definition to understand the API for daemon sets.",297
5.2.5 - Jobs,default,"A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate.As pods successfully complete, the Job tracks the successful completions. When a specified numberof successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean upthe Pods it created. Suspending a Job will delete its active Pods until the Jobis resumed again. A simple case is to create one Job object in order to reliably run one Pod to completion.The Job object will start a new Pod if the first Pod fails or is deleted (for exampledue to a node hardware failure or a node reboot). You can also use a Job to run multiple Pods in parallel. If you want to run a Job (either a single task, or several in parallel) on a schedule,see CronJob.",182
5.2.5 - Jobs,Running an example Job,"Running an example Job Here is an example Job config. It computes π to 2000 places and prints it out.It takes around 10s to complete. controllers/job.yamlapiVersion: batch/v1kind: Jobmetadata:  name: pispec:  template:    spec:      containers:      - name: pi        image: perl:5.34.0        command: [""perl"",  ""-Mbignum=bpi"", ""-wle"", ""print bpi(2000)""]      restartPolicy: Never  backoffLimit: 4 You can run the example with this command: kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml The output is similar to this: job.batch/pi created Check on the status of the Job with kubectl: kubectl describe job pikubectl get job pi -o yaml Name:             piNamespace:        defaultSelector:         controller-uid=0cd26dd5-88a2-4a5f-a203-ea19a1d5d578Labels:           controller-uid=0cd26dd5-88a2-4a5f-a203-ea19a1d5d578                  job-name=piAnnotations:      batch.kubernetes.io/job-tracking: Parallelism:      1Completions:      1Completion Mode:  NonIndexedStart Time:       Fri, 28 Oct 2022 13:05:18 +0530Completed At:     Fri, 28 Oct 2022 13:05:21 +0530Duration:         3sPods Statuses:    0 Active / 1 Succeeded / 0 FailedPod Template:  Labels:  controller-uid=0cd26dd5-88a2-4a5f-a203-ea19a1d5d578           job-name=pi  Containers:   pi:    Image:      perl:5.34.0    Port:       <none>    Host Port:  <none>    Command:      perl      -Mbignum=bpi      -wle      print bpi(2000)    Environment:  <none>    Mounts:       <none>  Volumes:        <none>Events:  Type    Reason            Age   From            Message  ----    ------            ----  ----            -------  Normal  SuccessfulCreate  21s   job-controller  Created pod: pi-xf9p4  Normal  Completed         18s   job-controller  Job completedapiVersion: batch/v1kind: Jobmetadata:  annotations:    batch.kubernetes.io/job-tracking: """"    kubectl.kubernetes.io/last-applied-configuration: |      {""apiVersion"":""batch/v1"",""kind"":""Job"",""metadata"":{""annotations"":{},""name"":""pi"",""namespace"":""default""},""spec"":{""backoffLimit"":4,""template"":{""spec"":{""containers"":[{""command"":[""perl"",""-Mbignum=bpi"",""-wle"",""print bpi(2000)""],""image"":""perl:5.34.0"",""name"":""pi""}],""restartPolicy"":""Never""}}}}  creationTimestamp: ""2022-11-10T17:53:53Z""  generation: 1  labels:    controller-uid: 204fb678-040b-497f-9266-35ffa8716d14    job-name: pi  name: pi  namespace: default  resourceVersion: ""4751""  uid: 204fb678-040b-497f-9266-35ffa8716d14spec:  backoffLimit: 4  completionMode: NonIndexed  completions: 1  parallelism: 1  selector:    matchLabels:      controller-uid: 204fb678-040b-497f-9266-35ffa8716d14  suspend: false  template:    metadata:      creationTimestamp: null      labels:        controller-uid: 204fb678-040b-497f-9266-35ffa8716d14        job-name: pi    spec:      containers:      - command:        - perl        - -Mbignum=bpi        - -wle        - print bpi(2000)        image: perl:5.34.0        imagePullPolicy: IfNotPresent        name: pi        resources: {}        terminationMessagePath: /dev/termination-log        terminationMessagePolicy: File      dnsPolicy: ClusterFirst      restartPolicy: Never      schedulerName: default-scheduler      securityContext: {}      terminationGracePeriodSeconds: 30status:  active: 1  ready: 0  startTime: ""2022-11-10T17:53:57Z""  uncountedTerminatedPods: {} To view completed Pods of a Job, use kubectl get pods. To list all the Pods that belong to a Job in a machine readable form, you can use a command like this: pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath='{.items[*].metadata.name}')echo $pods The output is similar to this: pi-5rwd7 Here, the selector is the same as the selector for the Job. The --output=jsonpath option specifies an expressionwith the name from each Pod in the returned list. View the standard output of one of the pods: kubectl logs $pods The output is similar to this: 3.13",2075
5.2.5 - Jobs,Writing a Job spec,"Writing a Job spec As with all other Kubernetes config, a Job needs apiVersion, kind, and metadata fields. When the control plane creates new Pods for a Job, the .metadata.name of theJob is part of the basis for naming those Pods. The name of a Job must be a validDNS subdomainvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,the name should follow the more restrictive rules for aDNS label.Even when the name is a DNS subdomain, the name must be no longer than 63characters. A Job also needs a .spec section.",132
5.2.5 - Jobs,Pod Template,"Pod Template The .spec.template is the only required field of the .spec. The .spec.template is a pod template. It has exactly the same schema as a Pod, except it is nested and does not have an apiVersion or kind. In addition to required fields for a Pod, a pod template in a Job must specify appropriatelabels (see pod selector) and an appropriate restart policy. Only a RestartPolicy equal to Never or OnFailure is allowed.",96
5.2.5 - Jobs,Parallel execution for Jobs,"Parallel execution for Jobs There are three main types of task suitable to run as a Job: Non-parallel Jobsnormally, only one Pod is started, unless the Pod fails.the Job is complete as soon as its Pod terminates successfully.Parallel Jobs with a fixed completion count:specify a non-zero positive value for .spec.completions.the Job represents the overall task, and is complete when there are .spec.completions successful Pods.when using .spec.completionMode=""Indexed"", each Pod gets a different index in the range 0 to .spec.completions-1.Parallel Jobs with a work queue:do not specify .spec.completions, default to .spec.parallelism.the Pods must coordinate amongst themselves or an external service to determine what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.each Pod is independently capable of determining whether or not all its peers are done, and thus that the entire Job is done.when any Pod from the Job terminates with success, no new Pods are created.once at least one Pod has terminated with success and all Pods are terminated, then the Job is completed with success.once any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output. They should all be in the process of exiting. For a non-parallel Job, you can leave both .spec.completions and .spec.parallelism unset. When both areunset, both are defaulted to 1. For a fixed completion count Job, you should set .spec.completions to the number of completions needed.You can set .spec.parallelism, or leave it unset and it will default to 1. For a work queue Job, you must leave .spec.completions unset, and set .spec.parallelism toa non-negative integer. For more information about how to make use of the different types of job, see the job patterns section.",439
5.2.5 - Jobs,Controlling parallelism,"Controlling parallelism The requested parallelism (.spec.parallelism) can be set to any non-negative value.If it is unspecified, it defaults to 1.If it is specified as 0, then the Job is effectively paused until it is increased. Actual parallelism (number of pods running at any instant) may be more or less than requestedparallelism, for a variety of reasons: For fixed completion count Jobs, the actual number of pods running in parallel will not exceed the number ofremaining completions. Higher values of .spec.parallelism are effectively ignored.For work queue Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete, however.If the Job Controller has not had time to react.If the Job controller failed to create Pods for any reason (lack of ResourceQuota, lack of permission, etc.),then there may be fewer pods than requested.The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.When a Pod is gracefully shut down, it takes time to stop.",227
5.2.5 - Jobs,Completion mode,"Completion mode FEATURE STATE: Kubernetes v1.24 [stable] Jobs with fixed completion count - that is, jobs that have non null.spec.completions - can have a completion mode that is specified in .spec.completionMode: NonIndexed (default): the Job is considered complete when there have been.spec.completions successfully completed Pods. In other words, each Podcompletion is homologous to each other. Note that Jobs that have null.spec.completions are implicitly NonIndexed.Indexed: the Pods of a Job get an associated completion index from 0 to.spec.completions-1. The index is available through three mechanisms:The Pod annotation batch.kubernetes.io/job-completion-index.As part of the Pod hostname, following the pattern $(job-name)-$(index).When you use an Indexed Job in combination with aService, Pods within the Job can usethe deterministic hostnames to address each other via DNS. For more information abouthow to configure this, see Job with Pod-to-Pod Communication.From the containerized task, in the environment variable JOB_COMPLETION_INDEX.The Job is considered complete when there is one successfully completed Podfor each index. For more information about how to use this mode, seeIndexed Job for Parallel Processing with Static Work Assignment.Note that, although rare, more than one Pod could be started for the sameindex, but only one of them will count towards the completion count.",327
5.2.5 - Jobs,Handling Pod and container failures,"Handling Pod and container failures A container in a Pod may fail for a number of reasons, such as because the process in it exited witha non-zero exit code, or the container was killed for exceeding a memory limit, etc. If thishappens, and the .spec.template.spec.restartPolicy = ""OnFailure"", then the Pod stayson the node, but the container is re-run. Therefore, your program needs to handle the case when it isrestarted locally, or else specify .spec.template.spec.restartPolicy = ""Never"".See pod lifecycle for more information on restartPolicy. An entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node(node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the.spec.template.spec.restartPolicy = ""Never"". When a Pod fails, then the Job controllerstarts a new Pod. This means that your application needs to handle the case when it is restarted in a newpod. In particular, it needs to handle temporary files, locks, incomplete output and the likecaused by previous runs. By default, each pod failure is counted towards the .spec.backoffLimit limit,see pod backoff failure policy. However, you cancustomize handling of pod failures by setting the Job's pod failure policy. Note that even if you specify .spec.parallelism = 1 and .spec.completions = 1 and.spec.template.spec.restartPolicy = ""Never"", the same program maysometimes be started twice. If you do specify .spec.parallelism and .spec.completions both greater than 1, then there may bemultiple pods running at once. Therefore, your pods must also be tolerant of concurrency. When the feature gatesPodDisruptionConditions and JobPodFailurePolicy are both enabled,and the .spec.podFailurePolicy field is set, the Job controller does not consider a terminatingPod (a pod that has a .metadata.deletionTimestamp field set) as a failure until that Pod isterminal (its .status.phase is Failed or Succeeded). However, the Job controllercreates a replacement Pod as soon as the termination becomes apparent. Once thepod terminates, the Job controller evaluates .backoffLimit and .podFailurePolicyfor the relevant Job, taking this now-terminated Pod into consideration. If either of these requirements is not satisfied, the Job controller countsa terminating Pod as an immediate failure, even if that Pod later terminateswith phase: ""Succeeded"".",553
5.2.5 - Jobs,Pod backoff failure policy,"Pod backoff failure policy There are situations where you want to fail a Job after some amount of retriesdue to a logical error in configuration etc.To do so, set .spec.backoffLimit to specify the number of retries beforeconsidering a Job as failed. The back-off limit is set by default to 6. FailedPods associated with the Job are recreated by the Job controller with anexponential back-off delay (10s, 20s, 40s ...) capped at six minutes. The number of retries is calculated in two ways: The number of Pods with .status.phase = ""Failed"".When using restartPolicy = ""OnFailure"", the number of retries in all thecontainers of Pods with .status.phase equal to Pending or Running. If either of the calculations reaches the .spec.backoffLimit, the Job isconsidered failed. Note: If your job has restartPolicy = ""OnFailure"", keep in mind that your Pod running the Jobwill be terminated once the job backoff limit has been reached. This can make debugging the Job's executable more difficult. We suggest settingrestartPolicy = ""Never"" when debugging the Job or using a logging system to ensure outputfrom failed Jobs is not lost inadvertently.",263
5.2.5 - Jobs,Job termination and cleanup,"Job termination and cleanup When a Job completes, no more Pods are created, but the Pods are usually not deleted either.Keeping them aroundallows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output.The job object also remains after it is completed so that you can view its status. It is up to the user to deleteold jobs after noting their status. Delete the job with kubectl (e.g. kubectl delete jobs/pi or kubectl delete -f ./job.yaml). When you delete the job using kubectl, all the pods it created are deleted too. By default, a Job will run uninterrupted unless a Pod fails (restartPolicy=Never) or a Container exits in error (restartPolicy=OnFailure), at which point the Job defers to the.spec.backoffLimit described above. Once .spec.backoffLimit has been reached the Job will be marked as failed and any running Pods will be terminated. Another way to terminate a Job is by setting an active deadline.Do this by setting the .spec.activeDeadlineSeconds field of the Job to a number of seconds.The activeDeadlineSeconds applies to the duration of the job, no matter how many Pods are created.Once a Job reaches activeDeadlineSeconds, all of its running Pods are terminated and the Job status will become type: Failed with reason: DeadlineExceeded. Note that a Job's .spec.activeDeadlineSeconds takes precedence over its .spec.backoffLimit. Therefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once it reaches the time limit specified by activeDeadlineSeconds, even if the backoffLimit is not yet reached. Example: apiVersion: batch/v1kind: Jobmetadata:  name: pi-with-timeoutspec:  backoffLimit: 5  activeDeadlineSeconds: 100  template:    spec:      containers:      - name: pi        image: perl:5.34.0        command: [""perl"",  ""-Mbignum=bpi"", ""-wle"", ""print bpi(2000)""]      restartPolicy: Never Note that both the Job spec and the Pod template spec within the Job have an activeDeadlineSeconds field. Ensure that you set this field at the proper level. Keep in mind that the restartPolicy applies to the Pod, and not to the Job itself: there is no automatic Job restart once the Job status is type: Failed.That is, the Job termination mechanisms activated with .spec.activeDeadlineSeconds and .spec.backoffLimit result in a permanent Job failure that requires manual intervention to resolve.",579
5.2.5 - Jobs,Clean up finished jobs automatically,"Clean up finished jobs automatically Finished Jobs are usually no longer needed in the system. Keeping them around inthe system will put pressure on the API server. If the Jobs are managed directlyby a higher level controller, such asCronJobs, the Jobs can becleaned up by CronJobs based on the specified capacity-based cleanup policy.",70
5.2.5 - Jobs,TTL mechanism for finished Jobs,"TTL mechanism for finished Jobs FEATURE STATE: Kubernetes v1.23 [stable] Another way to clean up finished Jobs (either Complete or Failed)automatically is to use a TTL mechanism provided by aTTL controller forfinished resources, by specifying the .spec.ttlSecondsAfterFinished field ofthe Job. When the TTL controller cleans up the Job, it will delete the Job cascadingly,i.e. delete its dependent objects, such as Pods, together with the Job. Notethat when the Job is deleted, its lifecycle guarantees, such as finalizers, willbe honored. For example: apiVersion: batch/v1kind: Jobmetadata:  name: pi-with-ttlspec:  ttlSecondsAfterFinished: 100  template:    spec:      containers:      - name: pi        image: perl:5.34.0        command: [""perl"",  ""-Mbignum=bpi"", ""-wle"", ""print bpi(2000)""]      restartPolicy: Never The Job pi-with-ttl will be eligible to be automatically deleted, 100seconds after it finishes. If the field is set to 0, the Job will be eligible to be automatically deletedimmediately after it finishes. If the field is unset, this Job won't be cleanedup by the TTL controller after it finishes. Note:It is recommended to set ttlSecondsAfterFinished field because unmanaged jobs(Jobs that you created directly, and not indirectly through other workload APIssuch as CronJob) have a default deletionpolicy of orphanDependents causing Pods created by an unmanaged Job to be left aroundafter that Job is fully deleted.Even though the control plane eventuallygarbage collectsthe Pods from a deleted Job after they either fail or complete, sometimes thoselingering pods may cause cluster performance degradation or in worst case cause thecluster to go offline due to this degradation.You can use LimitRanges andResourceQuotas to place acap on the amount of resources that a particular namespace canconsume.",440
5.2.5 - Jobs,Job patterns,"Job patterns The Job object can be used to support reliable parallel execution of Pods. The Job object is notdesigned to support closely-communicating parallel processes, as commonly found in scientificcomputing. It does support parallel processing of a set of independent but related work items.These might be emails to be sent, frames to be rendered, files to be transcoded, ranges of keys in aNoSQL database to scan, and so on. In a complex system, there may be multiple different sets of work items. Here we are justconsidering one set of work items that the user wants to manage together — a batch job. There are several different patterns for parallel computation, each with strengths and weaknesses.The tradeoffs are: One Job object for each work item, vs. a single Job object for all work items. The latter isbetter for large numbers of work items. The former creates some overhead for the user and for thesystem to manage large numbers of Job objects.Number of pods created equals number of work items, vs. each Pod can process multiple work items.The former typically requires less modification to existing code and containers. The latteris better for large numbers of work items, for similar reasons to the previous bullet.Several approaches use a work queue. This requires running a queue service,and modifications to the existing program or container to make it use the work queue.Other approaches are easier to adapt to an existing containerised application. The tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.The pattern names are also links to examples and more detailed description. PatternSingle Job objectFewer pods than work items?Use app unmodified?Queue with Pod Per Work Item✓sometimesQueue with Variable Pod Count✓✓Indexed Job with Static Work Assignment✓✓Job Template Expansion✓Job with Pod-to-Pod Communication✓sometimessometimes When you specify completions with .spec.completions, each Pod created by the Job controllerhas an identical spec. This means thatall pods for a task will have the same command line and the sameimage, the same volumes, and (almost) the same environment variables. These patternsare different ways to arrange for pods to work on different things. This table shows the required settings for .spec.parallelism and .spec.completions for each of the patterns.Here, W is the number of work items. Pattern.spec.completions.spec.parallelismQueue with Pod Per Work ItemWanyQueue with Variable Pod CountnullanyIndexed Job with Static Work AssignmentWanyJob Template Expansion1should be 1Job with Pod-to-Pod CommunicationWW",549
5.2.5 - Jobs,Suspending a Job,"Suspending a Job FEATURE STATE: Kubernetes v1.24 [stable] When a Job is created, the Job controller will immediately begin creating Podsto satisfy the Job's requirements and will continue to do so until the Job iscomplete. However, you may want to temporarily suspend a Job's execution andresume it later, or start Jobs in suspended state and have a custom controllerdecide later when to start them. To suspend a Job, you can update the .spec.suspend field ofthe Job to true; later, when you want to resume it again, update it to false.Creating a Job with .spec.suspend set to true will create it in the suspendedstate. When a Job is resumed from suspension, its .status.startTime field will bereset to the current time. This means that the .spec.activeDeadlineSecondstimer will be stopped and reset when a Job is suspended and resumed. When you suspend a Job, any running Pods that don't have a status of Completed will be terminated.with a SIGTERM signal. The Pod's graceful termination period will be honored andyour Pod must handle this signal in this period. This may involve savingprogress for later or undoing changes. Pods terminated this way will not counttowards the Job's completions count. An example Job definition in the suspended state can be like so: kubectl get job myjob -o yaml apiVersion: batch/v1kind: Jobmetadata:  name: myjobspec:  suspend: true  parallelism: 1  completions: 5  template:    spec:      ... You can also toggle Job suspension by patching the Job using the command line. Suspend an active Job: kubectl patch job/myjob --type=strategic --patch '{""spec"":{""suspend"":true}}' Resume a suspended Job: kubectl patch job/myjob --type=strategic --patch '{""spec"":{""suspend"":false}}' The Job's status can be used to determine if a Job is suspended or has beensuspended in the past: kubectl get jobs/myjob -o yaml apiVersion: batch/v1kind: Job# .metadata and .spec omittedstatus:  conditions:  - lastProbeTime: ""2021-02-05T13:14:33Z""    lastTransitionTime: ""2021-02-05T13:14:33Z""    status: ""True""    type: Suspended  startTime: ""2021-02-05T13:13:48Z"" The Job condition of type ""Suspended"" with status ""True"" means the Job issuspended; the lastTransitionTime field can be used to determine how long theJob has been suspended for. If the status of that condition is ""False"", then theJob was previously suspended and is now running. If such a condition does notexist in the Job's status, the Job has never been stopped. Events are also created when the Job is suspended and resumed: kubectl describe jobs/myjob Name:           myjob...Events:  Type    Reason            Age   From            Message  ----    ------            ----  ----            -------  Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl  Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl  Normal  Suspended         11m   job-controller  Job suspended  Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44  Normal  Resumed           3s    job-controller  Job resumed The last four events, particularly the ""Suspended"" and ""Resumed"" events, aredirectly a result of toggling the .spec.suspend field. In the time betweenthese two events, we see that no Pods were created, but Pod creation restartedas soon as the Job was resumed.",853
5.2.5 - Jobs,Mutable Scheduling Directives,"Mutable Scheduling Directives FEATURE STATE: Kubernetes v1.23 [beta] Note: In order to use this behavior, you must enable the JobMutableNodeSchedulingDirectivesfeature gateon the API server.It is enabled by default. In most cases a parallel job will want the pods to run with constraints,like all in the same zone, or all either on GPU model x or y but not a mix of both. The suspend field is the first step towards achieving those semantics. Suspend allows acustom queue controller to decide when a job should start; However, once a job is unsuspended,a custom queue controller has no influence on where the pods of a job will actually land. This feature allows updating a Job's scheduling directives before it starts, which gives custom queuecontrollers the ability to influence pod placement while at the same time offloading actualpod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that have neverbeen unsuspended before. The fields in a Job's pod template that can be updated are node affinity, node selector,tolerations, labels and annotations.",239
5.2.5 - Jobs,Specifying your own Pod selector,"Specifying your own Pod selector Normally, when you create a Job object, you do not specify .spec.selector.The system defaulting logic adds this field when the Job is created.It picks a selector value that will not overlap with any other jobs. However, in some cases, you might need to override this automatically set selector.To do this, you can specify the .spec.selector of the Job. Be very careful when doing this. If you specify a label selector which is notunique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelatedjob may be deleted, or this Job may count other Pods as completing it, or one or bothJobs may refuse to create Pods or run to completion. If a non-unique selector ischosen, then other controllers (e.g. ReplicationController) and their Pods may behavein unpredictable ways too. Kubernetes will not stop you from making a mistake whenspecifying .spec.selector. Here is an example of a case when you might want to use this feature. Say Job old is already running. You want existing Podsto keep running, but you want the rest of the Pods it createsto use a different pod template and for the Job to have a new name.You cannot update the Job because these fields are not updatable.Therefore, you delete Job old but leave its podsrunning, using kubectl delete jobs/old --cascade=orphan.Before deleting it, you make a note of what selector it uses: kubectl get job old -o yaml The output is similar to this: kind: Jobmetadata:  name: old  ...spec:  selector:    matchLabels:      controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002  ... Then you create a new Job with name new and you explicitly specify the same selector.Since the existing Pods have label controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002,they are controlled by Job new as well. You need to specify manualSelector: true in the new Job since you are not usingthe selector that the system normally generates for you automatically. kind: Jobmetadata:  name: new  ...spec:  manualSelector: true  selector:    matchLabels:      controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002  ... The new Job itself will have a different uid from a8f3d00d-c6d2-11e5-9f87-42010af00002. SettingmanualSelector: true tells the system that you know what you are doing and to allow thismismatch.",613
5.2.5 - Jobs,Pod failure policy,"Pod failure policy FEATURE STATE: Kubernetes v1.26 [beta] Note: You can only configure a Pod failure policy for a Job if you have theJobPodFailurePolicy feature gateenabled in your cluster. Additionally, it is recommendedto enable the PodDisruptionConditions feature gate in order to be able to detect and handlePod disruption conditions in the Pod failure policy (see also:Pod disruption conditions). Both feature gates areavailable in Kubernetes 1.26. A Pod failure policy, defined with the .spec.podFailurePolicy field, enablesyour cluster to handle Pod failures based on the container exit codes and thePod conditions. In some situations, you may want to have a better control when handling Podfailures than the control provided by the Pod backoff failure policy,which is based on the Job's .spec.backoffLimit. These are some examples of use cases: To optimize costs of running workloads by avoiding unnecessary Pod restarts,you can terminate a Job as soon as one of its Pods fails with an exit codeindicating a software bug.To guarantee that your Job finishes even if there are disruptions, you canignore Pod failures caused by disruptions (such preemption,API-initiated evictionor taint-based eviction) sothat they don't count towards the .spec.backoffLimit limit of retries. You can configure a Pod failure policy, in the .spec.podFailurePolicy field,to meet the above use cases. This policy can handle Pod failures based on thecontainer exit codes and the Pod conditions. Here is a manifest for a Job that defines a podFailurePolicy: /controllers/job-pod-failure-policy-example.yamlapiVersion: batch/v1kind: Jobmetadata:  name: job-pod-failure-policy-examplespec:  completions: 12  parallelism: 3  template:    spec:      restartPolicy: Never      containers:      - name: main        image: docker.io/library/bash:5        command: [""bash""]        # example command simulating a bug which triggers the FailJob action        args:        - -c        - echo ""Hello world!"" && sleep 5 && exit 42  backoffLimit: 6  podFailurePolicy:    rules:    - action: FailJob      onExitCodes:        containerName: main      # optional        operator: In             # one of: In, NotIn        values: [42]    - action: Ignore             # one of: Ignore, FailJob, Count      onPodConditions:      - type: DisruptionTarget   # indicates Pod disruption In the example above, the first rule of the Pod failure policy specifies thatthe Job should be marked failed if the main container fails with the 42 exitcode. The following are the rules for the main container specifically: an exit code of 0 means that the container succeededan exit code of 42 means that the entire Job failedany other exit code represents that the container failed, and hence the entirePod. The Pod will be re-created if the total number of restarts isbelow backoffLimit. If the backoffLimit is reached the entire Job failed. Note: Because the Pod template specifies a restartPolicy: Never,the kubelet does not restart the main container in that particular Pod. The second rule of the Pod failure policy, specifying the Ignore action forfailed Pods with condition DisruptionTarget excludes Pod disruptions frombeing counted towards the .spec.backoffLimit limit of retries. Note: If the Job failed, either by the Pod failure policy or Pod backofffailure policy, and the Job is running multiple Pods, Kubernetes terminates allthe Pods in that Job that are still Pending or Running. These are some requirements and semantics of the API: if you want to use a .spec.podFailurePolicy field for a Job, you mustalso define that Job's pod template with .spec.restartPolicy set to Never.the Pod failure policy rules you specify under spec.podFailurePolicy.rulesare evaluated in order. Once a rule matches a Pod failure, the remaining rulesare ignored. When no rule matches the Pod failure, the defaulthandling applies.you may want to restrict a rule to a specific container by specifying its nameinspec.podFailurePolicy.rules[*].containerName. When not specified the ruleapplies to all containers. When specified, it should match one the containeror initContainer names in the Pod template.you may specify the action taken when a Pod failure policy is matched byspec.podFailurePolicy.rules[*].action. Possible values are:FailJob: use to indicate that the Pod's job should be marked as Failed andall running Pods should be terminated.Ignore: use to indicate that the counter towards the .spec.backoffLimitshould not be incremented and a replacement Pod should be created.Count: use to indicate that the Pod should be handled in the default way.The counter towards the .spec.backoffLimit should be incremented.",1055
5.2.5 - Jobs,Job tracking with finalizers,"Job tracking with finalizers FEATURE STATE: Kubernetes v1.26 [stable] Note: The control plane doesn't track Jobs using finalizers, if the Jobs were createdwhen the feature gate JobTrackingWithFinalizers was disabled, even after youupgrade the control plane to 1.26. The control plane keeps track of the Pods that belong to any Job and notices ifany such Pod is removed from the API server. To do that, the Job controllercreates Pods with the finalizer batch.kubernetes.io/job-tracking. Thecontroller removes the finalizer only after the Pod has been accounted for inthe Job status, allowing the Pod to be removed by other controllers or users. Jobs created before upgrading to Kubernetes 1.26 or before the feature gateJobTrackingWithFinalizers is enabled are tracked without the use of Podfinalizers.The Job controller updatesthe status counters for succeeded and failed Pods based only on the Podsthat exist in the cluster. The contol plane can lose track of the progress ofthe Job if Pods are deleted from the cluster. You can determine if the control plane is tracking a Job using Pod finalizers bychecking if the Job has the annotationbatch.kubernetes.io/job-tracking. You should not manually add or removethis annotation from Jobs. Instead, you can recreate the Jobs to ensure theyare tracked using Pod finalizers.",301
5.2.5 - Jobs,Bare Pods,"Bare Pods When the node that a Pod is running on reboots or fails, the pod is terminatedand will not be restarted. However, a Job will create new Pods to replace terminated ones.For this reason, we recommend that you use a Job rather than a bare Pod, even if your applicationrequires only a single Pod.",70
5.2.5 - Jobs,Replication Controller,"Replication Controller Jobs are complementary to Replication Controllers.A Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Jobmanages Pods that are expected to terminate (e.g. batch tasks). As discussed in Pod Lifecycle, Job is only appropriatefor pods with RestartPolicy equal to OnFailure or Never.(Note: If RestartPolicy is not set, the default value is Always.)",93
5.2.5 - Jobs,Single Job starts controller Pod,"Single Job starts controller Pod Another pattern is for a single Job to create a Pod which then creates other Pods, acting as a sortof custom controller for those Pods. This allows the most flexibility, but may be somewhatcomplicated to get started with and offers less integration with Kubernetes. One example of this pattern would be a Job which starts a Pod which runs a script that in turnstarts a Spark master controller (see spark example), runs a sparkdriver, and then cleans up. An advantage of this approach is that the overall process gets the completion guarantee of a Jobobject, but maintains complete control over what Pods are created and how work is assigned to them. Learn about Pods.Read about different ways of running Jobs:Coarse Parallel Processing Using a Work QueueFine Parallel Processing Using a Work QueueUse an indexed Job for parallel processing with static work assignmentCreate multiple Jobs based on a template: Parallel Processing using ExpansionsFollow the links within Clean up finished jobs automaticallyto learn more about how your cluster can clean up completed and / or failed tasks.Job is part of the Kubernetes REST API.Read theJobobject definition to understand the API for jobs.Read about CronJob, which youcan use to define a series of Jobs that will run based on a schedule, similar tothe UNIX tool cron.Practice how to configure handling of retriable and non-retriable pod failuresusing podFailurePolicy, based on the step-by-step examples.",308
5.2.6 - Automatic Cleanup for Finished Jobs,default,"A time-to-live mechanism to clean up old Jobs that have finished execution. FEATURE STATE: Kubernetes v1.23 [stable] When your Job has finished, it's useful to keep that Job in the API (and not immediately delete the Job)so that you can tell whether the Job succeeded or failed. Kubernetes' TTL-after-finished controller provides aTTL (time to live) mechanism to limit the lifetime of Job objects thathave finished execution.",101
5.2.6 - Automatic Cleanup for Finished Jobs,Cleanup for finished Jobs,"Cleanup for finished Jobs The TTL-after-finished controller is only supported for Jobs. You can use this mechanism to cleanup finished Jobs (either Complete or Failed) automatically by specifying the.spec.ttlSecondsAfterFinished field of a Job, as in thisexample. The TTL-after-finished controller assumes that a Job is eligible to be cleaned upTTL seconds after the Job has finished. The timer starts once thestatus condition of the Job changes to show that the Job is either Complete or Failed; once the TTL hasexpired, that Job becomes eligible forcascading removal. When theTTL-after-finished controller cleans up a job, it will delete it cascadingly, that is to say it will deleteits dependent objects together with it. Kubernetes honors object lifecycle guarantees on the Job, such as waiting forfinalizers. You can set the TTL seconds at any time. Here are some examples for setting the.spec.ttlSecondsAfterFinished field of a Job: Specify this field in the Job manifest, so that a Job can be cleaned upautomatically some time after it finishes.Manually set this field of existing, already finished Jobs, so that they become eligiblefor cleanup.Use amutating admission webhookto set this field dynamically at Job creation time. Cluster administrators canuse this to enforce a TTL policy for finished jobs.Use amutating admission webhookto set this field dynamically after the Job has finished, and choosedifferent TTL values based on job status, labels. For this case, the webhook needsto detect changes to the .status of the Job and only set a TTL when the Jobis being marked as completed.Write your own controller to manage the cleanup TTL for Jobs that match a particularselector-selector.",371
5.2.6 - Automatic Cleanup for Finished Jobs,Updating TTL for finished Jobs,"Updating TTL for finished Jobs You can modify the TTL period, e.g. .spec.ttlSecondsAfterFinished field of Jobs,after the job is created or has finished. If you extend the TTL period after theexisting ttlSecondsAfterFinished period has expired, Kubernetes doesn't guaranteeto retain that Job, even if an update to extend the TTL returns a successful APIresponse.",87
5.2.6 - Automatic Cleanup for Finished Jobs,Time skew,"Time skew Because the TTL-after-finished controller uses timestamps stored in the Kubernetes jobs todetermine whether the TTL has expired or not, this feature is sensitive to timeskew in your cluster, which may cause the control plane to clean up Job objectsat the wrong time. Clocks aren't always correct, but the difference should bevery small. Please be aware of this risk when setting a non-zero TTL. Read Clean up Jobs automaticallyRefer to the Kubernetes Enhancement Proposal(KEP) for adding this mechanism.",115
5.2.7 - CronJob,default,"FEATURE STATE: Kubernetes v1.21 [stable] A CronJob creates Jobs on a repeating schedule. CronJob is meant for performing regular scheduled actions such as backups, report generation,and so on. One CronJob object is like one line of a crontab (cron table) file on aUnix system. It runs a job periodically on a given schedule, written inCron format. CronJobs have limitations and idiosyncrasies.For example, in certain circumstances, a single CronJob can create multiple concurrent Jobs. See the limitations below. When the control plane creates new Jobs and (indirectly) Pods for a CronJob, the .metadata.nameof the CronJob is part of the basis for naming those Pods. The name of a CronJob must be a validDNS subdomainvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,the name should follow the more restrictive rules for aDNS label.Even when the name is a DNS subdomain, the name must be no longer than 52characters. This is because the CronJob controller will automatically append11 characters to the name you provide and there is a constraint that thelength of a Job name is no more than 63 characters.",260
5.2.7 - CronJob,Example,"Example This example CronJob manifest prints the current time and a hello message every minute: application/job/cronjob.yamlapiVersion: batch/v1kind: CronJobmetadata:  name: hellospec:  schedule: ""* * * * *""  jobTemplate:    spec:      template:        spec:          containers:          - name: hello            image: busybox:1.28            imagePullPolicy: IfNotPresent            command:            - /bin/sh            - -c            - date; echo Hello from the Kubernetes cluster          restartPolicy: OnFailure (Running Automated Tasks with a CronJobtakes you through this example in more detail).",147
5.2.7 - CronJob,Schedule syntax,"Schedule syntax The .spec.schedule field is required. The value of that field follows the Cron syntax: # ┌───────────── minute (0 - 59)# │ ┌───────────── hour (0 - 23)# │ │ ┌───────────── day of the month (1 - 31)# │ │ │ ┌───────────── month (1 - 12)# │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;# │ │ │ │ │                                   7 is also Sunday on some systems)# │ │ │ │ │                                   OR sun, mon, tue, wed, thu, fri, sat# │ │ │ │ │# * * * * * For example, 0 0 13 * 5 states that the task must be started every Friday at midnight, as well as on the 13th of each month at midnight. The format also includes extended ""Vixie cron"" step values. As explained in theFreeBSD manual: Step values can be used in conjunction with ranges. Following a rangewith /<number> specifies skips of the number's value through therange. For example, 0-23/2 can be used in the hours field to specifycommand execution every other hour (the alternative in the V7 standard is0,2,4,6,8,10,12,14,16,18,20,22). Steps are also permitted after anasterisk, so if you want to say ""every two hours"", just use */2. Note: A question mark (?) in the schedule has the same meaning as an asterisk *, that is,it stands for any of available value for a given field. Other than the standard syntax, some macros like @monthly can also be used: EntryDescriptionEquivalent to@yearly (or @annually)Run once a year at midnight of 1 January0 0 1 1 *@monthlyRun once a month at midnight of the first day of the month0 0 1 * *@weeklyRun once a week at midnight on Sunday morning0 0 * * 0@daily (or @midnight)Run once a day at midnight0 0 * * *@hourlyRun once an hour at the beginning of the hour0 * * * * To generate CronJob schedule expressions, you can also use web tools like crontab.guru.",497
5.2.7 - CronJob,Job template,"Job template The .spec.jobTemplate defines a template for the Jobs that the CronJob creates, and it is required.It has exactly the same schema as a Job, except thatit is nested and does not have an apiVersion or kind.You can specify common metadata for the templated Jobs, such aslabels orannotations.For information about writing a Job .spec, see Writing a Job Spec.",85
5.2.7 - CronJob,Deadline for delayed job start,"Deadline for delayed job start The .spec.startingDeadlineSeconds field is optional.This field defines a deadline (in whole seconds) for starting the Job, if that Job misses its scheduled timefor any reason. After missing the deadline, the CronJob skips that instance of the Job (future occurrences are still scheduled).For example, if you have a backup job that runs twice a day, you might allow it to start up to 8 hours late,but no later, because a backup taken any later wouldn't be useful: you would instead prefer to wait forthe next scheduled run. For Jobs that miss their configured deadline, Kubernetes treats them as failed Jobs.If you don't specify startingDeadlineSeconds for a CronJob, the Job occurrences have no deadline. If the .spec.startingDeadlineSeconds field is set (not null), the CronJobcontroller measures the time between when a job is expected to be created andnow. If the difference is higher than that limit, it will skip this execution. For example, if it is set to 200, it allows a job to be created for up to 200seconds after the actual schedule.",239
5.2.7 - CronJob,Concurrency policy,"Concurrency policy The .spec.concurrencyPolicy field is also optional.It specifies how to treat concurrent executions of a job that is created by this CronJob.The spec may specify only one of the following concurrency policies: Allow (default): The CronJob allows concurrently running jobsForbid: The CronJob does not allow concurrent runs; if it is time for a new job run and theprevious job run hasn't finished yet, the CronJob skips the new job runReplace: If it is time for a new job run and the previous job run hasn't finished yet, theCronJob replaces the currently running job run with a new job run Note that concurrency policy only applies to the jobs created by the same cron job.If there are multiple CronJobs, their respective jobs are always allowed to run concurrently.",171
5.2.7 - CronJob,Schedule suspension,"Schedule suspension You can suspend execution of Jobs for a CronJob, by setting the optional .spec.suspend fieldto true. The field defaults to false. This setting does not affect Jobs that the CronJob has already started. If you do set that field to true, all subsequent executions are suspended (they remainscheduled, but the CronJob controller does not start the Jobs to run the tasks) untilyou unsuspend the CronJob. Caution: Executions that are suspended during their scheduled time count as missed jobs.When .spec.suspend changes from true to false on an existing CronJob without astarting deadline, the missed jobs are scheduled immediately.",138
5.2.7 - CronJob,Jobs history limits,"Jobs history limits The .spec.successfulJobsHistoryLimit and .spec.failedJobsHistoryLimit fields are optional.These fields specify how many completed and failed jobs should be kept.By default, they are set to 3 and 1 respectively. Setting a limit to 0 corresponds to keepingnone of the corresponding kind of jobs after they finish. For another way to clean up jobs automatically, see Clean up finished jobs automatically.",86
5.2.7 - CronJob,Time zones,"Time zones For CronJobs with no time zone specified, the kube-controller-manager interprets schedules relative to its local time zone. FEATURE STATE: Kubernetes v1.25 [beta] If you enable the CronJobTimeZone feature gate,you can specify a time zone for a CronJob (if you don't enable that feature gate, or if you are using a version ofKubernetes that does not have experimental time zone support, all CronJobs in your cluster have an unspecifiedtimezone). When you have the feature enabled, you can set .spec.timeZone to the name of a valid time zone. For example, setting.spec.timeZone: ""Etc/UTC"" instructs Kubernetes to interpret the schedule relative to Coordinated Universal Time. Caution:The implementation of the CronJob API in Kubernetes 1.26 lets you setthe .spec.schedule field to include a timezone; for example: CRON_TZ=UTC * * * * *or TZ=UTC * * * * *.Specifying a timezone that way is not officially supported (and never has been).If you try to set a schedule that includes TZ or CRON_TZ timezone specification,Kubernetes reports a warning to the client.Future versions of Kubernetes might not implement that unofficial timezone mechanism at all. A time zone database from the Go standard library is included in the binaries and used as a fallback in case an external database is not available on the system.",324
5.2.7 - CronJob,Modifying a CronJob,"Modifying a CronJob By design, a CronJob contains a template for new Jobs.If you modify an existing CronJob, the changes you make will apply to new Jobs thatstart to run after your modification is complete. Jobs (and their Pods) that have alreadystarted continue to run without changes.That is, the CronJob does not update existing Jobs, even if those remain running.",80
5.2.7 - CronJob,Job creation,"Job creation A CronJob creates a Job object approximately once per execution time of its schedule.The scheduling is approximate because thereare certain circumstances where two Jobs might be created, or no Job might be created.Kubernetes tries to avoid those situations, but do not completely prevent them. Therefore,the Jobs that you define should be idempotent. If startingDeadlineSeconds is set to a large value or left unset (the default)and if concurrencyPolicy is set to Allow, the jobs will always runat least once. Caution: If startingDeadlineSeconds is set to a value less than 10 seconds, the CronJob may not be scheduled. This is because the CronJob controller checks things every 10 seconds. For every CronJob, the CronJob Controller checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the job and logs the error. Cannot determine if job needs to be started. Too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew. It is important to note that if the startingDeadlineSeconds field is set (not nil), the controller counts how many missed jobs occurred from the value of startingDeadlineSeconds until now rather than from the last scheduled time until now. For example, if startingDeadlineSeconds is 200, the controller counts how many missed jobs occurred in the last 200 seconds. A CronJob is counted as missed if it has failed to be created at its scheduled time. For example, if concurrencyPolicy is set to Forbid and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed. For example, suppose a CronJob is set to schedule a new Job every one minute beginning at 08:30:00, and itsstartingDeadlineSeconds field is not set. If the CronJob controller happens tobe down from 08:29:00 to 10:21:00, the job will not start as the number of missed jobs which missed their schedule is greater than 100. To illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at 08:30:00, and itsstartingDeadlineSeconds is set to 200 seconds. If the CronJob controller happens tobe down for the same period as the previous example (08:29:00 to 10:21:00,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (i.e., 3 missed schedules), rather than from the last scheduled time until now. The CronJob is only responsible for creating Jobs that match its schedule, andthe Job in turn is responsible for the management of the Pods it represents. Learn about Pods andJobs, two conceptsthat CronJobs rely upon.Read about the detailed formatof CronJob .spec.schedule fields.For instructions on creating and working with CronJobs, and for an exampleof a CronJob manifest,see Running automated tasks with CronJobs.CronJob is part of the Kubernetes REST API.Read theCronJobAPI reference for more details.",679
5.2.8 - ReplicationController,default,"Note: A Deployment that configures a ReplicaSet is now the recommended way to set up replication. A ReplicationController ensures that a specified number of pod replicas are running at any onetime. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods isalways up and available.",68
5.2.8 - ReplicationController,How a ReplicationController Works,"How a ReplicationController Works If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, theReplicationController starts more pods. Unlike manually created pods, the pods maintained by aReplicationController are automatically replaced if they fail, are deleted, or are terminated.For example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.For this reason, you should use a ReplicationController even if your application requiresonly a single pod. A ReplicationController is similar to a process supervisor,but instead of supervising individual processes on a single node, the ReplicationController supervises multiple podsacross multiple nodes. ReplicationController is often abbreviated to ""rc"" in discussion, and as a shortcut inkubectl commands. A simple case is to create one ReplicationController object to reliably run one instance ofa Pod indefinitely. A more complex use case is to run several identical replicas of a replicatedservice, such as web servers.",209
5.2.8 - ReplicationController,Running an example ReplicationController,"Running an example ReplicationController This example ReplicationController config runs three copies of the nginx web server. controllers/replication.yamlapiVersion: v1kind: ReplicationControllermetadata:  name: nginxspec:  replicas: 3  selector:    app: nginx  template:    metadata:      name: nginx      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx        ports:        - containerPort: 80 Run the example job by downloading the example file and then running this command: kubectl apply -f https://k8s.io/examples/controllers/replication.yaml The output is similar to this: replicationcontroller/nginx created Check on the status of the ReplicationController using this command: kubectl describe replicationcontrollers/nginx The output is similar to this: Name:        nginxNamespace:   defaultSelector:    app=nginxLabels:      app=nginxAnnotations:    <none>Replicas:    3 current / 3 desiredPods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 FailedPod Template:  Labels:       app=nginx  Containers:   nginx:    Image:              nginx    Port:               80/TCP    Environment:        <none>    Mounts:             <none>  Volumes:              <none>Events:  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message  ---------       --------     -----    ----                        -------------    ----      ------              -------  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v Here, three pods are created, but none is running yet, perhaps because the image is being pulled.A little later, the same command may show: Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed To list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this: pods=$(kubectl get pods --selector=app=nginx --output=jsonpath={.items..metadata.name})echo $pods The output is similar to this: nginx-3ntk0 nginx-4ok8v nginx-qrm3m Here, the selector is the same as the selector for the ReplicationController (seen in thekubectl describe output), and in a different form in replication.yaml. The --output=jsonpath optionspecifies an expression with the name from each pod in the returned list.",640
5.2.8 - ReplicationController,Writing a ReplicationController Manifest,"Writing a ReplicationController Manifest As with all other Kubernetes config, a ReplicationController needs apiVersion, kind, and metadata fields. When the control plane creates new Pods for a ReplicationController, the .metadata.name of theReplicationController is part of the basis for naming those Pods. The name of a ReplicationController must be a validDNS subdomainvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,the name should follow the more restrictive rules for aDNS label. For general information about working with configuration files, see object management. A ReplicationController also needs a .spec section.",136
5.2.8 - ReplicationController,Pod Template,"Pod Template The .spec.template is the only required field of the .spec. The .spec.template is a pod template. It has exactly the same schema as a Pod, except it is nested and does not have an apiVersion or kind. In addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriatelabels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See pod selector. Only a .spec.template.spec.restartPolicy equal to Always is allowed, which is the default if not specified. For local container restarts, ReplicationControllers delegate to an agent on the node,for example the Kubelet.",146
5.2.8 - ReplicationController,Labels on the ReplicationController,"Labels on the ReplicationController The ReplicationController can itself have labels (.metadata.labels). Typically, youwould set these the same as the .spec.template.metadata.labels; if .metadata.labels is not specifiedthen it defaults to .spec.template.metadata.labels. However, they are allowed to bedifferent, and the .metadata.labels do not affect the behavior of the ReplicationController.",91
5.2.8 - ReplicationController,Pod Selector,"Pod Selector The .spec.selector field is a label selector. A ReplicationControllermanages all the pods with labels that match the selector. It does not distinguishbetween pods that it created or deleted and pods that another person or process created ordeleted. This allows the ReplicationController to be replaced without affecting the running pods. If specified, the .spec.template.metadata.labels must be equal to the .spec.selector, or it willbe rejected by the API. If .spec.selector is unspecified, it will be defaulted to.spec.template.metadata.labels. Also you should not normally create any pods whose labels match this selector, either directly, withanother ReplicationController, or with another controller such as Job. If you do so, theReplicationController thinks that it created the other pods. Kubernetes does not stop youfrom doing this. If you do end up with multiple controllers that have overlapping selectors, youwill have to manage the deletion yourself (see below).",214
5.2.8 - ReplicationController,Multiple Replicas,"Multiple Replicas You can specify how many pods should run concurrently by setting .spec.replicas to the numberof pods you would like to have running concurrently. The number running at any time may be higheror lower, such as if the replicas were just increased or decreased, or if a pod is gracefullyshutdown, and a replacement starts early. If you do not specify .spec.replicas, then it defaults to 1.",91
5.2.8 - ReplicationController,Deleting a ReplicationController and its Pods,"Deleting a ReplicationController and its Pods To delete a ReplicationController and all its pods, use kubectl delete. Kubectl will scale the ReplicationController to zero and waitfor it to delete each pod before deleting the ReplicationController itself. If this kubectlcommand is interrupted, it can be restarted. When using the REST API or client library, you need to do the steps explicitly (scale replicas to0, wait for pod deletions, then delete the ReplicationController).",108
5.2.8 - ReplicationController,Deleting only a ReplicationController,"Deleting only a ReplicationController You can delete a ReplicationController without affecting any of its pods. Using kubectl, specify the --cascade=orphan option to kubectl delete. When using the REST API or client library, you can delete the ReplicationController object. Once the original is deleted, you can create a new ReplicationController to replace it. As longas the old and new .spec.selector are the same, then the new one will adopt the old pods.However, it will not make any effort to make existing pods match a new, different pod template.To update pods to a new spec in a controlled way, use a rolling update.",143
5.2.8 - ReplicationController,Isolating pods from a ReplicationController,Isolating pods from a ReplicationController Pods may be removed from a ReplicationController's target set by changing their labels. This technique may be used to remove pods from service for debugging and data recovery. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).,68
5.2.8 - ReplicationController,Rescheduling,"Rescheduling As mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).",58
5.2.8 - ReplicationController,Rolling updates,"Rolling updates The ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one. As explained in #1353, the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures. Ideally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time. The two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.",162
5.2.8 - ReplicationController,Multiple release tracks,"Multiple release tracks In addition to running multiple releases of an application while a rolling update is in progress, it's common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels. For instance, a service might target all pods with tier in (frontend), environment in (prod). Now say you have 10 replicated pods that make up this tier. But you want to be able to 'canary' a new version of this component. You could set up a ReplicationController with replicas set to 9 for the bulk of the replicas, with labels tier=frontend, environment=prod, track=stable, and another ReplicationController with replicas set to 1 for the canary, with labels tier=frontend, environment=prod, track=canary. Now the service is covering both the canary and non-canary pods. But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.",213
5.2.8 - ReplicationController,Using ReplicationControllers with Services,"Using ReplicationControllers with Services Multiple ReplicationControllers can sit behind a single service, so that, for example, some trafficgoes to the old version, and some goes to the new version. A ReplicationController will never terminate on its own, but it isn't expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.",140
5.2.8 - ReplicationController,Writing programs for Replication,"Writing programs for Replication Pods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the RabbitMQ work queues, as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.",151
5.2.8 - ReplicationController,Responsibilities of the ReplicationController,"Responsibilities of the ReplicationController The ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, readiness and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies. The ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in #492), which would change its replicas field. We will not add scheduling policies (for example, spreading) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation (#170). The ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The ""macro"" operations currently supported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like Asgard managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.",332
5.2.8 - ReplicationController,ReplicaSet,"ReplicaSet ReplicaSet is the next-generation ReplicationController that supports the new set-based label selector.It's mainly used by Deployment as a mechanism to orchestrate pod creation, deletion and updates.Note that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don't require updates at all.",74
5.2.8 - ReplicationController,Deployment (Recommended),"Deployment (Recommended) Deployment is a higher-level API object that updates its underlying Replica Sets and their Pods. Deployments are recommended if you want the rolling update functionality, because they are declarative, server-side, and have additional features.",54
5.2.8 - ReplicationController,Bare Pods,"Bare Pods Unlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node. A ReplicationController delegates local container restarts to some agent on the node, such as the kubelet.",122
5.2.8 - ReplicationController,DaemonSet,"DaemonSet Use a DaemonSet instead of a ReplicationController for pods that provide amachine-level function, such as machine monitoring or machine logging. These pods have a lifetime that is tiedto a machine lifetime: the pod needs to be running on the machine before other pods start, and aresafe to terminate when the machine is otherwise ready to be rebooted/shutdown. Learn about Pods.Learn about Deployment, the replacementfor ReplicationController.ReplicationController is part of the Kubernetes REST API.Read theReplicationControllerobject definition to understand the API for replication controllers.",124
"6 - Services, Load Balancing, and Networking",The Kubernetes network model,"The Kubernetes network model Every Pod in a cluster gets its own unique cluster-wide IP address.This means you do not need to explicitly create links between Pods and youalmost never need to deal with mapping container ports to host ports.This creates a clean, backwards-compatible model where Pods can be treatedmuch like VMs or physical hosts from the perspectives of port allocation,naming, service discovery, load balancing,application configuration, and migration. Kubernetes imposes the following fundamental requirements on any networkingimplementation (barring any intentional network segmentation policies): pods can communicate with all other pods on any other nodewithout NATagents on a node (e.g. system daemons, kubelet) can communicate with allpods on that node Note: For those platforms that support Pods running in the host network (e.g.Linux), when pods are attached to the host network of a node they can still communicatewith all pods on all nodes without NAT. This model is not only less complex overall, but it is principally compatiblewith the desire for Kubernetes to enable low-friction porting of apps from VMsto containers. If your job previously ran in a VM, your VM had an IP and couldtalk to other VMs in your project. This is the same basic model. Kubernetes IP addresses exist at the Pod scope - containers within a Podshare their network namespaces - including their IP address and MAC address.This means that containers within a Pod can all reach each other's ports onlocalhost. This also means that containers within a Pod must coordinate portusage, but this is no different from processes in a VM. This is called the""IP-per-pod"" model. How this is implemented is a detail of the particular container runtime in use. It is possible to request ports on the Node itself which forward to your Pod(called host ports), but this is a very niche operation. How that forwarding isimplemented is also a detail of the container runtime. The Pod itself isblind to the existence or non-existence of host ports. Kubernetes networking addresses four concerns: Containers within a Pod use networking to communicate via loopback.Cluster networking provides communication between different Pods.The Service API lets youexpose an application running in Podsto be reachable from outside your cluster.Ingress provides extra functionalityspecifically for exposing HTTP applications, websites and APIs.You can also use Services topublish services only for consumption inside your cluster. The Connecting Applications with Services tutorial lets you learn about Services and Kubernetes networking with a hands-on example. Cluster Networking explains how to setup networking for your cluster, and also provides an overview of the technologies involved.",567
6.1 - Service,default,"Expose an application running in your cluster behind a single outward-facing endpoint, even when the workload is split across multiple backends. Pods With Kubernetes you don't need to modify your application to use an unfamiliar service discovery mechanism.Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods,and can load-balance across them.",81
6.1 - Service,Motivation,"Motivation Kubernetes Pods are created and destroyedto match the desired state of your cluster. Pods are nonpermanent resources.If you use a Deployment to run your app,it can create and destroy Pods dynamically. Each Pod gets its own IP address, however in a Deployment, the set of Podsrunning in one moment in time could be different fromthe set of Pods running that application a moment later. This leads to a problem: if some set of Pods (call them ""backends"") providesfunctionality to other Pods (call them ""frontends"") inside your cluster,how do the frontends find out and keep track of which IP address to connectto, so that the frontend can use the backend part of the workload? Enter Services.",162
6.1 - Service,Service resources,"Service resources In Kubernetes, a Service is an abstraction which defines a logical set of Podsand a policy by which to access them (sometimes this pattern is calleda micro-service). The set of Pods targeted by a Service is usually determinedby a selector.To learn about other ways to define Service endpoints,see Services without selectors. For example, consider a stateless image-processing backend which is running with3 replicas. Those replicas are fungible—frontends do not care which backendthey use. While the actual Pods that compose the backend set may change, thefrontend clients should not need to be aware of that, nor should they need to keeptrack of the set of backends themselves. The Service abstraction enables this decoupling.",160
6.1 - Service,Cloud-native service discovery,"Cloud-native service discovery If you're able to use Kubernetes APIs for service discovery in your application,you can query the API serverfor matching EndpointSlices. Kubernetes updates the EndpointSlices for a Servicewhenever the set of Pods in a Service changes. For non-native applications, Kubernetes offers ways to place a network port or loadbalancer in between your application and the backend Pods.",96
6.1 - Service,Defining a Service,"Defining a Service A Service in Kubernetes is a REST object, similar to a Pod. Like all of theREST objects, you can POST a Service definition to the API server to createa new instance.The name of a Service object must be a validRFC 1035 label name. For example, suppose you have a set of Pods where each listens on TCP port 9376and contains a label app.kubernetes.io/name=MyApp: apiVersion: v1kind: Servicemetadata:  name: my-servicespec:  selector:    app.kubernetes.io/name: MyApp  ports:    - protocol: TCP      port: 80      targetPort: 9376 This specification creates a new Service object named ""my-service"", whichtargets TCP port 9376 on any Pod with the app.kubernetes.io/name=MyApp label. Kubernetes assigns this Service an IP address (sometimes called the ""cluster IP""),which is used by the Service proxies(see Virtual IP addressing mechanism below). The controller for the Service selector continuously scans for Pods thatmatch its selector, and then POSTs any updates to an Endpoint objectalso named ""my-service"". Note: A Service can map any incoming port to a targetPort. By default andfor convenience, the targetPort is set to the same value as the portfield. Port definitions in Pods have names, and you can reference these names in thetargetPort attribute of a Service. For example, we can bind the targetPortof the Service to the Pod port in the following way: apiVersion: v1kind: Podmetadata:  name: nginx  labels:    app.kubernetes.io/name: proxyspec:  containers:  - name: nginx    image: nginx:stable    ports:      - containerPort: 80        name: http-web-svc---apiVersion: v1kind: Servicemetadata:  name: nginx-servicespec:  selector:    app.kubernetes.io/name: proxy  ports:  - name: name-of-service-port    protocol: TCP    port: 80    targetPort: http-web-svc This works even if there is a mixture of Pods in the Service using a singleconfigured name, with the same network protocol available via differentport numbers. This offers a lot of flexibility for deploying and evolvingyour Services. For example, you can change the port numbers that Pods exposein the next version of your backend software, without breaking clients. The default protocol for Services isTCP; you can alsouse any other supported protocol. As many Services need to expose more than one port, Kubernetes supports multipleport definitions on a Service object.Each port definition can have the same protocol, or a different one.",606
6.1 - Service,Services without selectors,"Services without selectors Services most commonly abstract access to Kubernetes Pods thanks to the selector,but when used with a corresponding set ofEndpointSlicesobjects and without a selector, the Service can abstract other kinds of backends,including ones that run outside the cluster. For example: You want to have an external database cluster in production, but in yourtest environment you use your own databases.You want to point your Service to a Service in a differentNamespace or on another cluster.You are migrating a workload to Kubernetes. While evaluating the approach,you run only a portion of your backends in Kubernetes. In any of these scenarios you can define a Service without a Pod selector.For example: apiVersion: v1kind: Servicemetadata:  name: my-servicespec:  ports:    - protocol: TCP      port: 80      targetPort: 9376 Because this Service has no selector, the corresponding EndpointSlice (andlegacy Endpoints) objects are not created automatically. You can map the Serviceto the network address and port where it's running, by adding an EndpointSliceobject manually. For example: apiVersion: discovery.k8s.io/v1kind: EndpointSlicemetadata:  name: my-service-1 # by convention, use the name of the Service                     # as a prefix for the name of the EndpointSlice  labels:    # You should set the ""kubernetes.io/service-name"" label.    # Set its value to match the name of the Service    kubernetes.io/service-name: my-serviceaddressType: IPv4ports:  - name: '' # empty because port 9376 is not assigned as a well-known             # port (by IANA)    appProtocol: http    protocol: TCP    port: 9376endpoints:  - addresses:      - ""10.4.5.6"" # the IP addresses in this list can appear in any order      - ""10.1.2.3""",439
6.1 - Service,Custom EndpointSlices,"Custom EndpointSlices When you create an EndpointSlice object for a Service, you canuse any name for the EndpointSlice. Each EndpointSlice in a namespace must have aunique name. You link an EndpointSlice to a Service by setting thekubernetes.io/service-name labelon that EndpointSlice. Note:The endpoint IPs must not be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), orlink-local (169.254.0.0/16 and 224.0.0.0/24 for IPv4, fe80::/64 for IPv6).The endpoint IP addresses cannot be the cluster IPs of other Kubernetes Services,because kube-proxy doesn't support virtual IPsas a destination. For an EndpointSlice that you create yourself, or in your own code,you should also pick a value to use for the endpointslice.kubernetes.io/managed-by label.If you create your own controller code to manage EndpointSlices, consider using avalue similar to ""my-domain.example/name-of-controller"". If you are using a thirdparty tool, use the name of the tool in all-lowercase and change spaces and otherpunctuation to dashes (-).If people are directly using a tool such as kubectl to manage EndpointSlices,use a name that describes this manual management, such as ""staff"" or""cluster-admins"". You shouldavoid using the reserved value ""controller"", which identifies EndpointSlicesmanaged by Kubernetes' own control plane.",360
6.1 - Service,Accessing a Service without a selector,"Accessing a Service without a selector Accessing a Service without a selector works the same as if it had a selector.In the example for a Service without a selector, traffic is routed to one of the two endpoints defined inthe EndpointSlice manifest: a TCP connection to 10.1.2.3 or 10.4.5.6, on port 9376. Note: The Kubernetes API server does not allow proxying to endpoints that are not mapped topods. Actions such as kubectl proxy <service-name> where the service has noselector will fail due to this constraint. This prevents the Kubernetes API serverfrom being used as a proxy to endpoints the caller may not be authorized to access. An ExternalName Service is a special case of Service that does not haveselectors and uses DNS names instead. For more information, see theExternalName section later in this document.",193
6.1 - Service,EndpointSlices,"EndpointSlices FEATURE STATE: Kubernetes v1.21 [stable] EndpointSlices are objects thatrepresent a subset (a slice) of the backing network endpoints for a Service. Your Kubernetes cluster tracks how many endpoints each EndpointSlice represents.If there are so many endpoints for a Service that a threshold is reached, thenKubernetes adds another empty EndpointSlice and stores new endpoint informationthere.By default, Kubernetes makes a new EndpointSlice once the existing EndpointSlicesall contain at least 100 endpoints. Kubernetes does not make the new EndpointSliceuntil an extra endpoint needs to be added. See EndpointSlices for moreinformation about this API.",163
6.1 - Service,Endpoints,"Endpoints In the Kubernetes API, anEndpoints(the resource kind is plural) defines a list of network endpoints, typicallyreferenced by a Service to define which Pods the traffic can be sent to. The EndpointSlice API is the recommended replacement for Endpoints.",61
6.1 - Service,Over-capacity endpoints,"Over-capacity endpoints Kubernetes limits the number of endpoints that can fit in a single Endpointsobject. When there are over 1000 backing endpoints for a Service, Kubernetestruncates the data in the Endpoints object. Because a Service can be linkedwith more than one EndpointSlice, the 1000 backing endpoint limit onlyaffects the legacy Endpoints API. In that case, Kubernetes selects at most 1000 possible backend endpoints to storeinto the Endpoints object, and sets anannotation on theEndpoints:endpoints.kubernetes.io/over-capacity: truncated.The control plane also removes that annotation if the number of backend Pods drops below 1000. Traffic is still sent to backends, but any load balancing mechanism that relies on thelegacy Endpoints API only sends traffic to at most 1000 of the available backing endpoints. The same API limit means that you cannot manually update an Endpoints to have more than 1000 endpoints.",210
6.1 - Service,Application protocol,Application protocol FEATURE STATE: Kubernetes v1.20 [stable] The appProtocol field provides a way to specify an application protocol foreach Service port. The value of this field is mirrored by the correspondingEndpoints and EndpointSlice objects. This field follows standard Kubernetes label syntax. Values should either beIANA standard service names ordomain prefixed names such as mycompany.com/my-custom-protocol.,94
6.1 - Service,Multi-Port Services,"Multi-Port Services For some Services, you need to expose more than one port.Kubernetes lets you configure multiple port definitions on a Service object.When using multiple ports for a Service, you must give all of your ports namesso that these are unambiguous.For example: apiVersion: v1kind: Servicemetadata:  name: my-servicespec:  selector:    app.kubernetes.io/name: MyApp  ports:    - name: http      protocol: TCP      port: 80      targetPort: 9376    - name: https      protocol: TCP      port: 443      targetPort: 9377 Note:As with Kubernetes names in general, names for portsmust only contain lowercase alphanumeric characters and -. Port names mustalso start and end with an alphanumeric character.For example, the names 123-abc and web are valid, but 123_abc and -web are not.",202
6.1 - Service,Choosing your own IP address,"Choosing your own IP address You can specify your own cluster IP address as part of a Service creationrequest. To do this, set the .spec.clusterIP field. For example, if youalready have an existing DNS entry that you wish to reuse, or legacy systemsthat are configured for a specific IP address and difficult to re-configure. The IP address that you choose must be a valid IPv4 or IPv6 address from within theservice-cluster-ip-range CIDR range that is configured for the API server.If you try to create a Service with an invalid clusterIP address value, the APIserver will return a 422 HTTP status code to indicate that there's a problem.",146
6.1 - Service,Environment variables,"Environment variables When a Pod is run on a Node, the kubelet adds a set of environment variablesfor each active Service. It adds {SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT variables,where the Service name is upper-cased and dashes are converted to underscores.It also supports variables (see makeLinkVariables)that are compatible with Docker Engine's""legacy container links"" feature. For example, the Service redis-primary which exposes TCP port 6379 and has beenallocated cluster IP address 10.0.0.11, produces the following environmentvariables: REDIS_PRIMARY_SERVICE_HOST=10.0.0.11REDIS_PRIMARY_SERVICE_PORT=6379REDIS_PRIMARY_PORT=tcp://10.0.0.11:6379REDIS_PRIMARY_PORT_6379_TCP=tcp://10.0.0.11:6379REDIS_PRIMARY_PORT_6379_TCP_PROTO=tcpREDIS_PRIMARY_PORT_6379_TCP_PORT=6379REDIS_PRIMARY_PORT_6379_TCP_ADDR=10.0.0.11 Note:When you have a Pod that needs to access a Service, and you are usingthe environment variable method to publish the port and cluster IP to the clientPods, you must create the Service before the client Pods come into existence.Otherwise, those client Pods won't have their environment variables populated.If you only use DNS to discover the cluster IP for a Service, you don't need toworry about this ordering issue.",369
6.1 - Service,DNS,"DNS You can (and almost always should) set up a DNS service for your Kubernetescluster using an add-on. A cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for newServices and creates a set of DNS records for each one. If DNS has been enabledthroughout your cluster then all Pods should automatically be able to resolveServices by their DNS name. For example, if you have a Service called my-service in a Kubernetesnamespace my-ns, the control plane and the DNS Service acting togethercreate a DNS record for my-service.my-ns. Pods in the my-ns namespaceshould be able to find the service by doing a name lookup for my-service(my-service.my-ns would also work). Pods in other namespaces must qualify the name as my-service.my-ns. These nameswill resolve to the cluster IP assigned for the Service. Kubernetes also supports DNS SRV (Service) records for named ports. If themy-service.my-ns Service has a port named http with the protocol set toTCP, you can do a DNS SRV query for _http._tcp.my-service.my-ns to discoverthe port number for http, as well as the IP address. The Kubernetes DNS server is the only way to access ExternalName Services.You can find more information about ExternalName resolution inDNS for Services and Pods.",316
6.1 - Service,Headless Services,"Headless Services Sometimes you don't need load-balancing and a single Service IP. Inthis case, you can create what are termed ""headless"" Services, by explicitlyspecifying ""None"" for the cluster IP (.spec.clusterIP). You can use a headless Service to interface with other service discovery mechanisms,without being tied to Kubernetes' implementation. For headless Services, a cluster IP is not allocated, kube-proxy does not handlethese Services, and there is no load balancing or proxying done by the platformfor them. How DNS is automatically configured depends on whether the Service hasselectors defined:",131
6.1 - Service,With selectors,"With selectors For headless Services that define selectors, the Kubernetes control plane createsEndpointSlice objects in the Kubernetes API, and modifies the DNS configuration to returnA or AAAA records (IPv4 or IPv6 addresses) that point directly to the Pods backingthe Service.",66
6.1 - Service,Without selectors,"Without selectors For headless Services that do not define selectors, the control plane doesnot create EndpointSlice objects. However, the DNS system looks for and configureseither: DNS CNAME records for type: ExternalName Services.DNS A / AAAA records for all IP addresses of the Service's ready endpoints,for all Service types other than ExternalName.For IPv4 endpoints, the DNS system creates A records.For IPv6 endpoints, the DNS system creates AAAA records.",104
6.1 - Service,Publishing Services (ServiceTypes),"Publishing Services (ServiceTypes) For some parts of your application (for example, frontends) you may want to expose aService onto an external IP address, that's outside of your cluster. Kubernetes ServiceTypes allow you to specify what kind of Service you want. Type values and their behaviors are: ClusterIP: Exposes the Service on a cluster-internal IP. Choosing this valuemakes the Service only reachable from within the cluster. This is thedefault that is used if you don't explicitly specify a type for a Service.You can expose the service to the public with an Ingress or theGateway API.NodePort: Exposes the Service on each Node's IP at a static port(the NodePort).To make the node port available, Kubernetes sets up a cluster IP address,the same as if you had requested a Service of type: ClusterIP.LoadBalancer: Exposes the Service externally using a cloudprovider's load balancer.ExternalName: Maps the Service to the contents of theexternalName field (e.g. foo.bar.example.com), by returning a CNAME recordwith its value. No proxying of any kind is set up.Note: You need either kube-dns version 1.7 or CoreDNS version 0.0.8 or higherto use the ExternalName type. The type field was designed as nested functionality - each level adds to theprevious. This is not strictly required on all cloud providers (for example: GoogleCompute Engine does not need to allocate a node port to make type: LoadBalancer work,but another cloud provider integration might do). Although strict nesting is not required,but the Kubernetes API design for Service requires it anyway. You can also use Ingress to expose your Service.Ingress is not a Service type, but it acts as the entry point for your cluster.It lets you consolidate your routing rules into a single resource as it can expose multipleservices under the same IP address.",420
6.1 - Service,Type NodePort,"Type NodePort If you set the type field to NodePort, the Kubernetes control planeallocates a port from a range specified by --service-node-port-range flag (default: 30000-32767).Each node proxies that port (the same port number on every Node) into your Service.Your Service reports the allocated port in its .spec.ports[*].nodePort field. Using a NodePort gives you the freedom to set up your own load balancing solution,to configure environments that are not fully supported by Kubernetes, or evento expose one or more nodes' IP addresses directly. For a node port Service, Kubernetes additionally allocates a port (TCP, UDP orSCTP to match the protocol of the Service). Every node in the cluster configuresitself to listen on that assigned port and to forward traffic to one of the readyendpoints associated with that Service. You'll be able to contact the type: NodePortService, from outside the cluster, by connecting to any node using the appropriateprotocol (for example: TCP), and the appropriate port (as assigned to that Service).",236
6.1 - Service,Choosing your own port,"Choosing your own port If you want a specific port number, you can specify a value in the nodePortfield. The control plane will either allocate you that port or report thatthe API transaction failed.This means that you need to take care of possible port collisions yourself.You also have to use a valid port number, one that's inside the range configuredfor NodePort use. Here is an example manifest for a Service of type: NodePort that specifiesa NodePort value (30007, in this example). apiVersion: v1kind: Servicemetadata:  name: my-servicespec:  type: NodePort  selector:    app.kubernetes.io/name: MyApp  ports:      # By default and for convenience, the `targetPort` is set to the same value as the `port` field.    - port: 80      targetPort: 80      # Optional field      # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767)      nodePort: 30007",226
6.1 - Service,Custom IP address configuration for type: NodePort Services,"Custom IP address configuration for type: NodePort Services You can set up nodes in your cluster to use a particular IP address for serving node portservices. You might want to do this if each node is connected to multiple networks (for example:one network for application traffic, and another network for traffic between nodes and thecontrol plane). If you want to specify particular IP address(es) to proxy the port, you can set the--nodeport-addresses flag for kube-proxy or the equivalent nodePortAddressesfield of thekube-proxy configuration fileto particular IP block(s). This flag takes a comma-delimited list of IP blocks (e.g. 10.0.0.0/8, 192.0.2.0/25)to specify IP address ranges that kube-proxy should consider as local to this node. For example, if you start kube-proxy with the --nodeport-addresses=127.0.0.0/8 flag,kube-proxy only selects the loopback interface for NodePort Services.The default for --nodeport-addresses is an empty list.This means that kube-proxy should consider all available network interfaces for NodePort.(That's also compatible with earlier Kubernetes releases.)Note: This Service is visible as <NodeIP>:spec.ports[*].nodePort and .spec.clusterIP:spec.ports[*].port.If the --nodeport-addresses flag for kube-proxy or the equivalent fieldin the kube-proxy configuration file is set, <NodeIP> would be a filtered node IP address (or possibly IP addresses).",351
6.1 - Service,Type LoadBalancer,"Type LoadBalancer On cloud providers which support external load balancers, setting the typefield to LoadBalancer provisions a load balancer for your Service.The actual creation of the load balancer happens asynchronously, andinformation about the provisioned balancer is published in the Service's.status.loadBalancer field.For example: apiVersion: v1kind: Servicemetadata:  name: my-servicespec:  selector:    app.kubernetes.io/name: MyApp  ports:    - protocol: TCP      port: 80      targetPort: 9376  clusterIP: 10.0.171.239  type: LoadBalancerstatus:  loadBalancer:    ingress:    - ip: 192.0.2.127 Traffic from the external load balancer is directed at the backend Pods.The cloud provider decides how it is load balanced. Some cloud providers allow you to specify the loadBalancerIP. In those cases, the load-balancer is createdwith the user-specified loadBalancerIP. If the loadBalancerIP field is not specified,the loadBalancer is set up with an ephemeral IP address. If you specify a loadBalancerIPbut your cloud provider does not support the feature, the loadbalancerIP field that youset is ignored. To implement a Service of type: LoadBalancer, Kubernetes typically starts offby making the changes that are equivalent to you requesting a Service oftype: NodePort. The cloud-controller-manager component then configures the external load balancer toforward traffic to that assigned node port. As an alpha feature, you can configure a load balanced Service toomit assigning a node port, provided that thecloud provider implementation supports this. Note:On Azure, if you want to use a user-specified public type loadBalancerIP, you first needto create a static type public IP address resource. This public IP address resource shouldbe in the same resource group of the other automatically created resources of the cluster.For example, MC_myResourceGroup_myAKSCluster_eastus.Specify the assigned IP address as loadBalancerIP. Ensure that you have updated thesecurityGroupName in the cloud provider configuration file.For information about troubleshooting CreatingLoadBalancerFailed permission issues see,Use a static IP address with the Azure Kubernetes Service (AKS) load balanceror CreatingLoadBalancerFailed on AKS cluster with advanced networking.",520
6.1 - Service,Load balancers with mixed protocol types,"Load balancers with mixed protocol types FEATURE STATE: Kubernetes v1.24 [beta] By default, for LoadBalancer type of Services, when there is more than one port defined, allports must have the same protocol, and the protocol must be one which is supportedby the cloud provider. The feature gate MixedProtocolLBService (enabled by default for the kube-apiserver as of v1.24) allows the use ofdifferent protocols for LoadBalancer type of Services, when there is more than one port defined. Note: The set of protocols that can be used for LoadBalancer type of Services is still defined by the cloud provider. If acloud provider does not support mixed protocols they will provide only a single protocol.",158
6.1 - Service,Disabling load balancer NodePort allocation,"Disabling load balancer NodePort allocation FEATURE STATE: Kubernetes v1.24 [stable] You can optionally disable node port allocation for a Service of type=LoadBalancer, by settingthe field spec.allocateLoadBalancerNodePorts to false. This should only be used for load balancer implementationsthat route traffic directly to pods as opposed to using node ports. By default, spec.allocateLoadBalancerNodePortsis true and type LoadBalancer Services will continue to allocate node ports. If spec.allocateLoadBalancerNodePortsis set to false on an existing Service with allocated node ports, those node ports will not be de-allocated automatically.You must explicitly remove the nodePorts entry in every Service port to de-allocate those node ports.",167
6.1 - Service,Specifying class of load balancer implementation,"Specifying class of load balancer implementation FEATURE STATE: Kubernetes v1.24 [stable] spec.loadBalancerClass enables you to use a load balancer implementation other than the cloud provider default.By default, spec.loadBalancerClass is nil and a LoadBalancer type of Service usesthe cloud provider's default load balancer implementation if the cluster is configured witha cloud provider using the --cloud-provider component flag.If spec.loadBalancerClass is specified, it is assumed that a load balancerimplementation that matches the specified class is watching for Services.Any default load balancer implementation (for example, the one provided bythe cloud provider) will ignore Services that have this field set.spec.loadBalancerClass can be set on a Service of type LoadBalancer only.Once set, it cannot be changed.The value of spec.loadBalancerClass must be a label-style identifier,with an optional prefix such as ""internal-vip"" or ""example.com/internal-vip"".Unprefixed names are reserved for end-users.",229
6.1 - Service,Internal load balancer,"Internal load balancer In a mixed environment it is sometimes necessary to route traffic from Services inside the same(virtual) network address block. In a split-horizon DNS environment you would need two Services to be able to route both externaland internal traffic to your endpoints. To set an internal load balancer, add one of the following annotations to your Servicedepending on the cloud Service provider you're using. DefaultGCPAWSAzureIBM CloudOpenStackBaidu CloudTencent CloudAlibaba CloudOCI Select one of the tabs.[...]metadata:    name: my-service    annotations:        cloud.google.com/load-balancer-type: ""Internal""[...][...]metadata:    name: my-service    annotations:        service.beta.kubernetes.io/aws-load-balancer-internal: ""true""[...][...]metadata:    name: my-service    annotations:        service.beta.kubernetes.io/azure-load-balancer-internal: ""true""[...][...]metadata:    name: my-service    annotations:        service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type: ""private""[...][...]metadata:    name: my-service    annotations:        service.beta.kubernetes.io/openstack-internal-load-balancer: ""true""[...][...]metadata:    name: my-service    annotations:        service.beta.kubernetes.io/cce-load-balancer-internal-vpc: ""true""[...][...]metadata:  annotations:    service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: subnet-xxxxx[...][...]metadata:  annotations:    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type: ""intranet""[...][...]metadata:    name: my-service    annotations:        service.beta.kubernetes.io/oci-load-balancer-internal: true[...]",458
6.1 - Service,TLS support on AWS,"TLS support on AWS For partial TLS / SSL support on clusters running on AWS, you can add threeannotations to a LoadBalancer service: metadata:  name: my-service  annotations:    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012 The first specifies the ARN of the certificate to use. It can be either acertificate from a third party issuer that was uploaded to IAM or one createdwithin AWS Certificate Manager. metadata:  name: my-service  annotations:    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: (https|http|ssl|tcp) The second annotation specifies which protocol a Pod speaks. For HTTPS andSSL, the ELB expects the Pod to authenticate itself over the encryptedconnection, using a certificate. HTTP and HTTPS selects layer 7 proxying: the ELB terminatesthe connection with the user, parses headers, and injects the X-Forwarded-Forheader with the user's IP address (Pods only see the IP address of theELB at the other end of its connection) when forwarding requests. TCP and SSL selects layer 4 proxying: the ELB forwards traffic withoutmodifying the headers. In a mixed-use environment where some ports are secured and others are left unencrypted,you can use the following annotations: metadata:      name: my-service      annotations:        service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http        service.beta.kubernetes.io/aws-load-balancer-ssl-ports: ""443,8443"" In the above example, if the Service contained three ports, 80, 443, and8443, then 443 and 8443 would use the SSL certificate, but 80 would be proxied HTTP. From Kubernetes v1.9 onwards you can usepredefined AWS SSL policieswith HTTPS or SSL listeners for your Services.To see which policies are available for use, you can use the aws command line tool: aws elb describe-load-balancer-policies --query 'PolicyDescriptions[].PolicyName' You can then specify any one of those policies using the""service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy""annotation; for example: metadata:      name: my-service      annotations:        service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ""ELBSecurityPolicy-TLS-1-2-2017-01""",609
6.1 - Service,PROXY protocol support on AWS,"PROXY protocol support on AWS To enable PROXY protocolsupport for clusters running on AWS, you can use the following serviceannotation: metadata:      name: my-service      annotations:        service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: ""*"" Since version 1.3.0, the use of this annotation applies to all ports proxied by the ELBand cannot be configured otherwise.",93
6.1 - Service,ELB Access Logs on AWS,"ELB Access Logs on AWS There are several annotations to manage access logs for ELB Services on AWS. The annotation service.beta.kubernetes.io/aws-load-balancer-access-log-enabledcontrols whether access logs are enabled. The annotation service.beta.kubernetes.io/aws-load-balancer-access-log-emit-intervalcontrols the interval in minutes for publishing the access logs. You can specifyan interval of either 5 or 60 minutes. The annotation service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-namecontrols the name of the Amazon S3 bucket where load balancer access logs arestored. The annotation service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefixspecifies the logical hierarchy you created for your Amazon S3 bucket. metadata:      name: my-service      annotations:        # Specifies whether access logs are enabled for the load balancer        service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: ""true""        # The interval for publishing the access logs. You can specify an interval of either 5 or 60 (minutes).        service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval: ""60""        # The name of the Amazon S3 bucket where the access logs are stored        service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: ""my-bucket""        # The logical hierarchy you created for your Amazon S3 bucket, for example `my-bucket-prefix/prod`        service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: ""my-bucket-prefix/prod""",435
6.1 - Service,Connection Draining on AWS,"Connection Draining on AWS Connection draining for Classic ELBs can be managed with the annotationservice.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled setto the value of ""true"". The annotationservice.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout canalso be used to set maximum time, in seconds, to keep the existing connections open beforederegistering the instances. metadata:      name: my-service      annotations:        service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled: ""true""        service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout: ""60""",173
6.1 - Service,Other ELB annotations,"Other ELB annotations There are other annotations to manage Classic Elastic Load Balancers that are described below. metadata:      name: my-service      annotations:        # The time, in seconds, that the connection is allowed to be idle (no data has been sent        # over the connection) before it is closed by the load balancer        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: ""60""        # Specifies whether cross-zone load balancing is enabled for the load balancer        service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: ""true""        # A comma-separated list of key-value pairs which will be recorded as        # additional tags in the ELB.        service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: ""environment=prod,owner=devops""        # The number of successive successful health checks required for a backend to        # be considered healthy for traffic. Defaults to 2, must be between 2 and 10        service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: """"        # The number of unsuccessful health checks required for a backend to be        # considered unhealthy for traffic. Defaults to 6, must be between 2 and 10        service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: ""3""        # The approximate interval, in seconds, between health checks of an        # individual instance. Defaults to 10, must be between 5 and 300        service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: ""20""        # The amount of time, in seconds, during which no response means a failed        # health check. This value must be less than the service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval        # value. Defaults to 5, must be between 2 and 60        service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: ""5""        # A list of existing security groups to be configured on the ELB created. Unlike the annotation        # service.beta.kubernetes.io/aws-load-balancer-extra-security-groups, this replaces all other        # security groups previously assigned to the ELB and also overrides the creation        # of a uniquely generated security group for this ELB.        # The first security group ID on this list is used as a source to permit incoming traffic to        # target worker nodes (service traffic and health checks).        # If multiple ELBs are configured with the same security group ID, only a single permit line        # will be added to the worker node security groups, that means if you delete any        # of those ELBs it will remove the single permit line and block access for all ELBs that shared the same security group ID.        # This can cause a cross-service outage if not used properly        service.beta.kubernetes.io/aws-load-balancer-security-groups: ""sg-53fae93f""        # A list of additional security groups to be added to the created ELB, this leaves the uniquely        # generated security group in place, this ensures that every ELB        # has a unique security group ID and a matching permit line to allow traffic to the target worker nodes        # (service traffic and health checks).        # Security groups defined here can be shared between services.        service.beta.kubernetes.io/aws-load-balancer-extra-security-groups: ""sg-53fae93f,sg-42efd82e""        # A comma separated list of key-value pairs which are used        # to select the target nodes for the load balancer        service.beta.kubernetes.io/aws-load-balancer-target-node-labels: ""ingress-gw,gw-name=public-api""",897
6.1 - Service,Network Load Balancer support on AWS,"Network Load Balancer support on AWS FEATURE STATE: Kubernetes v1.15 [beta] To use a Network Load Balancer on AWS, use the annotation service.beta.kubernetes.io/aws-load-balancer-type with the value set to nlb. metadata:      name: my-service      annotations:        service.beta.kubernetes.io/aws-load-balancer-type: ""nlb"" Note: NLB only works with certain instance classes; see theAWS documentationon Elastic Load Balancing for a list of supported instance types. Unlike Classic Elastic Load Balancers, Network Load Balancers (NLBs) forward theclient's IP address through to the node. If a Service's .spec.externalTrafficPolicyis set to Cluster, the client's IP address is not propagated to the endPods. By setting .spec.externalTrafficPolicy to Local, the client IP addresses ispropagated to the end Pods, but this could result in uneven distribution oftraffic. Nodes without any Pods for a particular LoadBalancer Service will failthe NLB Target Group's health check on the auto-assigned.spec.healthCheckNodePort and not receive any traffic. In order to achieve even traffic, either use a DaemonSet or specify apod anti-affinityto not locate on the same node. You can also use NLB Services with the internal load balancerannotation. In order for client traffic to reach instances behind an NLB, the Node securitygroups are modified with the following IP rules: RuleProtocolPort(s)IpRange(s)IpRange DescriptionHealth CheckTCPNodePort(s) (.spec.healthCheckNodePort for .spec.externalTrafficPolicy = Local)Subnet CIDRkubernetes.io/rule/nlb/health=<loadBalancerName>Client TrafficTCPNodePort(s).spec.loadBalancerSourceRanges (defaults to 0.0.0.0/0)kubernetes.io/rule/nlb/client=<loadBalancerName>MTU DiscoveryICMP3,4.spec.loadBalancerSourceRanges (defaults to 0.0.0.0/0)kubernetes.io/rule/nlb/mtu=<loadBalancerName> In order to limit which client IP's can access the Network Load Balancer,specify loadBalancerSourceRanges. spec:  loadBalancerSourceRanges:    - ""143.231.0.0/16"" Note: If .spec.loadBalancerSourceRanges is not set, Kubernetesallows traffic from 0.0.0.0/0 to the Node Security Group(s). If nodes havepublic IP addresses, be aware that non-NLB traffic can also reach all instancesin those modified security groups. Further documentation on annotations for Elastic IPs and other common use-cases may be foundin the AWS Load Balancer Controller documentation.",646
6.1 - Service,Type ExternalName,"Type ExternalName Services of type ExternalName map a Service to a DNS name, not to a typical selector such asmy-service or cassandra. You specify these Services with the spec.externalName parameter. This Service definition, for example, mapsthe my-service Service in the prod namespace to my.database.example.com: apiVersion: v1kind: Servicemetadata:  name: my-service  namespace: prodspec:  type: ExternalName  externalName: my.database.example.com Note: ExternalName accepts an IPv4 address string, but as a DNS name comprised of digits, not as an IP address.ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalNameis intended to specify a canonical DNS name. To hardcode an IP address, consider usingheadless Services. When looking up the host my-service.prod.svc.cluster.local, the cluster DNS Servicereturns a CNAME record with the value my.database.example.com. Accessingmy-service works in the same way as other Services but with the crucialdifference that redirection happens at the DNS level rather than via proxying orforwarding. Should you later decide to move your database into your cluster, youcan start its Pods, add appropriate selectors or endpoints, and change theService's type. Warning:You may have trouble using ExternalName for some common protocols, including HTTP and HTTPS.If you use ExternalName then the hostname used by clients inside your cluster is different fromthe name that the ExternalName references.For protocols that use hostnames this difference may lead to errors or unexpected responses.HTTP requests will have a Host: header that the origin server does not recognize;TLS servers will not be able to provide a certificate matching the hostname that the client connected to. Note: This section is indebted to the Kubernetes Tips - Part1 blog post from Alen Komljen.",414
6.1 - Service,External IPs,"External IPs If there are external IPs that route to one or more cluster nodes, Kubernetes Services can be exposed on thoseexternalIPs. Traffic that ingresses into the cluster with the external IP (as destination IP), on the Service port,will be routed to one of the Service endpoints. externalIPs are not managed by Kubernetes and are the responsibilityof the cluster administrator. In the Service spec, externalIPs can be specified along with any of the ServiceTypes.In the example below, ""my-service"" can be accessed by clients on ""80.11.12.10:80"" (externalIP:port) apiVersion: v1kind: Servicemetadata:  name: my-servicespec:  selector:    app.kubernetes.io/name: MyApp  ports:    - name: http      protocol: TCP      port: 80      targetPort: 9376  externalIPs:    - 80.11.12.10",209
6.1 - Service,Session stickiness,"Session stickiness If you want to make sure that connections from a particular client are passed tothe same Pod each time, you can configure session affinity based on the client'sIP address. Read session affinityto learn more.",44
6.1 - Service,Virtual IP addressing mechanism,Virtual IP addressing mechanism Read Virtual IPs and Service Proxies to learn about themechanism Kubernetes provides to expose a Service with a virtual IP address. Learn more about the following: Follow the Connecting Applications with Services tutorialIngress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster.EndpointSlices For more context: Virtual IPs and Service ProxiesAPI reference for the Service APIAPI reference for the Endpoints APIAPI reference for the EndpointSlice API,104
6.2 - Ingress,default,"Make your HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress concept lets you map traffic to different backends based on rules you define via the Kubernetes API. FEATURE STATE: Kubernetes v1.19 [stable]An API object that manages external access to the services in a cluster, typically HTTP.Ingress may provide load balancing, SSL termination and name-based virtual hosting.",107
6.2 - Ingress,Terminology,"Terminology For clarity, this guide defines the following terms: Node: A worker machine in Kubernetes, part of a cluster.Cluster: A set of Nodes that run containerized applications managed by Kubernetes. For this example, and in most common Kubernetes deployments, nodes in the cluster are not part of the public internet.Edge router: A router that enforces the firewall policy for your cluster. This could be a gateway managed by a cloud provider or a physical piece of hardware.Cluster network: A set of links, logical or physical, that facilitate communication within a cluster according to the Kubernetes networking model.Service: A Kubernetes Service that identifies a set of Pods using label selectors. Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network.",175
6.2 - Ingress,What is Ingress?,"What is Ingress? Ingress exposes HTTP and HTTPS routes from outside the cluster toservices within the cluster.Traffic routing is controlled by rules defined on the Ingress resource. Here is a simple example where an Ingress sends all its traffic to one Service: Figure. Ingress An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typicallyuses a service of type Service.Type=NodePort orService.Type=LoadBalancer.",169
6.2 - Ingress,Prerequisites,"Prerequisites You must have an Ingress controller to satisfy an Ingress. Only creating an Ingress resource has no effect. You may need to deploy an Ingress controller such as ingress-nginx. You can choose from a number ofIngress controllers. Ideally, all Ingress controllers should fit the reference specification. In reality, the various Ingresscontrollers operate slightly differently. Note: Make sure you review your Ingress controller's documentation to understand the caveats of choosing it.",98
6.2 - Ingress,The Ingress resource,"The Ingress resource A minimal Ingress resource example: service/networking/minimal-ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: minimal-ingress  annotations:    nginx.ingress.kubernetes.io/rewrite-target: /spec:  ingressClassName: nginx-example  rules:  - http:      paths:      - path: /testpath        pathType: Prefix        backend:          service:            name: test            port:              number: 80 An Ingress needs apiVersion, kind, metadata and spec fields.The name of an Ingress object must be a validDNS subdomain name.For general information about working with config files, see deploying applications, configuring containers, managing resources.Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of whichis the rewrite-target annotation.Different Ingress controllers support different annotations. Review the documentation foryour choice of Ingress controller to learn which annotations are supported. The Ingress spechas all the information needed to configure a load balancer or proxy server. Most importantly, itcontains a list of rules matched against all incoming requests. Ingress resource only supports rulesfor directing HTTP(S) traffic. If the ingressClassName is omitted, a default Ingress classshould be defined. There are some ingress controllers, that work without the definition of adefault IngressClass. For example, the Ingress-NGINX controller can beconfigured with a flag--watch-ingress-without-class. It is recommended though, to specify thedefault IngressClass as shown below.",360
6.2 - Ingress,Ingress rules,"Ingress rules Each HTTP rule contains the following information: An optional host. In this example, no host is specified, so the rule applies to all inboundHTTP traffic through the IP address specified. If a host is provided (for example,foo.bar.com), the rules apply to that host.A list of paths (for example, /testpath), each of which has an associatedbackend defined with a service.name and a service.port.name orservice.port.number. Both the host and path must match the content of anincoming request before the load balancer directs traffic to the referencedService.A backend is a combination of Service and port names as described in theService doc or a custom resource backend by way of a CRD. HTTP (and HTTPS) requests to theIngress that match the host and path of the rule are sent to the listed backend. A defaultBackend is often configured in an Ingress controller to service any requests that do notmatch a path in the spec.",208
6.2 - Ingress,DefaultBackend,"DefaultBackend An Ingress with no rules sends all traffic to a single default backend and .spec.defaultBackendis the backend that should handle requests in that case.The defaultBackend is conventionally a configuration option of theIngress controller andis not specified in your Ingress resources.If no .spec.rules are specified, .spec.defaultBackend must be specified.If defaultBackend is not set, the handling of requests that do not match any of the rules will be up to theingress controller (consult the documentation for your ingress controller to find out how it handles this case). If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic isrouted to your default backend.",153
6.2 - Ingress,Resource backends,"Resource backends A Resource backend is an ObjectRef to another Kubernetes resource within thesame namespace as the Ingress object. A Resource is a mutually exclusivesetting with Service, and will fail validation if both are specified. A commonusage for a Resource backend is to ingress data to an object storage backendwith static assets. service/networking/ingress-resource-backend.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: ingress-resource-backendspec:  defaultBackend:    resource:      apiGroup: k8s.example.com      kind: StorageBucket      name: static-assets  rules:    - http:        paths:          - path: /icons            pathType: ImplementationSpecific            backend:              resource:                apiGroup: k8s.example.com                kind: StorageBucket                name: icon-assets After creating the Ingress above, you can view it with the following command: kubectl describe ingress ingress-resource-backend Name:             ingress-resource-backendNamespace:        defaultAddress:Default backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assetsRules:  Host        Path  Backends  ----        ----  --------  *              /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assetsAnnotations:  <none>Events:       <none>",327
6.2 - Ingress,Path types,"Path types Each path in an Ingress is required to have a corresponding path type. Pathsthat do not include an explicit pathType will fail validation. There are threesupported path types: ImplementationSpecific: With this path type, matching is up to theIngressClass. Implementations can treat this as a separate pathType or treatit identically to Prefix or Exact path types.Exact: Matches the URL path exactly and with case sensitivity.Prefix: Matches based on a URL path prefix split by /. Matching is casesensitive and done on a path element by element basis. A path element refersto the list of labels in the path split by the / separator. A request is amatch for path p if every p is an element-wise prefix of p of therequest path.Note: If the last element of the path is a substring of the lastelement in request path, it is not a match (for example: /foo/barmatches /foo/bar/baz, but does not match /foo/barbaz).",225
6.2 - Ingress,Examples,"Examples KindPath(s)Request path(s)Matches?Prefix/(all paths)YesExact/foo/fooYesExact/foo/barNoExact/foo/foo/NoExact/foo//fooNoPrefix/foo/foo, /foo/YesPrefix/foo//foo, /foo/YesPrefix/aaa/bb/aaa/bbbNoPrefix/aaa/bbb/aaa/bbbYesPrefix/aaa/bbb//aaa/bbbYes, ignores trailing slashPrefix/aaa/bbb/aaa/bbb/Yes, matches trailing slashPrefix/aaa/bbb/aaa/bbb/cccYes, matches subpathPrefix/aaa/bbb/aaa/bbbxyzNo, does not match string prefixPrefix/, /aaa/aaa/cccYes, matches /aaa prefixPrefix/, /aaa, /aaa/bbb/aaa/bbbYes, matches /aaa/bbb prefixPrefix/, /aaa, /aaa/bbb/cccYes, matches / prefixPrefix/aaa/cccNo, uses default backendMixed/foo (Prefix), /foo (Exact)/fooYes, prefers Exact",265
6.2 - Ingress,Multiple matches,"Multiple matches In some cases, multiple paths within an Ingress will match a request. In thosecases precedence will be given first to the longest matching path. If two pathsare still equally matched, precedence will be given to paths with an exact pathtype over prefix path type.",55
6.2 - Ingress,Hostname wildcards,"Hostname wildcards Hosts can be precise matches (for example “foo.bar.com”) or a wildcard (forexample “*.foo.com”). Precise matches require that the HTTP host headermatches the host field. Wildcard matches require the HTTP host header isequal to the suffix of the wildcard rule. HostHost headerMatch?*.foo.combar.foo.comMatches based on shared suffix*.foo.combaz.bar.foo.comNo match, wildcard only covers a single DNS label*.foo.comfoo.comNo match, wildcard only covers a single DNS label service/networking/ingress-wildcard-host.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: ingress-wildcard-hostspec:  rules:  - host: ""foo.bar.com""    http:      paths:      - pathType: Prefix        path: ""/bar""        backend:          service:            name: service1            port:              number: 80  - host: ""*.foo.com""    http:      paths:      - pathType: Prefix        path: ""/foo""        backend:          service:            name: service2            port:              number: 80",278
6.2 - Ingress,Ingress class,"Ingress class Ingresses can be implemented by different controllers, often with differentconfiguration. Each Ingress should specify a class, a reference to anIngressClass resource that contains additional configuration including the nameof the controller that should implement the class. service/networking/external-lb.yamlapiVersion: networking.k8s.io/v1kind: IngressClassmetadata:  name: external-lbspec:  controller: example.com/ingress-controller  parameters:    apiGroup: k8s.example.com    kind: IngressParameters    name: external-lb The .spec.parameters field of an IngressClass lets you reference anotherresource that provides configuration related to that IngressClass. The specific type of parameters to use depends on the ingress controllerthat you specify in the .spec.controller field of the IngressClass.",183
6.2 - Ingress,IngressClass scope,"IngressClass scope Depending on your ingress controller, you may be able to use parametersthat you set cluster-wide, or just for one namespace. ClusterNamespaced The default scope for IngressClass parameters is cluster-wide.If you set the .spec.parameters field and don't set.spec.parameters.scope, or if you set .spec.parameters.scope toCluster, then the IngressClass refers to a cluster-scoped resource.The kind (in combination the apiGroup) of the parametersrefers to a cluster-scoped API (possibly a custom resource), andthe name of the parameters identifies a specific cluster scopedresource for that API.For example:---apiVersion: networking.k8s.io/v1kind: IngressClassmetadata:  name: external-lb-1spec:  controller: example.com/ingress-controller  parameters:    # The parameters for this IngressClass are specified in a    # ClusterIngressParameter (API group k8s.example.net) named    # ""external-config-1"". This definition tells Kubernetes to    # look for a cluster-scoped parameter resource.    scope: Cluster    apiGroup: k8s.example.net    kind: ClusterIngressParameter    name: external-config-1FEATURE STATE: Kubernetes v1.23 [stable]If you set the .spec.parameters field and set.spec.parameters.scope to Namespace, then the IngressClass refersto a namespaced-scoped resource. You must also set the namespacefield within .spec.parameters to the namespace that containsthe parameters you want to use.The kind (in combination the apiGroup) of the parametersrefers to a namespaced API (for example: ConfigMap), andthe name of the parameters identifies a specific resourcein the namespace you specified in namespace.Namespace-scoped parameters help the cluster operator delegate control over theconfiguration (for example: load balancer settings, API gateway definition)that is used for a workload. If you used a cluster-scoped parameter then either:the cluster operator team needs to approve a different team's changes everytime there's a new configuration change being applied.the cluster operator must define specific access controls, such asRBAC roles and bindings, that letthe application team make changes to the cluster-scoped parameters resource.The IngressClass API itself is always cluster-scoped.Here is an example of an IngressClass that refers to parameters that arenamespaced:---apiVersion: networking.k8s.io/v1kind: IngressClassmetadata:  name: external-lb-2spec:  controller: example.com/ingress-controller  parameters:    # The parameters for this IngressClass are specified in an    # IngressParameter (API group k8s.example.com) named ""external-config"",    # that's in the ""external-configuration"" namespace.    scope: Namespace    apiGroup: k8s.example.com    kind: IngressParameter    namespace: external-configuration    name: external-config",669
6.2 - Ingress,Deprecated annotation,"Deprecated annotation Before the IngressClass resource and ingressClassName field were added inKubernetes 1.18, Ingress classes were specified with akubernetes.io/ingress.class annotation on the Ingress. This annotation wasnever formally defined, but was widely supported by Ingress controllers. The newer ingressClassName field on Ingresses is a replacement for thatannotation, but is not a direct equivalent. While the annotation was generallyused to reference the name of the Ingress controller that should implement theIngress, the field is a reference to an IngressClass resource that containsadditional Ingress configuration, including the name of the Ingress controller.",141
6.2 - Ingress,Default IngressClass,"Default IngressClass You can mark a particular IngressClass as default for your cluster. Setting theingressclass.kubernetes.io/is-default-class annotation to true on anIngressClass resource will ensure that new Ingresses without aningressClassName field specified will be assigned this default IngressClass. Caution: If you have more than one IngressClass marked as the default for your cluster,the admission controller prevents creating new Ingress objects that don't havean ingressClassName specified. You can resolve this by ensuring that at most 1IngressClass is marked as default in your cluster. There are some ingress controllers, that work without the definition of adefault IngressClass. For example, the Ingress-NGINX controller can beconfigured with a flag--watch-ingress-without-class. It is recommended though, to specify thedefault IngressClass: service/networking/default-ingressclass.yamlapiVersion: networking.k8s.io/v1kind: IngressClassmetadata:  labels:    app.kubernetes.io/component: controller  name: nginx-example  annotations:    ingressclass.kubernetes.io/is-default-class: ""true""spec:  controller: k8s.io/ingress-nginx",287
6.2 - Ingress,Ingress backed by a single Service,"Ingress backed by a single Service There are existing Kubernetes concepts that allow you to expose a single Service(see alternatives). You can also do this with an Ingress by specifying adefault backend with no rules. service/networking/test-ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: test-ingressspec:  defaultBackend:    service:      name: test      port:        number: 80 If you create it using kubectl apply -f you should be able to view the stateof the Ingress you added: kubectl get ingress test-ingress NAME           CLASS         HOSTS   ADDRESS         PORTS   AGEtest-ingress   external-lb   *       203.0.113.123   80      59s Where 203.0.113.123 is the IP allocated by the Ingress controller to satisfythis Ingress. Note: Ingress controllers and load balancers may take a minute or two to allocate an IP address.Until that time, you often see the address listed as <pending>.",243
6.2 - Ingress,Simple fanout,"Simple fanout A fanout configuration routes traffic from a single IP address to more than one Service,based on the HTTP URI being requested. An Ingress allows you to keep the number of load balancersdown to a minimum. For example, a setup like: Figure. Ingress Fan Out would require an Ingress such as: service/networking/simple-fanout-example.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: simple-fanout-examplespec:  rules:  - host: foo.bar.com    http:      paths:      - path: /foo        pathType: Prefix        backend:          service:            name: service1            port:              number: 4200      - path: /bar        pathType: Prefix        backend:          service:            name: service2            port:              number: 8080 When you create the Ingress with kubectl apply -f: kubectl describe ingress simple-fanout-example Name:             simple-fanout-exampleNamespace:        defaultAddress:          178.91.123.132Default backend:  default-http-backend:80 (10.8.2.3:8080)Rules:  Host         Path  Backends  ----         ----  --------  foo.bar.com               /foo   service1:4200 (10.8.0.90:4200)               /bar   service2:8080 (10.8.0.91:8080)Events:  Type     Reason  Age                From                     Message  ----     ------  ----               ----                     -------  Normal   ADD     22s                loadbalancer-controller  default/test The Ingress controller provisions an implementation-specific load balancerthat satisfies the Ingress, as long as the Services (service1, service2) exist.When it has done so, you can see the address of the load balancer at theAddress field. Note: Depending on the Ingress controlleryou are using, you may need to create a default-http-backendService.",451
6.2 - Ingress,Name based virtual hosting,"Name based virtual hosting Name-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address. Figure. Ingress Name Based Virtual hosting The following Ingress tells the backing load balancer to route requests based onthe Host header. service/networking/name-virtual-host-ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: name-virtual-host-ingressspec:  rules:  - host: foo.bar.com    http:      paths:      - pathType: Prefix        path: ""/""        backend:          service:            name: service1            port:              number: 80  - host: bar.foo.com    http:      paths:      - pathType: Prefix        path: ""/""        backend:          service:            name: service2            port:              number: 80 If you create an Ingress resource without any hosts defined in the rules, then anyweb traffic to the IP address of your Ingress controller can be matched without a name basedvirtual host being required. For example, the following Ingress routes trafficrequested for first.bar.com to service1, second.bar.com to service2, and any traffic whose request host header doesn't match first.bar.com and second.bar.com to service3. service/networking/name-virtual-host-ingress-no-third-host.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: name-virtual-host-ingress-no-third-hostspec:  rules:  - host: first.bar.com    http:      paths:      - pathType: Prefix        path: ""/""        backend:          service:            name: service1            port:              number: 80  - host: second.bar.com    http:      paths:      - pathType: Prefix        path: ""/""        backend:          service:            name: service2            port:              number: 80  - http:      paths:      - pathType: Prefix        path: ""/""        backend:          service:            name: service3            port:              number: 80",476
6.2 - Ingress,TLS,"TLS You can secure an Ingress by specifying a Secretthat contains a TLS private key and certificate. The Ingress resource onlysupports a single TLS port, 443, and assumes TLS termination at the ingress point(traffic to the Service and its Pods is in plaintext).If the TLS configuration section in an Ingress specifies different hosts, they aremultiplexed on the same port according to the hostname specified through theSNI TLS extension (provided the Ingress controller supports SNI). The TLS secretmust contain keys named tls.crt and tls.key that contain the certificateand private key to use for TLS. For example: apiVersion: v1kind: Secretmetadata:  name: testsecret-tls  namespace: defaultdata:  tls.crt: base64 encoded cert  tls.key: base64 encoded keytype: kubernetes.io/tls Referencing this secret in an Ingress tells the Ingress controller tosecure the channel from the client to the load balancer using TLS. You need to makesure the TLS secret you created came from a certificate that contains a CommonName (CN), also known as a Fully Qualified Domain Name (FQDN) for https-example.foo.com. Note: Keep in mind that TLS will not work on the default rule because thecertificates would have to be issued for all the possible sub-domains. Therefore,hosts in the tls section need to explicitly match the host in the rulessection. service/networking/tls-example-ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: tls-example-ingressspec:  tls:  - hosts:      - https-example.foo.com    secretName: testsecret-tls  rules:  - host: https-example.foo.com    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: service1            port:              number: 80 Note: There is a gap between TLS features supported by various Ingresscontrollers. Please refer to documentation onnginx,GCE, or any otherplatform specific Ingress controller to understand how TLS works in your environment.",484
6.2 - Ingress,Load balancing,"Load balancing An Ingress controller is bootstrapped with some load balancing policy settingsthat it applies to all Ingress, such as the load balancing algorithm, backendweight scheme, and others. More advanced load balancing concepts(e.g. persistent sessions, dynamic weights) are not yet exposed through theIngress. You can instead get these features through the load balancer used fora Service. It's also worth noting that even though health checks are not exposed directlythrough the Ingress, there exist parallel concepts in Kubernetes such asreadiness probesthat allow you to achieve the same end result. Please review the controllerspecific documentation to see how they handle health checks (for example:nginx, orGCE).",149
6.2 - Ingress,Updating an Ingress,"Updating an Ingress To update an existing Ingress to add a new Host, you can update it by editing the resource: kubectl describe ingress test Name:             testNamespace:        defaultAddress:          178.91.123.132Default backend:  default-http-backend:80 (10.8.2.3:8080)Rules:  Host         Path  Backends  ----         ----  --------  foo.bar.com               /foo   service1:80 (10.8.0.90:80)Annotations:  nginx.ingress.kubernetes.io/rewrite-target:  /Events:  Type     Reason  Age                From                     Message  ----     ------  ----               ----                     -------  Normal   ADD     35s                loadbalancer-controller  default/test kubectl edit ingress test This pops up an editor with the existing configuration in YAML format.Modify it to include the new Host: spec:  rules:  - host: foo.bar.com    http:      paths:      - backend:          service:            name: service1            port:              number: 80        path: /foo        pathType: Prefix  - host: bar.baz.com    http:      paths:      - backend:          service:            name: service2            port:              number: 80        path: /foo        pathType: Prefix.. After you save your changes, kubectl updates the resource in the API server, which tells theIngress controller to reconfigure the load balancer. Verify this: kubectl describe ingress test Name:             testNamespace:        defaultAddress:          178.91.123.132Default backend:  default-http-backend:80 (10.8.2.3:8080)Rules:  Host         Path  Backends  ----         ----  --------  foo.bar.com               /foo   service1:80 (10.8.0.90:80)  bar.baz.com               /foo   service2:80 (10.8.0.91:80)Annotations:  nginx.ingress.kubernetes.io/rewrite-target:  /Events:  Type     Reason  Age                From                     Message  ----     ------  ----               ----                     -------  Normal   ADD     45s                loadbalancer-controller  default/test You can achieve the same outcome by invoking kubectl replace -f on a modified Ingress YAML file.",549
6.2 - Ingress,Alternatives,Alternatives You can expose a Service in multiple ways that don't directly involve the Ingress resource: Use Service.Type=LoadBalancerUse Service.Type=NodePort Learn about the Ingress APILearn about Ingress controllersSet up Ingress on Minikube with the NGINX Controller,62
6.3 - Ingress Controllers,default,"In order for an Ingress to work in your cluster, there must be an ingress controller running. You need to select at least one ingress controller and make sure it is set up in your cluster. This page lists common ingress controllers that you can deploy. In order for the Ingress resource to work, the cluster must have an ingress controller running. Unlike other types of controllers which run as part of the kube-controller-manager binary, Ingress controllersare not started automatically with a cluster. Use this page to choose the ingress controller implementationthat best fits your cluster. Kubernetes as a project supports and maintains AWS, GCE, andnginx ingress controllers.",142
6.3 - Ingress Controllers,Additional controllers,"Additional controllers Note:This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information. AKS Application Gateway Ingress Controller is an ingress controller that configures the Azure Application Gateway.Ambassador API Gateway is an Envoy-based ingresscontroller.Apache APISIX ingress controller is an Apache APISIX-based ingress controller.Avi Kubernetes Operator provides L4-L7 load-balancing using VMware NSX Advanced Load Balancer.BFE Ingress Controller is a BFE-based ingress controller.The Citrix ingress controller works withCitrix Application Delivery Controller.Contour is an Envoy based ingress controller.EnRoute is an Envoy based API gateway that can run as an ingress controller.Easegress IngressController is an Easegress based API gateway that can run as an ingress controller.F5 BIG-IP Container Ingress Services for Kuberneteslets you use an Ingress to configure F5 BIG-IP virtual servers.Gloo is an open-source ingress controller based on Envoy,which offers API gateway functionality.HAProxy Ingress is an ingress controller forHAProxy.The HAProxy Ingress Controller for Kubernetesis also an ingress controller for HAProxy.Istio Ingressis an Istio based ingress controller.The Kong Ingress Controller for Kubernetesis an ingress controller driving Kong Gateway.Kusk Gateway is an OpenAPI-driven ingress controller based on Envoy.The NGINX Ingress Controller for Kubernetesworks with the NGINX webserver (as a proxy).The Pomerium Ingress Controller is based on Pomerium, which offers context-aware access policy.Skipper HTTP router and reverse proxy for service composition, including use cases like Kubernetes Ingress, designed as a library to build your custom proxy.The Traefik Kubernetes Ingress provider is aningress controller for the Traefik proxy.Tyk Operator extends Ingress with Custom Resources to bring API Management capabilities to Ingress. Tyk Operator works with the Open Source Tyk Gateway & Tyk Cloud control plane.Voyager is an ingress controller forHAProxy.Wallarm Ingress Controller is an Ingress Controller that provides WAAP (WAF) and API Security capabilities.",531
6.3 - Ingress Controllers,Using multiple Ingress controllers,"Using multiple Ingress controllers You may deploy any number of ingress controllers using ingress classwithin a cluster. Note the .metadata.name of your ingress class resource. When you create an ingress you would need that name to specify the ingressClassName field on your Ingress object (refer to IngressSpec v1 reference). ingressClassName is a replacement of the older annotation method. If you do not specify an IngressClass for an Ingress, and your cluster has exactly one IngressClass marked as default, then Kubernetes applies the cluster's default IngressClass to the Ingress.You mark an IngressClass as default by setting the ingressclass.kubernetes.io/is-default-class annotation on that IngressClass, with the string value ""true"". Ideally, all ingress controllers should fulfill this specification, but the various ingresscontrollers operate slightly differently. Note: Make sure you review your ingress controller's documentation to understand the caveats of choosing it. Learn more about Ingress.Set up Ingress on Minikube with the NGINX Controller.",232
6.4 - EndpointSlices,default,"The EndpointSlice API is the mechanism that Kubernetes uses to let your Service scale to handle large numbers of backends, and allows the cluster to update its list of healthy backends efficiently. FEATURE STATE: Kubernetes v1.21 [stable] Kubernetes' EndpointSlice API provides a way to track network endpointswithin a Kubernetes cluster. EndpointSlices offer a more scalable and extensiblealternative to Endpoints.",102
6.4 - EndpointSlices,EndpointSlice API,"EndpointSlice API In Kubernetes, an EndpointSlice contains references to a set of networkendpoints. The control plane automatically creates EndpointSlicesfor any Kubernetes Service that has a selector specified. These EndpointSlices includereferences to all the Pods that match the Service selector. EndpointSlices groupnetwork endpoints together by unique combinations of protocol, port number, andService name.The name of a EndpointSlice object must be a validDNS subdomain name. As an example, here's a sample EndpointSlice object, that's owned by the exampleKubernetes Service. apiVersion: discovery.k8s.io/v1kind: EndpointSlicemetadata:  name: example-abc  labels:    kubernetes.io/service-name: exampleaddressType: IPv4ports:  - name: http    protocol: TCP    port: 80endpoints:  - addresses:      - ""10.1.2.3""    conditions:      ready: true    hostname: pod-1    nodeName: node-1    zone: us-west2-a By default, the control plane creates and manages EndpointSlices to have nomore than 100 endpoints each. You can configure this with the--max-endpoints-per-slicekube-controller-managerflag, up to a maximum of 1000. EndpointSlices can act as the source of truth forkube-proxy when it comes tohow to route internal traffic.",330
6.4 - EndpointSlices,Address types,"Address types EndpointSlices support three address types: IPv4IPv6FQDN (Fully Qualified Domain Name) Each EndpointSlice object represents a specific IP address type. If you havea Service that is available via IPv4 and IPv6, there will be at least twoEndpointSlice objects (one for IPv4, and one for IPv6).",79
6.4 - EndpointSlices,Ready,"Ready ready is a condition that maps to a Pod's Ready condition. A running Pod with the Readycondition set to True should have this EndpointSlice condition also set to true. Forcompatibility reasons, ready is NEVER true when a Pod is terminating. Consumers should referto the serving condition to inspect the readiness of terminating Pods. The only exception tothis rule is for Services with spec.publishNotReadyAddresses set to true. Endpoints for theseServices will always have the ready condition set to true.",105
6.4 - EndpointSlices,Serving,"Serving FEATURE STATE: Kubernetes v1.22 [beta] serving is identical to the ready condition, except it does not account for terminating states.Consumers of the EndpointSlice API should check this condition if they care about pod readiness whilethe pod is also terminating. Note: Although serving is almost identical to ready, it was added to prevent breaking the existing meaningof ready. It may be unexpected for existing clients if ready could be true for terminatingendpoints, since historically terminating endpoints were never included in the Endpoints orEndpointSlice API to begin with. For this reason, ready is always false for terminatingendpoints, and a new condition serving was added in v1.20 so that clients can track readinessfor terminating pods independent of the existing semantics for ready.",163
6.4 - EndpointSlices,Terminating,"Terminating FEATURE STATE: Kubernetes v1.22 [beta] Terminating is a condition that indicates whether an endpoint is terminating.For pods, this is any pod that has a deletion timestamp set.",44
6.4 - EndpointSlices,Topology information,"Topology information Each endpoint within an EndpointSlice can contain relevant topology information.The topology information includes the location of the endpoint and informationabout the corresponding Node and zone. These are available in the followingper endpoint fields on EndpointSlices: nodeName - The name of the Node this endpoint is on.zone - The zone this endpoint is in. Note:In the v1 API, the per endpoint topology was effectively removed in favor ofthe dedicated fields nodeName and zone.Setting arbitrary topology fields on the endpoint field of an EndpointSliceresource has been deprecated and is not supported in the v1 API.Instead, the v1 API supports setting individual nodeName and zone fields.These fields are automatically translated between API versions. For example, thevalue of the ""topology.kubernetes.io/zone"" key in the topology field inthe v1beta1 API is accessible as the zone field in the v1 API.",200
6.4 - EndpointSlices,Management,"Management Most often, the control plane (specifically, the endpoint slicecontroller) creates andmanages EndpointSlice objects. There are a variety of other use cases forEndpointSlices, such as service mesh implementations, that could result in otherentities or controllers managing additional sets of EndpointSlices. To ensure that multiple entities can manage EndpointSlices without interferingwith each other, Kubernetes defines thelabelendpointslice.kubernetes.io/managed-by, which indicates the entity managingan EndpointSlice.The endpoint slice controller sets endpointslice-controller.k8s.io as the valuefor this label on all EndpointSlices it manages. Other entities managingEndpointSlices should also set a unique value for this label.",167
6.4 - EndpointSlices,Ownership,"Ownership In most use cases, EndpointSlices are owned by the Service that the endpointslice object tracks endpoints for. This ownership is indicated by an ownerreference on each EndpointSlice as well as a kubernetes.io/service-namelabel that enables simple lookups of all EndpointSlices belonging to a Service.",74
6.4 - EndpointSlices,EndpointSlice mirroring,"EndpointSlice mirroring In some cases, applications create custom Endpoints resources. To ensure thatthese applications do not need to concurrently write to both Endpoints andEndpointSlice resources, the cluster's control plane mirrors most Endpointsresources to corresponding EndpointSlices. The control plane mirrors Endpoints resources unless: the Endpoints resource has a endpointslice.kubernetes.io/skip-mirror labelset to true.the Endpoints resource has a control-plane.alpha.kubernetes.io/leaderannotation.the corresponding Service resource does not exist.the corresponding Service resource has a non-nil selector. Individual Endpoints resources may translate into multiple EndpointSlices. Thiswill occur if an Endpoints resource has multiple subsets or includes endpointswith multiple IP families (IPv4 and IPv6). A maximum of 1000 addresses persubset will be mirrored to EndpointSlices.",193
6.4 - EndpointSlices,Distribution of EndpointSlices,"Distribution of EndpointSlices Each EndpointSlice has a set of ports that applies to all endpoints within theresource. When named ports are used for a Service, Pods may end up withdifferent target port numbers for the same named port, requiring differentEndpointSlices. This is similar to the logic behind how subsets are groupedwith Endpoints. The control plane tries to fill EndpointSlices as full as possible, but does notactively rebalance them. The logic is fairly straightforward: Iterate through existing EndpointSlices, remove endpoints that are no longerdesired and update matching endpoints that have changed.Iterate through EndpointSlices that have been modified in the first step andfill them up with any new endpoints needed.If there's still new endpoints left to add, try to fit them into a previouslyunchanged slice and/or create new ones. Importantly, the third step prioritizes limiting EndpointSlice updates over aperfectly full distribution of EndpointSlices. As an example, if there are 10new endpoints to add and 2 EndpointSlices with room for 5 more endpoints each,this approach will create a new EndpointSlice instead of filling up the 2existing EndpointSlices. In other words, a single EndpointSlice creation ispreferrable to multiple EndpointSlice updates. With kube-proxy running on each Node and watching EndpointSlices, every changeto an EndpointSlice becomes relatively expensive since it will be transmitted toevery Node in the cluster. This approach is intended to limit the number ofchanges that need to be sent to every Node, even if it may result with multipleEndpointSlices that are not full. In practice, this less than ideal distribution should be rare. Most changesprocessed by the EndpointSlice controller will be small enough to fit in anexisting EndpointSlice, and if not, a new EndpointSlice is likely going to benecessary soon anyway. Rolling updates of Deployments also provide a naturalrepacking of EndpointSlices with all Pods and their corresponding endpointsgetting replaced.",458
6.4 - EndpointSlices,Duplicate endpoints,"Duplicate endpoints Due to the nature of EndpointSlice changes, endpoints may be represented in morethan one EndpointSlice at the same time. This naturally occurs as changes todifferent EndpointSlice objects can arrive at the Kubernetes client watch / cacheat different times. Note:Clients of the EndpointSlice API must iterate through all the existing EndpointSlicesassociated to a Service and build a complete list of unique network endpoints. It isimportant to mention that endpoints may be duplicated in different EndointSlices.You can find a reference implementation for how to perform this endpoint aggregationand deduplication as part of the EndpointSliceCache code within kube-proxy.",154
6.4 - EndpointSlices,Comparison with Endpoints,"Comparison with Endpoints The original Endpoints API provided a simple and straightforward way oftracking network endpoints in Kubernetes. As Kubernetes clustersand Services grew to handlemore traffic and to send more traffic to more backend Pods, thelimitations of that original API became more visible.Most notably, those included challenges with scaling to larger numbers ofnetwork endpoints. Since all network endpoints for a Service were stored in a single Endpointsobject, those Endpoints objects could get quite large. For Services that stayedstable (the same set of endpoints over a long period of time) the impact wasless noticeable; even then, some use cases of Kubernetes weren't well served. When a Service had a lot of backend endpoints and the workload was eitherscaling frequently, or rolling out new changes frequently, each update tothe single Endpoints object for that Service meant a lot of traffic betweenKubernetes cluster components (within the control plane, and also betweennodes and the API server). This extra traffic also had a cost in terms ofCPU use. With EndpointSlices, adding or removing a single Pod triggers the same numberof updates to clients that are watching for changes, but the size of thoseupdate message is much smaller at large scale. EndpointSlices also enabled innovation around new features such dual-stacknetworking and topology-aware routing. Follow the Connecting Applications with Services tutorialRead the API reference for the EndpointSlice APIRead the API reference for the Endpoints API",319
6.5 - Network Policies,default,"If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), NetworkPolicies allow you to specify rules for traffic flow within your cluster, and also between Pods and the outside world. Your cluster must use a network plugin that supports NetworkPolicy enforcement. If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), then youmight consider using Kubernetes NetworkPolicies for particular applications in your cluster.NetworkPolicies are an application-centric construct which allow you to specify how a pod is allowed to communicate with various network""entities"" (we use the word ""entity"" here to avoid overloading the more common terms such as""endpoints"" and ""services"", which have specific Kubernetes connotations) over the network.NetworkPolicies apply to a connection with a pod on one or both ends, and are not relevant toother connections. The entities that a Pod can communicate with are identified through a combination of the following3 identifiers: Other pods that are allowed (exception: a pod cannot block access to itself)Namespaces that are allowedIP blocks (exception: traffic to and from the node where a Pod is running is always allowed,regardless of the IP address of the Pod or the node) When defining a pod- or namespace- based NetworkPolicy, you use aselector to specify what traffic is allowed toand from the Pod(s) that match the selector. Meanwhile, when IP based NetworkPolicies are created, we define policies based on IP blocks (CIDR ranges).",334
6.5 - Network Policies,Prerequisites,"Prerequisites Network policies are implemented by the network plugin.To use network policies, you must be using a networking solution which supports NetworkPolicy.Creating a NetworkPolicy resource without a controller that implements it will have no effect.",44
6.5 - Network Policies,The Two Sorts of Pod Isolation,"The Two Sorts of Pod Isolation There are two sorts of isolation for a pod: isolation for egress, and isolation for ingress.They concern what connections may be established. ""Isolation"" here is not absolute, rather itmeans ""some restrictions apply"". The alternative, ""non-isolated for $direction"", means that norestrictions apply in the stated direction. The two sorts of isolation (or not) are declaredindependently, and are both relevant for a connection from one pod to another. By default, a pod is non-isolated for egress; all outbound connections are allowed.A pod is isolated for egress if there is any NetworkPolicy that both selects the pod and has""Egress"" in its policyTypes; we say that such a policy applies to the pod for egress.When a pod is isolated for egress, the only allowed connections from the pod are those allowed bythe egress list of some NetworkPolicy that applies to the pod for egress.The effects of those egress lists combine additively. By default, a pod is non-isolated for ingress; all inbound connections are allowed.A pod is isolated for ingress if there is any NetworkPolicy that both selects the pod andhas ""Ingress"" in its policyTypes; we say that such a policy applies to the pod for ingress.When a pod is isolated for ingress, the only allowed connections into the pod are those fromthe pod's node and those allowed by the ingress list of some NetworkPolicy that applies tothe pod for ingress. The effects of those ingress lists combine additively. Network policies do not conflict; they are additive. If any policy or policies apply to a givenpod for a given direction, the connections allowed in that direction from that pod is the union ofwhat the applicable policies allow. Thus, order of evaluation does not affect the policy result. For a connection from a source pod to a destination pod to be allowed, both the egress policy onthe source pod and the ingress policy on the destination pod need to allow the connection. Ifeither side does not allow the connection, it will not happen.",447
6.5 - Network Policies,The NetworkPolicy resource,"The NetworkPolicy resource See the NetworkPolicyreference for a full definition of the resource. An example NetworkPolicy might look like this: service/networking/networkpolicy.yamlapiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: test-network-policy  namespace: defaultspec:  podSelector:    matchLabels:      role: db  policyTypes:    - Ingress    - Egress  ingress:    - from:        - ipBlock:            cidr: 172.17.0.0/16            except:              - 172.17.1.0/24        - namespaceSelector:            matchLabels:              project: myproject        - podSelector:            matchLabels:              role: frontend      ports:        - protocol: TCP          port: 6379  egress:    - to:        - ipBlock:            cidr: 10.0.0.0/24      ports:        - protocol: TCP          port: 5978 Note: POSTing this to the API server for your cluster will have no effect unless your chosen networkingsolution supports network policy. Mandatory Fields: As with all other Kubernetes config, a NetworkPolicy needs apiVersion,kind, and metadata fields. For general information about working with config files, seeConfigure a Pod to Use a ConfigMap,and Object Management. spec: NetworkPolicy spechas all the information needed to define a particular network policy in the given namespace. podSelector: Each NetworkPolicy includes a podSelector which selects the grouping of pods towhich the policy applies. The example policy selects pods with the label ""role=db"". An emptypodSelector selects all pods in the namespace. policyTypes: Each NetworkPolicy includes a policyTypes list which may include eitherIngress, Egress, or both. The policyTypes field indicates whether or not the given policyapplies to ingress traffic to selected pod, egress traffic from selected pods, or both. If nopolicyTypes are specified on a NetworkPolicy then by default Ingress will always be set andEgress will be set if the NetworkPolicy has any egress rules. ingress: Each NetworkPolicy may include a list of allowed ingress rules. Each rule allowstraffic which matches both the from and ports sections. The example policy contains a singlerule, which matches traffic on a single port, from one of three sources, the first specified viaan ipBlock, the second via a namespaceSelector and the third via a podSelector. egress: Each NetworkPolicy may include a list of allowed egress rules. Each rule allowstraffic which matches both the to and ports sections. The example policy contains a singlerule, which matches traffic on a single port to any destination in 10.0.0.0/24. So, the example NetworkPolicy: isolates role=db pods in the default namespace for both ingress and egress traffic(if they weren't already isolated)(Ingress rules) allows connections to all pods in the default namespace with the labelrole=db on TCP port 6379 from:any pod in the default namespace with the label role=frontendany pod in a namespace with the label project=myprojectIP addresses in the ranges 172.17.0.0–172.17.0.255 and 172.17.2.0–172.17.255.255(ie, all of 172.17.0.0/16 except 172.17.1.0/24)(Egress rules) allows connections from any pod in the default namespace with the labelrole=db to CIDR 10.0.0.0/24 on TCP port 5978 See the Declare Network Policywalkthrough for further examples.",791
6.5 - Network Policies,Behavior of to and from selectors,"Behavior of to and from selectors There are four kinds of selectors that can be specified in an ingress from section or egressto section: podSelector: This selects particular Pods in the same namespace as the NetworkPolicy whichshould be allowed as ingress sources or egress destinations. namespaceSelector: This selects particular namespaces for which all Pods should be allowed asingress sources or egress destinations. namespaceSelector and podSelector: A single to/from entry that specifies bothnamespaceSelector and podSelector selects particular Pods within particular namespaces. Becareful to use correct YAML syntax. For example: ...  ingress:  - from:    - namespaceSelector:        matchLabels:          user: alice      podSelector:        matchLabels:          role: client  ... This policy contains a single from element allowing connections from Pods with the labelrole=client in namespaces with the label user=alice. But the following policy is different: ...  ingress:  - from:    - namespaceSelector:        matchLabels:          user: alice    - podSelector:        matchLabels:          role: client  ... It contains two elements in the from array, and allows connections from Pods in the localNamespace with the label role=client, or from any Pod in any namespace with the labeluser=alice. When in doubt, use kubectl describe to see how Kubernetes has interpreted the policy. ipBlock: This selects particular IP CIDR ranges to allow as ingress sources or egressdestinations. These should be cluster-external IPs, since Pod IPs are ephemeral and unpredictable. Cluster ingress and egress mechanisms often require rewriting the source or destination IPof packets. In cases where this happens, it is not defined whether this happens before orafter NetworkPolicy processing, and the behavior may be different for differentcombinations of network plugin, cloud provider, Service implementation, etc. In the case of ingress, this means that in some cases you may be able to filter incomingpackets based on the actual original source IP, while in other cases, the ""source IP"" thatthe NetworkPolicy acts on may be the IP of a LoadBalancer or of the Pod's node, etc. For egress, this means that connections from pods to Service IPs that get rewritten tocluster-external IPs may or may not be subject to ipBlock-based policies.",523
6.5 - Network Policies,Default policies,"Default policies By default, if no policies exist in a namespace, then all ingress and egress traffic is allowed toand from pods in that namespace. The following examples let you change the default behaviorin that namespace.",44
6.5 - Network Policies,Default deny all ingress traffic,"Default deny all ingress traffic You can create a ""default"" ingress isolation policy for a namespace by creating a NetworkPolicythat selects all pods but does not allow any ingress traffic to those pods. service/networking/network-policy-default-deny-ingress.yaml---apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: default-deny-ingressspec:  podSelector: {}  policyTypes:  - Ingress This ensures that even pods that aren't selected by any other NetworkPolicy will still be isolatedfor ingress. This policy does not affect isolation for egress from any pod.",143
6.5 - Network Policies,Allow all ingress traffic,"Allow all ingress traffic If you want to allow all incoming connections to all pods in a namespace, you can create a policythat explicitly allows that. service/networking/network-policy-allow-all-ingress.yaml---apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: allow-all-ingressspec:  podSelector: {}  ingress:  - {}  policyTypes:  - Ingress With this policy in place, no additional policy or policies can cause any incoming connection tothose pods to be denied. This policy has no effect on isolation for egress from any pod.",137
6.5 - Network Policies,Default deny all egress traffic,"Default deny all egress traffic You can create a ""default"" egress isolation policy for a namespace by creating a NetworkPolicythat selects all pods but does not allow any egress traffic from those pods. service/networking/network-policy-default-deny-egress.yaml---apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: default-deny-egressspec:  podSelector: {}  policyTypes:  - Egress This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowedegress traffic. This policy does not change the ingress isolation behavior of any pod.",144
6.5 - Network Policies,Allow all egress traffic,"Allow all egress traffic If you want to allow all connections from all pods in a namespace, you can create a policy thatexplicitly allows all outgoing connections from pods in that namespace. service/networking/network-policy-allow-all-egress.yaml---apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: allow-all-egressspec:  podSelector: {}  egress:  - {}  policyTypes:  - Egress With this policy in place, no additional policy or policies can cause any outgoing connection fromthose pods to be denied. This policy has no effect on isolation for ingress to any pod.",145
6.5 - Network Policies,Default deny all ingress and all egress traffic,"Default deny all ingress and all egress traffic You can create a ""default"" policy for a namespace which prevents all ingress AND egress traffic bycreating the following NetworkPolicy in that namespace. service/networking/network-policy-default-deny-all.yaml---apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: default-deny-allspec:  podSelector: {}  policyTypes:  - Ingress  - Egress This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowedingress or egress traffic.",133
6.5 - Network Policies,SCTP support,"SCTP support FEATURE STATE: Kubernetes v1.20 [stable] As a stable feature, this is enabled by default. To disable SCTP at a cluster level, you (or yourcluster administrator) will need to disable the SCTPSupportfeature gatefor the API server with --feature-gates=SCTPSupport=false,….When the feature gate is enabled, you can set the protocol field of a NetworkPolicy to SCTP. Note: You must be using a CNI plugin that supports SCTPprotocol NetworkPolicies.",124
6.5 - Network Policies,Targeting a range of ports,"Targeting a range of ports FEATURE STATE: Kubernetes v1.25 [stable] When writing a NetworkPolicy, you can target a range of ports instead of a single port. This is achievable with the usage of the endPort field, as the following example: service/networking/networkpolicy-multiport-egress.yamlapiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: multi-port-egress  namespace: defaultspec:  podSelector:    matchLabels:      role: db  policyTypes:    - Egress  egress:    - to:        - ipBlock:            cidr: 10.0.0.0/24      ports:        - protocol: TCP          port: 32000          endPort: 32768 The above rule allows any Pod with label role=db on the namespace default to communicatewith any IP within the range 10.0.0.0/24 over TCP, provided that the targetport is between the range 32000 and 32768. The following restrictions apply when using this field: The endPort field must be equal to or greater than the port field.endPort can only be defined if port is also defined.Both ports must be numeric. Note: Your cluster must be using a CNI plugin thatsupports the endPort field in NetworkPolicy specifications.If your network plugindoes not support the endPort field and you specify a NetworkPolicy with that,the policy will be applied only for the single port field.",324
6.5 - Network Policies,Targeting a Namespace by its name,"Targeting a Namespace by its name FEATURE STATE: Kubernetes 1.22 [stable] The Kubernetes control plane sets an immutable label kubernetes.io/metadata.name on allnamespaces, provided that the NamespaceDefaultLabelNamefeature gate is enabled.The value of the label is the namespace name. While NetworkPolicy cannot target a namespace by its name with some object field, you can use thestandardized label to target a specific namespace.",99
6.5 - Network Policies,"What you can't do with network policies (at least, not yet)","What you can't do with network policies (at least, not yet) As of Kubernetes 1.26, the following functionality does not exist in theNetworkPolicy API, but you might be able to implement workarounds using Operating Systemcomponents (such as SELinux, OpenVSwitch, IPTables, and so on) or Layer 7 technologies (Ingresscontrollers, Service Mesh implementations) or admission controllers. In case you are new tonetwork security in Kubernetes, its worth noting that the following User Stories cannot (yet) beimplemented using the NetworkPolicy API. Forcing internal cluster traffic to go through a common gateway (this might be best served witha service mesh or other proxy).Anything TLS related (use a service mesh or ingress controller for this).Node specific policies (you can use CIDR notation for these, but you cannot target nodes bytheir Kubernetes identities specifically).Targeting of services by name (you can, however, target pods or namespaces by theirlabels, which is often a viable workaround).Creation or management of ""Policy requests"" that are fulfilled by a third party.Default policies which are applied to all namespaces or pods (there are some third partyKubernetes distributions and projects which can do this).Advanced policy querying and reachability tooling.The ability to log network security events (for example connections that are blocked or accepted).The ability to explicitly deny policies (currently the model for NetworkPolicies are deny bydefault, with only the ability to add allow rules).The ability to prevent loopback or incoming host traffic (Pods cannot currently block localhostaccess, nor do they have the ability to block access from their resident node). See the Declare Network Policywalkthrough for further examples.See more recipes for commonscenarios enabled by the NetworkPolicy resource.",387
6.6 - DNS for Services and Pods,default,"Your workload can discover Services within your cluster using DNS; this page explains how that works. Kubernetes creates DNS records for Services and Pods. You can contactServices with consistent DNS names instead of IP addresses. Kubernetes publishes information about Pods and Services which is usedto program DNS. Kubelet configures Pods' DNS so that running containerscan lookup Services by name rather than IP. Services defined in the cluster are assigned DNS names. By default, aclient Pod's DNS search list includes the Pod's own namespace and thecluster's default domain.",118
6.6 - DNS for Services and Pods,Namespaces of Services,"Namespaces of Services A DNS query may return different results based on the namespace of the Pod makingit. DNS queries that don't specify a namespace are limited to the Pod'snamespace. Access Services in other namespaces by specifying it in the DNS query. For example, consider a Pod in a test namespace. A data Service is inthe prod namespace. A query for data returns no results, because it uses the Pod's test namespace. A query for data.prod returns the intended result, because it specifies thenamespace. DNS queries may be expanded using the Pod's /etc/resolv.conf. Kubeletconfigures this file for each Pod. For example, a query for just data may beexpanded to data.test.svc.cluster.local. The values of the search optionare used to expand queries. To learn more about DNS queries, seethe resolv.conf manual page. nameserver 10.32.0.10search <namespace>.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5 In summary, a Pod in the test namespace can successfully resolve eitherdata.prod or data.prod.svc.cluster.local.",259
6.6 - DNS for Services and Pods,DNS Records,"DNS Records What objects get DNS records? ServicesPods The following sections detail the supported DNS record types and layout that issupported. Any other layout or names or queries that happen to work areconsidered implementation details and are subject to change without warning.For more up-to-date specification, seeKubernetes DNS-Based Service Discovery.",72
6.6 - DNS for Services and Pods,A/AAAA records,"A/AAAA records ""Normal"" (not headless) Services are assigned DNS A and/or AAAA records,depending on the IP family or families of the Service, with a name of the formmy-svc.my-namespace.svc.cluster-domain.example. This resolves to the cluster IPof the Service. Headless Services(without a cluster IP) Services are also assigned DNS A and/or AAAA records,with a name of the form my-svc.my-namespace.svc.cluster-domain.example. Unlike normalServices, this resolves to the set of IPs of all of the Pods selected by the Service.Clients are expected to consume the set or else use standard round-robinselection from the set. A/AAAA records In general a Pod has the following DNS resolution: pod-ip-address.my-namespace.pod.cluster-domain.example. For example, if a Pod in the default namespace has the IP address 172.17.0.3,and the domain name for your cluster is cluster.local, then the Pod has a DNS name: 172-17-0-3.default.pod.cluster.local. Any Pods exposed by a Service have the following DNS resolution available: pod-ip-address.service-name.my-namespace.svc.cluster-domain.example.",295
6.6 - DNS for Services and Pods,SRV records,"SRV records SRV Records are created for named ports that are part of normal or headlessservices. For each named port, the SRV record has the form_port-name._port-protocol.my-svc.my-namespace.svc.cluster-domain.example.For a regular Service, this resolves to the port number and the domain name:my-svc.my-namespace.svc.cluster-domain.example.For a headless Service, this resolves to multiple answers, one for each Podthat is backing the Service, and contains the port number and the domain name of the Podof the form hostname.my-svc.my-namespace.svc.cluster-domain.example.",159
6.6 - DNS for Services and Pods,Pod's hostname and subdomain fields,"Pod's hostname and subdomain fields Currently when a Pod is created, its hostname (as observed from within the Pod)is the Pod's metadata.name value. The Pod spec has an optional hostname field, which can be used to specify adifferent hostname. When specified, it takes precedence over the Pod's name to bethe hostname of the Pod (again, as observed from within the Pod). For example,given a Pod with spec.hostname set to ""my-host"", the Pod will have itshostname set to ""my-host"". The Pod spec also has an optional subdomain field which can be used to indicatethat the pod is part of sub-group of the namespace. For example, a Pod with spec.hostnameset to ""foo"", and spec.subdomain set to ""bar"", in namespace ""my-namespace"", willhave its hostname set to ""foo"" and its fully qualified domain name (FQDN) set to""foo.bar.my-namespace.svc.cluster.local"" (once more, as observed from withinthe Pod). If there exists a headless Service in the same namespace as the Pod, withthe same name as the subdomain, the cluster's DNS Server also returns A and/or AAAArecords for the Pod's fully qualified hostname. Example: apiVersion: v1kind: Servicemetadata:  name: busybox-subdomainspec:  selector:    name: busybox  clusterIP: None  ports:  - name: foo # name is not required for single-port Services    port: 1234---apiVersion: v1kind: Podmetadata:  name: busybox1  labels:    name: busyboxspec:  hostname: busybox-1  subdomain: busybox-subdomain  containers:  - image: busybox:1.28    command:      - sleep      - ""3600""    name: busybox---apiVersion: v1kind: Podmetadata:  name: busybox2  labels:    name: busyboxspec:  hostname: busybox-2  subdomain: busybox-subdomain  containers:  - image: busybox:1.28    command:      - sleep      - ""3600""    name: busybox Given the above Service ""busybox-subdomain"" and the Pods which set spec.subdomainto ""busybox-subdomain"", the first Pod will see its own FQDN as""busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example"". DNS servesA and/or AAAA records at that name, pointing to the Pod's IP. Both Pods ""busybox1"" and""busybox2"" will have their own address records. An EndpointSlice can specifythe DNS hostname for any endpoint addresses, along with its IP. Note: Because A and AAAA records are not created for Pod names, hostname is required for the Pod's A or AAAArecord to be created. A Pod with no hostname but with subdomain will only create theA or AAAA record for the headless Service (busybox-subdomain.my-namespace.svc.cluster-domain.example),pointing to the Pods' IP addresses. Also, the Pod needs to be ready in order to have arecord unless publishNotReadyAddresses=True is set on the Service.",744
6.6 - DNS for Services and Pods,Pod's setHostnameAsFQDN field,"Pod's setHostnameAsFQDN field FEATURE STATE: Kubernetes v1.22 [stable] When a Pod is configured to have fully qualified domain name (FQDN), itshostname is the short hostname. For example, if you have a Pod with the fullyqualified domain name busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example,then by default the hostname command inside that Pod returns busybox-1 and thehostname --fqdn command returns the FQDN. When you set setHostnameAsFQDN: true in the Pod spec, the kubelet writes the Pod's FQDN into the hostname for that Pod's namespace. In this case, both hostname and hostname --fqdn return the Pod's FQDN. Note:In Linux, the hostname field of the kernel (the nodename field of struct utsname) is limited to 64 characters.If a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The Pod will remain in Pending status (ContainerCreating as seen by kubectl) generating error events, such as Failed to construct FQDN from Pod hostname and cluster domain, FQDN long-FQDN is too long (64 characters is the max, 70 characters requested). One way of improving user experience for this scenario is to create an admission webhook controller to control FQDN size when users create top level objects, for example, Deployment.",330
6.6 - DNS for Services and Pods,Pod's DNS Policy,"Pod's DNS Policy DNS policies can be set on a per-Pod basis. Currently Kubernetes supports thefollowing Pod-specific DNS policies. These policies are specified in thednsPolicy field of a Pod Spec. ""Default"": The Pod inherits the name resolution configuration from the nodethat the Pods run on.See related discussionfor more details.""ClusterFirst"": Any DNS query that does not match the configured clusterdomain suffix, such as ""www.kubernetes.io"", is forwarded to an upstreamnameserver by the DNS server. Cluster administrators may have extrastub-domain and upstream DNS servers configured.See related discussionfor details on how DNS queries are handled in those cases.""ClusterFirstWithHostNet"": For Pods running with hostNetwork, you shouldexplicitly set its DNS policy to ""ClusterFirstWithHostNet"". Otherwise, Podsrunning with hostNetwork and ""ClusterFirst"" will fallback to the behaviorof the ""Default"" policy.Note: This is not supported on Windows. See below for details""None"": It allows a Pod to ignore DNS settings from the Kubernetesenvironment. All DNS settings are supposed to be provided using thednsConfig field in the Pod Spec.See Pod's DNS config subsection below. Note: ""Default"" is not the default DNS policy. If dnsPolicy is notexplicitly specified, then ""ClusterFirst"" is used. The example below shows a Pod with its DNS policy set to""ClusterFirstWithHostNet"" because it has hostNetwork set to true. apiVersion: v1kind: Podmetadata:  name: busybox  namespace: defaultspec:  containers:  - image: busybox:1.28    command:      - sleep      - ""3600""    imagePullPolicy: IfNotPresent    name: busybox  restartPolicy: Always  hostNetwork: true  dnsPolicy: ClusterFirstWithHostNet",405
6.6 - DNS for Services and Pods,Pod's DNS Config,"Pod's DNS Config FEATURE STATE: Kubernetes v1.14 [stable] Pod's DNS Config allows users more control on the DNS settings for a Pod. The dnsConfig field is optional and it can work with any dnsPolicy settings.However, when a Pod's dnsPolicy is set to ""None"", the dnsConfig field hasto be specified. Below are the properties a user can specify in the dnsConfig field: nameservers: a list of IP addresses that will be used as DNS servers for thePod. There can be at most 3 IP addresses specified. When the Pod's dnsPolicyis set to ""None"", the list must contain at least one IP address, otherwisethis property is optional.The servers listed will be combined to the base nameservers generated from thespecified DNS policy with duplicate addresses removed.searches: a list of DNS search domains for hostname lookup in the Pod.This property is optional. When specified, the provided list will be mergedinto the base search domain names generated from the chosen DNS policy.Duplicate domain names are removed.Kubernetes allows for at most 6 search domains.options: an optional list of objects where each object may have a nameproperty (required) and a value property (optional). The contents in thisproperty will be merged to the options generated from the specified DNS policy.Duplicate entries are removed. The following is an example Pod with custom DNS settings: service/networking/custom-dns.yamlapiVersion: v1kind: Podmetadata:  namespace: default  name: dns-examplespec:  containers:    - name: test      image: nginx  dnsPolicy: ""None""  dnsConfig:    nameservers:      - 192.0.2.1 # this is an example    searches:      - ns1.svc.cluster-domain.example      - my.dns.search.suffix    options:      - name: ndots        value: ""2""      - name: edns0 When the Pod above is created, the container test gets the following contentsin its /etc/resolv.conf file: nameserver 192.0.2.1search ns1.svc.cluster-domain.example my.dns.search.suffixoptions ndots:2 edns0 For IPv6 setup, search path and name server should be set up like this: kubectl exec -it dns-example -- cat /etc/resolv.conf The output is similar to this: nameserver 2001:db8:30::asearch default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.exampleoptions ndots:5",589
6.6 - DNS for Services and Pods,DNS search domain list limits,"DNS search domain list limits FEATURE STATE: Kubernetes 1.26 [beta] Kubernetes itself does not limit the DNS Config until the length of the searchdomain list exceeds 32 or the total length of all search domains exceeds 2048.This limit applies to the node's resolver configuration file, the Pod's DNSConfig, and the merged DNS Config respectively. Note:Some container runtimes of earlier versions may have their own restrictions onthe number of DNS search domains. Depending on the container runtimeenvironment, the pods with a large number of DNS search domains may get stuck inthe pending state.It is known that containerd v1.5.5 or earlier and CRI-O v1.21 or earlier havethis problem.",153
6.6 - DNS for Services and Pods,DNS resolution on Windows nodes,"DNS resolution on Windows nodes ClusterFirstWithHostNet is not supported for Pods that run on Windows nodes.Windows treats all names with a . as a FQDN and skips FQDN resolution.On Windows, there are multiple DNS resolvers that can be used. As these come withslightly different behaviors, using theResolve-DNSNamepowershell cmdlet for name query resolutions is recommended.On Linux, you have a DNS suffix list, which is used after resolution of a name as fullyqualified has failed.On Windows, you can only have 1 DNS suffix, which is the DNS suffix associated with thatPod's namespace (example: mydns.svc.cluster.local). Windows can resolve FQDNs, Services,or network name which can be resolved with this single suffix. For example, a Pod spawnedin the default namespace, will have the DNS suffix default.svc.cluster.local.Inside a Windows Pod, you can resolve both kubernetes.default.svc.cluster.localand kubernetes, but not the partially qualified names (kubernetes.default orkubernetes.default.svc). For guidance on administering DNS configurations, checkConfigure DNS Service",260
6.7 - IPv4/IPv6 dual-stack,default,"Kubernetes lets you configure single-stack IPv4 networking, single-stack IPv6 networking, or dual stack networking with both network families active. This page explains how. FEATURE STATE: Kubernetes v1.23 [stable] IPv4/IPv6 dual-stack networking enables the allocation of both IPv4 and IPv6 addresses toPods and Services. IPv4/IPv6 dual-stack networking is enabled by default for your Kubernetes cluster starting in1.21, allowing the simultaneous assignment of both IPv4 and IPv6 addresses.",118
6.7 - IPv4/IPv6 dual-stack,Supported Features,Supported Features IPv4/IPv6 dual-stack on your Kubernetes cluster provides the following features: Dual-stack Pod networking (a single IPv4 and IPv6 address assignment per Pod)IPv4 and IPv6 enabled ServicesPod off-cluster egress routing (eg. the Internet) via both IPv4 and IPv6 interfaces,71
6.7 - IPv4/IPv6 dual-stack,Prerequisites,"Prerequisites The following prerequisites are needed in order to utilize IPv4/IPv6 dual-stack Kubernetes clusters: Kubernetes 1.20 or laterFor information about using dual-stack services with earlierKubernetes versions, refer to the documentation for that versionof Kubernetes.Provider support for dual-stack networking (Cloud provider or otherwise must be able to provideKubernetes nodes with routable IPv4/IPv6 network interfaces)A network plugin thatsupports dual-stack networking.",110
6.7 - IPv4/IPv6 dual-stack,Configure IPv4/IPv6 dual-stack,"Configure IPv4/IPv6 dual-stack To configure IPv4/IPv6 dual-stack, set dual-stack cluster network assignments: kube-apiserver:--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>kube-controller-manager:--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6 defaults to /24 for IPv4 and /64 for IPv6kube-proxy:--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>kubelet:when there is no --cloud-provider the administrator can pass a comma-separated pair of IPaddresses via --node-ip to manually configure dual-stack .status.addresses for that Node.If a Pod runs on that node in HostNetwork mode, the Pod reports these IP addresses in its.status.podIPs field.All podIPs in a node match the IP family preference defined by the .status.addressesfield for that Node. Note:An example of an IPv4 CIDR: 10.244.0.0/16 (though you would supply your own address range)An example of an IPv6 CIDR: fdXY:IJKL:MNOP:15::/64 (this shows the format but is not a validaddress - see RFC 4193)",365
6.7 - IPv4/IPv6 dual-stack,Services,"Services You can create Services which can use IPv4, IPv6, or both. The address family of a Service defaults to the address family of the first service cluster IPrange (configured via the --service-cluster-ip-range flag to the kube-apiserver). When you define a Service you can optionally configure it as dual stack. To specify the behavior you want, youset the .spec.ipFamilyPolicy field to one of the following values: SingleStack: Single-stack service. The control plane allocates a cluster IP for the Service,using the first configured service cluster IP range.PreferDualStack:Allocates IPv4 and IPv6 cluster IPs for the Service.RequireDualStack: Allocates Service .spec.ClusterIPs from both IPv4 and IPv6 address ranges.Selects the .spec.ClusterIP from the list of .spec.ClusterIPs based on the address familyof the first element in the .spec.ipFamilies array. If you would like to define which IP family to use for single stack or define the order of IPfamilies for dual-stack, you can choose the address families by setting an optional field,.spec.ipFamilies, on the Service. Note: The .spec.ipFamilies field is immutable because the .spec.ClusterIP cannot be reallocated on aService that already exists. If you want to change .spec.ipFamilies, delete and recreate theService. You can set .spec.ipFamilies to any of the following array values: [""IPv4""][""IPv6""][""IPv4"",""IPv6""] (dual stack)[""IPv6"",""IPv4""] (dual stack) The first family you list is used for the legacy .spec.ClusterIP field.",390
6.7 - IPv4/IPv6 dual-stack,Dual-stack options on new Services,"Dual-stack options on new Services This Service specification does not explicitly define .spec.ipFamilyPolicy. When you createthis Service, Kubernetes assigns a cluster IP for the Service from the first configuredservice-cluster-ip-range and sets the .spec.ipFamilyPolicy to SingleStack. (Serviceswithout selectors andheadless Services with selectorswill behave in this same way.)service/networking/dual-stack-default-svc.yamlapiVersion: v1   kind: Service   metadata:     name: my-service     labels:       app.kubernetes.io/name: MyApp   spec:     selector:       app.kubernetes.io/name: MyApp     ports:       - protocol: TCP         port: 80   This Service specification explicitly defines PreferDualStack in .spec.ipFamilyPolicy. Whenyou create this Service on a dual-stack cluster, Kubernetes assigns both IPv4 and IPv6addresses for the service. The control plane updates the .spec for the Service to record the IPaddress assignments. The field .spec.ClusterIPs is the primary field, and contains both assignedIP addresses; .spec.ClusterIP is a secondary field with its value calculated from.spec.ClusterIPs.For the .spec.ClusterIP field, the control plane records the IP address that is from thesame address family as the first service cluster IP range.On a single-stack cluster, the .spec.ClusterIPs and .spec.ClusterIP fields both only listone address.On a cluster with dual-stack enabled, specifying RequireDualStack in .spec.ipFamilyPolicybehaves the same as PreferDualStack.service/networking/dual-stack-preferred-svc.yamlapiVersion: v1   kind: Service   metadata:     name: my-service     labels:       app.kubernetes.io/name: MyApp   spec:     ipFamilyPolicy: PreferDualStack     selector:       app.kubernetes.io/name: MyApp     ports:       - protocol: TCP         port: 80   This Service specification explicitly defines IPv6 and IPv4 in .spec.ipFamilies as wellas defining PreferDualStack in .spec.ipFamilyPolicy. When Kubernetes assigns an IPv6 andIPv4 address in .spec.ClusterIPs, .spec.ClusterIP is set to the IPv6 address because that isthe first element in the .spec.ClusterIPs array, overriding the default.service/networking/dual-stack-preferred-ipfamilies-svc.yamlapiVersion: v1   kind: Service   metadata:     name: my-service     labels:       app.kubernetes.io/name: MyApp   spec:     ipFamilyPolicy: PreferDualStack     ipFamilies:     - IPv6     - IPv4     selector:       app.kubernetes.io/name: MyApp     ports:       - protocol: TCP         port: 80",665
6.7 - IPv4/IPv6 dual-stack,Dual-stack defaults on existing Services,"Dual-stack defaults on existing Services These examples demonstrate the default behavior when dual-stack is newly enabled on a clusterwhere Services already exist. (Upgrading an existing cluster to 1.21 or beyond will enabledual-stack.) When dual-stack is enabled on a cluster, existing Services (whether IPv4 or IPv6) areconfigured by the control plane to set .spec.ipFamilyPolicy to SingleStack and set.spec.ipFamilies to the address family of the existing Service. The existing Service cluster IPwill be stored in .spec.ClusterIPs.service/networking/dual-stack-default-svc.yamlapiVersion: v1   kind: Service   metadata:     name: my-service     labels:       app.kubernetes.io/name: MyApp   spec:     selector:       app.kubernetes.io/name: MyApp     ports:       - protocol: TCP         port: 80   You can validate this behavior by using kubectl to inspect an existing service.kubectl get svc my-service -o yamlapiVersion: v1kind: Servicemetadata:  labels:    app.kubernetes.io/name: MyApp  name: my-servicespec:  clusterIP: 10.0.197.123  clusterIPs:  - 10.0.197.123  ipFamilies:  - IPv4  ipFamilyPolicy: SingleStack  ports:  - port: 80    protocol: TCP    targetPort: 80  selector:    app.kubernetes.io/name: MyApp  type: ClusterIPstatus:  loadBalancer: {}When dual-stack is enabled on a cluster, existingheadless Services with selectors areconfigured by the control plane to set .spec.ipFamilyPolicy to SingleStack and set.spec.ipFamilies to the address family of the first service cluster IP range (configured via the--service-cluster-ip-range flag to the kube-apiserver) even though .spec.ClusterIP is set toNone.service/networking/dual-stack-default-svc.yamlapiVersion: v1   kind: Service   metadata:     name: my-service     labels:       app.kubernetes.io/name: MyApp   spec:     selector:       app.kubernetes.io/name: MyApp     ports:       - protocol: TCP         port: 80   You can validate this behavior by using kubectl to inspect an existing headless service with selectors.kubectl get svc my-service -o yamlapiVersion: v1kind: Servicemetadata:  labels:    app.kubernetes.io/name: MyApp  name: my-servicespec:  clusterIP: None  clusterIPs:  - None  ipFamilies:  - IPv4  ipFamilyPolicy: SingleStack  ports:  - port: 80    protocol: TCP    targetPort: 80  selector:    app.kubernetes.io/name: MyApp",675
6.7 - IPv4/IPv6 dual-stack,Switching Services between single-stack and dual-stack,"Switching Services between single-stack and dual-stack Services can be changed from single-stack to dual-stack and from dual-stack to single-stack. To change a Service from single-stack to dual-stack, change .spec.ipFamilyPolicy fromSingleStack to PreferDualStack or RequireDualStack as desired. When you change thisService from single-stack to dual-stack, Kubernetes assigns the missing address family so that theService now has IPv4 and IPv6 addresses.Edit the Service specification updating the .spec.ipFamilyPolicy from SingleStack to PreferDualStack.Before:spec:  ipFamilyPolicy: SingleStackAfter:spec:  ipFamilyPolicy: PreferDualStackTo change a Service from dual-stack to single-stack, change .spec.ipFamilyPolicy fromPreferDualStack or RequireDualStack to SingleStack. When you change this Service fromdual-stack to single-stack, Kubernetes retains only the first element in the .spec.ClusterIPsarray, and sets .spec.ClusterIP to that IP address and sets .spec.ipFamilies to the addressfamily of .spec.ClusterIPs.",253
6.7 - IPv4/IPv6 dual-stack,Service type LoadBalancer,"Service type LoadBalancer To provision a dual-stack load balancer for your Service: Set the .spec.type field to LoadBalancerSet .spec.ipFamilyPolicy field to PreferDualStack or RequireDualStack Note: To use a dual-stack LoadBalancer type Service, your cloud provider must support IPv4 and IPv6load balancers.",75
6.7 - IPv4/IPv6 dual-stack,Egress traffic,"Egress traffic If you want to enable egress traffic in order to reach off-cluster destinations (eg. the publicInternet) from a Pod that uses non-publicly routable IPv6 addresses, you need to enable the Pod touse a publicly routed IPv6 address via a mechanism such as transparent proxying or IPmasquerading. The ip-masq-agent projectsupports IP masquerading on dual-stack clusters. Note: Ensure your CNI provider supports IPv6.",101
6.7 - IPv4/IPv6 dual-stack,Windows support,"Windows support Kubernetes on Windows does not support single-stack ""IPv6-only"" networking. However,dual-stack IPv4/IPv6 networking for pods and nodes with single-family servicesis supported. You can use IPv4/IPv6 dual-stack networking with l2bridge networks. Note: Overlay (VXLAN) networks on Windows do not support dual-stack networking. You can read more about the different network modes for Windows within theNetworking on Windows topic. Validate IPv4/IPv6 dual-stack networkingEnable dual-stack networking using kubeadm",128
6.8 - Topology Aware Hints,default,"Topology Aware Hints provides a mechanism to help keep network traffic within the zone where it originated. Preferring same-zone traffic between Pods in your cluster can help with reliability, performance (network latency and throughput), or cost. FEATURE STATE: Kubernetes v1.23 [beta] Topology Aware Hints enable topology aware routing by including suggestionsfor how clients should consume endpoints. This approach adds metadata to enableconsumers of EndpointSlice (or Endpoints) objects, so that traffic tothose network endpoints can be routed closer to where it originated. For example, you can route traffic within a locality to reducecosts, or to improve network performance.",142
6.8 - Topology Aware Hints,Motivation,"Motivation Kubernetes clusters are increasingly deployed in multi-zone environments.Topology Aware Hints provides a mechanism to help keep traffic within the zoneit originated from. This concept is commonly referred to as ""Topology AwareRouting"". When calculating the endpoints for a Service,the EndpointSlice controller considers the topology (region and zone) of each endpointand populates the hints field to allocate it to a zone.Cluster components such as the kube-proxycan then consume those hints, and use them to influence how the traffic is routed(favoring topologically closer endpoints).",125
6.8 - Topology Aware Hints,Using Topology Aware Hints,"Using Topology Aware Hints You can activate Topology Aware Hints for a Service by setting theservice.kubernetes.io/topology-aware-hints annotation to auto. This tellsthe EndpointSlice controller to set topology hints if it is deemed safe.Importantly, this does not guarantee that hints will always be set.",74
6.8 - Topology Aware Hints,How it works,How it works The functionality enabling this feature is split into two components: TheEndpointSlice controller and the kube-proxy. This section provides a high level overviewof how each component implements this feature.,42
6.8 - Topology Aware Hints,EndpointSlice controller,"EndpointSlice controller The EndpointSlice controller is responsible for setting hints on EndpointSliceswhen this feature is enabled. The controller allocates a proportional amount ofendpoints to each zone. This proportion is based on theallocatableCPU cores for nodes running in that zone. For example, if one zone had 2 CPUcores and another zone only had 1 CPU core, the controller would allocate twiceas many endpoints to the zone with 2 CPU cores. The following example shows what an EndpointSlice looks like when hints havebeen populated: apiVersion: discovery.k8s.io/v1kind: EndpointSlicemetadata:  name: example-hints  labels:    kubernetes.io/service-name: example-svcaddressType: IPv4ports:  - name: http    protocol: TCP    port: 80endpoints:  - addresses:      - ""10.1.2.3""    conditions:      ready: true    hostname: pod-1    zone: zone-a    hints:      forZones:        - name: ""zone-a""",237
6.8 - Topology Aware Hints,kube-proxy,"kube-proxy The kube-proxy component filters the endpoints it routes to based on the hints set bythe EndpointSlice controller. In most cases, this means that the kube-proxy is ableto route traffic to endpoints in the same zone. Sometimes the controller allocates endpointsfrom a different zone to ensure more even distribution of endpoints between zones.This would result in some traffic being routed to other zones.",89
6.8 - Topology Aware Hints,Safeguards,"Safeguards The Kubernetes control plane and the kube-proxy on each node apply somesafeguard rules before using Topology Aware Hints. If these don't check out,the kube-proxy selects endpoints from anywhere in your cluster, regardless of thezone. Insufficient number of endpoints: If there are less endpoints than zonesin a cluster, the controller will not assign any hints.Impossible to achieve balanced allocation: In some cases, it will beimpossible to achieve a balanced allocation of endpoints among zones. Forexample, if zone-a is twice as large as zone-b, but there are only 2endpoints, an endpoint allocated to zone-a may receive twice as much trafficas zone-b. The controller does not assign hints if it can't get this ""expectedoverload"" value below an acceptable threshold for each zone. Importantly thisis not based on real-time feedback. It is still possible for individualendpoints to become overloaded.One or more Nodes has insufficient information: If any node does not havea topology.kubernetes.io/zone label or is not reporting a value forallocatable CPU, the control plane does not set any topology-aware endpointhints and so kube-proxy does not filter endpoints by zone.One or more endpoints does not have a zone hint: When this happens,the kube-proxy assumes that a transition from or to Topology Aware Hints isunderway. Filtering endpoints for a Service in this state would be dangerousso the kube-proxy falls back to using all endpoints.A zone is not represented in hints: If the kube-proxy is unable to findat least one endpoint with a hint targeting the zone it is running in, it fallsto using endpoints from all zones. This is most likely to happen as you adda new zone into your existing cluster.",401
6.8 - Topology Aware Hints,Constraints,"Constraints Topology Aware Hints are not used when either externalTrafficPolicy orinternalTrafficPolicy is set to Local on a Service. It is possible to useboth features in the same cluster on different Services, just not on the sameService.This approach will not work well for Services that have a large proportion oftraffic originating from a subset of zones. Instead this assumes that incomingtraffic will be roughly proportional to the capacity of the Nodes in eachzone.The EndpointSlice controller ignores unready nodes as it calculates theproportions of each zone. This could have unintended consequences if a largeportion of nodes are unready.The EndpointSlice controller does not take into account tolerations when deploying or calculating theproportions of each zone. If the Pods backing a Service are limited to asubset of Nodes in the cluster, this will not be taken into account.This may not work well with autoscaling. For example, if a lot of traffic isoriginating from a single zone, only the endpoints allocated to that zone willbe handling that traffic. That could result in Horizontal Pod Autoscalereither not picking up on this event, or newly added pods starting in adifferent zone. Follow the Connecting Applications with Services tutorial",263
6.9 - Networking on Windows,default,Kubernetes supports running nodes on either Linux or Windows. You can mix both kinds of nodewithin a single cluster.This page provides an overview to networking specific to the Windows operating system.,41
6.9 - Networking on Windows,Container networking on Windows,"Container networking on Windows Networking for Windows containers is exposed throughCNI plugins.Windows containers function similarly to virtual machines in regards tonetworking. Each container has a virtual network adapter (vNIC) which is connectedto a Hyper-V virtual switch (vSwitch). The Host Networking Service (HNS) and theHost Compute Service (HCS) work together to create containers and attach containervNICs to networks. HCS is responsible for the management of containers whereas HNSis responsible for the management of networking resources such as: Virtual networks (including creation of vSwitches)Endpoints / vNICsNamespacesPolicies including packet encapsulations, load-balancing rules, ACLs, and NAT rules. The Windows HNS and vSwitch implement namespacing and cancreate virtual NICs as needed for a pod or container. However, many configurations suchas DNS, routes, and metrics are stored in the Windows registry database rather than asfiles inside /etc, which is how Linux stores those configurations. The Windows registry for the containeris separate from that of the host, so concepts like mapping /etc/resolv.conf fromthe host into a container don't have the same effect they would on Linux. These mustbe configured using Windows APIs run in the context of that container. ThereforeCNI implementations need to call the HNS instead of relying on file mappings to passnetwork details into the pod or container.",297
6.9 - Networking on Windows,Network modes,"Network modes Windows supports five different networking drivers/modes: L2bridge, L2tunnel,Overlay (Beta), Transparent, and NAT. In a heterogeneous cluster with Windows and Linuxworker nodes, you need to select a networking solution that is compatible on bothWindows and Linux. The following table lists the out-of-tree plugins are supported on Windows,with recommendations on when to use each CNI: Network DriverDescriptionContainer Packet ModificationsNetwork PluginsNetwork Plugin CharacteristicsL2bridgeContainers are attached to an external vSwitch. Containers are attached to the underlay network, although the physical network doesn't need to learn the container MACs because they are rewritten on ingress/egress.MAC is rewritten to host MAC, IP may be rewritten to host IP using HNS OutboundNAT policy.win-bridge, Azure-CNI, Flannel host-gateway uses win-bridgewin-bridge uses L2bridge network mode, connects containers to the underlay of hosts, offering best performance. Requires user-defined routes (UDR) for inter-node connectivity.L2TunnelThis is a special case of l2bridge, but only used on Azure. All packets are sent to the virtualization host where SDN policy is applied.MAC rewritten, IP visible on the underlay networkAzure-CNIAzure-CNI allows integration of containers with Azure vNET, and allows them to leverage the set of capabilities that Azure Virtual Network provides. For example, securely connect to Azure services or use Azure NSGs. See azure-cni for some examplesOverlayContainers are given a vNIC connected to an external vSwitch. Each overlay network gets its own IP subnet, defined by a custom IP prefix.The overlay network driver uses VXLAN encapsulation.Encapsulated with an outer header.win-overlay, Flannel VXLAN (uses win-overlay)win-overlay should be used when virtual container networks are desired to be isolated from underlay of hosts (e.g. for security reasons). Allows for IPs to be re-used for different overlay networks (which have different VNID tags) if you are restricted on IPs in your datacenter. This option requires KB4489899 on Windows Server 2019.Transparent (special use case for ovn-kubernetes)Requires an external vSwitch. Containers are attached to an external vSwitch which enables intra-pod communication via logical networks (logical switches and routers).Packet is encapsulated either via GENEVE or STT tunneling to reach pods which are not on the same host.Packets are forwarded or dropped via the tunnel metadata information supplied by the ovn network controller.NAT is done for north-south communication.ovn-kubernetesDeploy via ansible. Distributed ACLs can be applied via Kubernetes policies. IPAM support. Load-balancing can be achieved without kube-proxy. NATing is done without using iptables/netsh.NAT (not used in Kubernetes)Containers are given a vNIC connected to an internal vSwitch. DNS/DHCP is provided using an internal component called WinNATMAC and IP is rewritten to host MAC/IP.natIncluded here for completeness As outlined above, the FlannelCNI pluginis also supported on Windows via theVXLAN network backend (Beta support ; delegates to win-overlay)and host-gateway network backend (stable support; delegates to win-bridge). This plugin supports delegating to one of the reference CNI plugins (win-overlay,win-bridge), to work in conjunction with Flannel daemon on Windows (Flanneld) forautomatic node subnet lease assignment and HNS network creation. This plugin readsin its own configuration file (cni.conf), and aggregates it with the environmentvariables from the FlannelD generated subnet.env file. It then delegates to one ofthe reference CNI plugins for network plumbing, and sends the correct configurationcontaining the node-assigned subnet to the IPAM plugin (for example: host-local). For Node, Pod, and Service objects, the following network flows are supported forTCP/UDP traffic: Pod → Pod (IP)Pod → Pod (Name)Pod → Service (Cluster IP)Pod → Service (PQDN, but only if there are no ""."")Pod → Service (FQDN)Pod → external (IP)Pod → external (DNS)Node → PodPod → Node",954
6.9 - Networking on Windows,IP address management (IPAM),IP address management (IPAM) The following IPAM options are supported on Windows: host-localazure-vnet-ipam (for azure-cni only)Windows Server IPAM (fallback option if no IPAM is set),52
6.9 - Networking on Windows,Load balancing and Services,"Load balancing and Services A Kubernetes Service is an abstractionthat defines a logical set of Pods and a means to access them over a network.In a cluster that includes Windows nodes, you can use the following types of Service: NodePortClusterIPLoadBalancerExternalName Windows container networking differs in some important ways from Linux networking.The Microsoft documentation for Windows Container Networkingprovides additional details and background. On Windows, you can use the following settings to configure Services and loadbalancing behavior: Windows Service SettingsFeatureDescriptionMinimum Supported Windows OS buildHow to enableSession affinityEnsures that connections from a particular client are passed to the same Pod each time.Windows Server 2022Set service.spec.sessionAffinity to ""ClientIP""Direct Server Return (DSR)Load balancing mode where the IP address fixups and the LBNAT occurs at the container vSwitch port directly; service traffic arrives with the source IP set as the originating pod IP.Windows Server 2019Set the following flags in kube-proxy: --feature-gates=""WinDSR=true"" --enable-dsr=truePreserve-DestinationSkips DNAT of service traffic, thereby preserving the virtual IP of the target service in packets reaching the backend Pod. Also disables node-node forwarding.Windows Server, version 1903Set ""preserve-destination"": ""true"" in service annotations and enable DSR in kube-proxy.IPv4/IPv6 dual-stack networkingNative IPv4-to-IPv4 in parallel with IPv6-to-IPv6 communications to, from, and within a clusterWindows Server 2019See IPv4/IPv6 dual-stackClient IP preservationEnsures that source IP of incoming ingress traffic gets preserved. Also disables node-node forwarding.Windows Server 2019Set service.spec.externalTrafficPolicy to ""Local"" and enable DSR in kube-proxy Warning:There are known issue with NodePort Services on overlay networking, if the destination node is running Windows Server 2022.To avoid the issue entirely, you can configure the service with externalTrafficPolicy: Local.There are known issues with Pod to Pod connectivity on l2bridge network on Windows Server 2022 with KB5005619 or higher installed.To workaround the issue and restore Pod to Pod connectivity, you can disable the WinDSR feature in kube-proxy.These issues require OS fixes.Please follow https://github.com/microsoft/Windows-Containers/issues/204 for updates.",519
6.9 - Networking on Windows,Limitations,"Limitations The following networking functionality is not supported on Windows nodes: Host networking modeLocal NodePort access from the node itself (works for other nodes or external clients)More than 64 backend pods (or unique destination addresses) for a single ServiceIPv6 communication between Windows pods connected to overlay networksLocal Traffic Policy in non-DSR modeOutbound communication using the ICMP protocol via the win-overlay, win-bridge, or using the Azure-CNI plugin.Specifically, the Windows data plane (VFP)doesn't support ICMP packet transpositions, and this means:ICMP packets directed to destinations within the same network (such as pod to pod communication via ping)work as expected;TCP/UDP packets work as expected;ICMP packets directed to pass through a remote network (e.g. pod to external internet communication via ping)cannot be transposed and thus will not be routed back to their source;Since TCP/UDP packets can still be transposed, you can substitute ping <destination> withcurl <destination> when debugging connectivity with the outside world. Other limitations: Windows reference network plugins win-bridge and win-overlay do not implementCNI spec v0.4.0,due to a missing CHECK implementation.The Flannel VXLAN CNI plugin has the following limitations on Windows:Node-pod connectivity is only possible for local pods with Flannel v0.12.0 (or higher).Flannel is restricted to using VNI 4096 and UDP port 4789. See the officialFlannel VXLANbackend docs for more details on these parameters.",336
6.10 - Service ClusterIP allocation,default,"In Kubernetes, Services are an abstract way to exposean application running on a set of Pods. Servicescan have a cluster-scoped virtual IP address (using a Service of type: ClusterIP).Clients can connect using that virtual IP address, and Kubernetes then load-balances traffic to thatService across the different backing Pods.",75
6.10 - Service ClusterIP allocation,How Service ClusterIPs are allocated?,"How Service ClusterIPs are allocated? When Kubernetes needs to assign a virtual IP address for a Service,that assignment happens one of two ways: dynamicallythe cluster's control plane automatically picks a free IP address from within the configured IP range for type: ClusterIP Services.staticallyyou specify an IP address of your choice, from within the configured IP range for Services. Across your whole cluster, every Service ClusterIP must be unique.Trying to create a Service with a specific ClusterIP that has alreadybeen allocated will return an error.",111
6.10 - Service ClusterIP allocation,Why do you need to reserve Service Cluster IPs?,"Why do you need to reserve Service Cluster IPs? Sometimes you may want to have Services running in well-known IP addresses, so other components andusers in the cluster can use them. The best example is the DNS Service for the cluster. As a soft convention, some Kubernetes installers assign the 10th IP address fromthe Service IP range to the DNS service. Assuming you configured your cluster with Service IP range10.96.0.0/16 and you want your DNS Service IP to be 10.96.0.10, you'd have to create a Service likethis: apiVersion: v1kind: Servicemetadata:  labels:    k8s-app: kube-dns    kubernetes.io/cluster-service: ""true""    kubernetes.io/name: CoreDNS  name: kube-dns  namespace: kube-systemspec:  clusterIP: 10.96.0.10  ports:  - name: dns    port: 53    protocol: UDP    targetPort: 53  - name: dns-tcp    port: 53    protocol: TCP    targetPort: 53  selector:    k8s-app: kube-dns  type: ClusterIP but as it was explained before, the IP address 10.96.0.10 has not been reserved; if other Services are createdbefore or in parallel with dynamic allocation, there is a chance they can allocate this IP, hence,you will not be able to create the DNS Service because it will fail with a conflict error.",337
6.10 - Service ClusterIP allocation,How can you avoid Service ClusterIP conflicts?,"How can you avoid Service ClusterIP conflicts? The allocation strategy implemented in Kubernetes to allocate ClusterIPs to Services reduces therisk of collision. The ClusterIP range is divided, based on the formula min(max(16, cidrSize / 16), 256),described as never less than 16 or more than 256 with a graduated step between them. Dynamic IP assignment uses the upper band by default, once this has been exhausted it willuse the lower range. This will allow users to use static allocations on the lower band with a lowrisk of collision.",115
6.10 - Service ClusterIP allocation,Example 1,"Example 1 This example uses the IP address range: 10.96.0.0/24 (CIDR notation) for the IP addressesof Services. Range Size: 28 - 2 = 254Band Offset: min(max(16, 256/16), 256) = min(16, 256) = 16Static band start: 10.96.0.1Static band end: 10.96.0.16Range end: 10.96.0.254 pie showDatatitle 10.96.0.0/24""Static"" : 16""Dynamic"" : 238 JavaScript must be enabled to view this content",129
6.10 - Service ClusterIP allocation,Example 2,"Example 2 This example uses the IP address range: 10.96.0.0/20 (CIDR notation) for the IP addressesof Services. Range Size: 212 - 2 = 4094Band Offset: min(max(16, 4096/16), 256) = min(256, 256) = 256Static band start: 10.96.0.1Static band end: 10.96.1.0Range end: 10.96.15.254 pie showDatatitle 10.96.0.0/20""Static"" : 256""Dynamic"" : 3838 JavaScript must be enabled to view this content",131
6.10 - Service ClusterIP allocation,Example 3,"Example 3 This example uses the IP address range: 10.96.0.0/16 (CIDR notation) for the IP addressesof Services. Range Size: 216 - 2 = 65534Band Offset: min(max(16, 65536/16), 256) = min(4096, 256) = 256Static band start: 10.96.0.1Static band ends: 10.96.1.0Range end: 10.96.255.254 pie showDatatitle 10.96.0.0/16""Static"" : 256""Dynamic"" : 65278 JavaScript must be enabled to view this content Read about Service External Traffic PolicyRead about Connecting Applications with ServicesRead about Services",149
6.11 - Service Internal Traffic Policy,default,"If two Pods in your cluster want to communicate, and both Pods are actually running on the same node, use Service Internal Traffic Policy to keep network traffic within that node. Avoiding a round trip via the cluster network can help with reliability, performance (network latency and throughput), or cost. FEATURE STATE: Kubernetes v1.26 [stable] Service Internal Traffic Policy enables internal traffic restrictions to only routeinternal traffic to endpoints within the node the traffic originated from. The""internal"" traffic here refers to traffic originated from Pods in the currentcluster. This can help to reduce costs and improve performance.",128
6.11 - Service Internal Traffic Policy,Using Service Internal Traffic Policy,"Using Service Internal Traffic Policy You can enable the internal-only traffic policy for aService, by setting its.spec.internalTrafficPolicy to Local. This tells kube-proxy to only use node localendpoints for cluster internal traffic. Note: For pods on nodes with no endpoints for a given Service, the Servicebehaves as if it has zero endpoints (for Pods on this node) even if the servicedoes have endpoints on other nodes. The following example shows what a Service looks like when you set.spec.internalTrafficPolicy to Local: apiVersion: v1kind: Servicemetadata:  name: my-servicespec:  selector:    app.kubernetes.io/name: MyApp  ports:    - protocol: TCP      port: 80      targetPort: 9376  internalTrafficPolicy: Local",180
6.11 - Service Internal Traffic Policy,How it works,"How it works The kube-proxy filters the endpoints it routes to based on thespec.internalTrafficPolicy setting. When it's set to Local, only node localendpoints are considered. When it's Cluster (the default), or is not set,Kubernetes considers all endpoints. Read about Topology Aware HintsRead about Service External Traffic PolicyFollow the Connecting Applications with Services tutorial",84
6.12 - Topology-aware traffic routing with topology keys,default,"FEATURE STATE: Kubernetes v1.21 [deprecated] Note: This feature, specifically the alpha topologyKeys API, is deprecated sinceKubernetes v1.21.Topology Aware Hints,introduced in Kubernetes v1.21, provide similar functionality. Service Topology enables a service to route traffic based upon the Nodetopology of the cluster. For example, a service can specify that traffic bepreferentially routed to endpoints that are on the same Node as the client, orin the same availability zone.",119
6.12 - Topology-aware traffic routing with topology keys,Topology-aware traffic routing,"Topology-aware traffic routing By default, traffic sent to a ClusterIP or NodePort Service may be routed toany backend address for the Service. Kubernetes 1.7 made it possible toroute ""external"" traffic to the Pods running on the same Node that received thetraffic. For ClusterIP Services, the equivalent same-node preference forrouting wasn't possible; nor could you configure your cluster to favor routingto endpoints within the same zone.By setting topologyKeys on a Service, you're able to define a policy for routingtraffic based upon the Node labels for the originating and destination Nodes. The label matching between the source and destination lets you, as a clusteroperator, designate sets of Nodes that are ""closer"" and ""farther"" from one another.You can define labels to represent whatever metric makes sense for your ownrequirements.In public clouds, for example, you might prefer to keep network traffic within thesame zone, because interzonal traffic has a cost associated with it (and intrazonaltraffic typically does not). Other common needs include being able to route trafficto a local Pod managed by a DaemonSet, or directing traffic to Nodes connected to thesame top-of-rack switch for the lowest latency.",266
6.12 - Topology-aware traffic routing with topology keys,Using Service Topology,"Using Service Topology If your cluster has the ServiceTopology feature gate enabled, you can control Service trafficrouting by specifying the topologyKeys field on the Service spec. This fieldis a preference-order list of Node labels which will be used to sort endpointswhen accessing this Service. Traffic will be directed to a Node whose value forthe first label matches the originating Node's value for that label. If there isno backend for the Service on a matching Node, then the second label will beconsidered, and so forth, until no labels remain. If no match is found, the traffic will be rejected, as if there were nobackends for the Service at all. That is, endpoints are chosen based on the firsttopology key with available backends. If this field is specified and all entrieshave no backends that match the topology of the client, the service has nobackends for that client and connections should fail. The special value ""*"" maybe used to mean ""any topology"". This catch-all value, if used, only makes senseas the last value in the list. If topologyKeys is not specified or empty, no topology constraints will be applied. Consider a cluster with Nodes that are labeled with their hostname, zone name,and region name. Then you can set the topologyKeys values of a service to directtraffic as follows. Only to endpoints on the same node, failing if no endpoint exists on the node:[""kubernetes.io/hostname""].Preferentially to endpoints on the same node, falling back to endpoints in thesame zone, followed by the same region, and failing otherwise: [""kubernetes.io/hostname"", ""topology.kubernetes.io/zone"", ""topology.kubernetes.io/region""].This may be useful, for example, in cases where data locality is critical.Preferentially to the same zone, but fallback on any available endpoint ifnone are available within this zone:[""topology.kubernetes.io/zone"", ""*""].",440
6.12 - Topology-aware traffic routing with topology keys,Constraints,"Constraints Service topology is not compatible with externalTrafficPolicy=Local, andtherefore a Service cannot use both of these features. It is possible to useboth features in the same cluster on different Services, only not on the sameService.Valid topology keys are currently limited to kubernetes.io/hostname,topology.kubernetes.io/zone, and topology.kubernetes.io/region, but willbe generalized to other node labels in the future.Topology keys must be valid label keys and at most 16 keys may be specified.The catch-all value, ""*"", must be the last value in the topology keys, ifit is used.",148
6.12 - Topology-aware traffic routing with topology keys,Only Node Local Endpoints,"Only Node Local Endpoints A Service that only routes to node local endpoints. If no endpoints exist on the node, traffic is dropped: apiVersion: v1kind: Servicemetadata:  name: my-servicespec:  selector:    app: my-app  ports:    - protocol: TCP      port: 80      targetPort: 9376  topologyKeys:    - ""kubernetes.io/hostname""",94
6.12 - Topology-aware traffic routing with topology keys,Prefer Node Local Endpoints,"Prefer Node Local Endpoints A Service that prefers node local Endpoints but falls back to cluster wide endpoints if node local endpoints do not exist: apiVersion: v1kind: Servicemetadata:  name: my-servicespec:  selector:    app: my-app  ports:    - protocol: TCP      port: 80      targetPort: 9376  topologyKeys:    - ""kubernetes.io/hostname""    - ""*""",101
6.12 - Topology-aware traffic routing with topology keys,Only Zonal or Regional Endpoints,"Only Zonal or Regional Endpoints A Service that prefers zonal then regional endpoints. If no endpoints exist in either, traffic is dropped. apiVersion: v1kind: Servicemetadata:  name: my-servicespec:  selector:    app: my-app  ports:    - protocol: TCP      port: 80      targetPort: 9376  topologyKeys:    - ""topology.kubernetes.io/zone""    - ""topology.kubernetes.io/region""",112
6.12 - Topology-aware traffic routing with topology keys,"Prefer Node Local, Zonal, then Regional Endpoints","Prefer Node Local, Zonal, then Regional Endpoints A Service that prefers node local, zonal, then regional endpoints but falls back to cluster wide endpoints. apiVersion: v1kind: Servicemetadata:  name: my-servicespec:  selector:    app: my-app  ports:    - protocol: TCP      port: 80      targetPort: 9376  topologyKeys:    - ""kubernetes.io/hostname""    - ""topology.kubernetes.io/zone""    - ""topology.kubernetes.io/region""    - ""*"" Read about Topology Aware HintsRead Connecting Applications with Services",148
7.1 - Volumes,default,"On-disk files in a container are ephemeral, which presents some problems fornon-trivial applications when running in containers. One problemis the loss of files when a container crashes. The kubelet restarts the containerbut with a clean state. A second problem occurs when sharing filesbetween containers running together in a Pod.The Kubernetes volume abstractionsolves both of these problems.Familiarity with Pods is suggested.",92
7.1 - Volumes,Background,"Background Docker has a concept ofvolumes, though it issomewhat looser and less managed. A Docker volume is a directory ondisk or in another container. Docker provides volumedrivers, but the functionality is somewhat limited. Kubernetes supports many types of volumes. A Podcan use any number of volume types simultaneously.Ephemeral volume types have a lifetime of a pod, but persistent volumes exist beyondthe lifetime of a pod. When a pod ceases to exist, Kubernetes destroys ephemeral volumes;however, Kubernetes does not destroy persistent volumes.For any kind of volume in a given pod, data is preserved across container restarts. At its core, a volume is a directory, possibly with some data in it, whichis accessible to the containers in a pod. How that directory comes to be, themedium that backs it, and the contents of it are determined by the particularvolume type used. To use a volume, specify the volumes to provide for the Pod in .spec.volumesand declare where to mount those volumes into containers in .spec.containers[*].volumeMounts.A process in a container sees a filesystem view composed from the initial contents ofthe container image, plus volumes(if defined) mounted inside the container.The process sees a root filesystem that initially matches the contents of the containerimage.Any writes to within that filesystem hierarchy, if allowed, affect what that process viewswhen it performs a subsequent filesystem access.Volumes mount at the specified paths withinthe image.For each container defined within a Pod, you must independently specify whereto mount each volume that the container uses. Volumes cannot mount within other volumes (but see Using subPathfor a related mechanism). Also, a volume cannot contain a hard link to anything ina different volume.",373
7.1 - Volumes,awsElasticBlockStore (deprecated),"awsElasticBlockStore (deprecated) FEATURE STATE: Kubernetes v1.17 [deprecated] An awsElasticBlockStore volume mounts an Amazon Web Services (AWS)EBS volume into your pod. UnlikeemptyDir, which is erased when a pod is removed, the contents of an EBSvolume are persisted and the volume is unmounted. This means that anEBS volume can be pre-populated with data, and that data can be shared between pods. Note: You must create an EBS volume by using aws ec2 create-volume or the AWS API before you can use it. There are some restrictions when using an awsElasticBlockStore volume: the nodes on which pods are running must be AWS EC2 instancesthose instances need to be in the same region and availability zone as the EBS volumeEBS only supports a single EC2 instance mounting a volume",191
7.1 - Volumes,Creating an AWS EBS volume,"Creating an AWS EBS volume Before you can use an EBS volume with a pod, you need to create it. aws ec2 create-volume --availability-zone=eu-west-1a --size=10 --volume-type=gp2 Make sure the zone matches the zone you brought up your cluster in. Check that the size and EBS volumetype are suitable for your use.",84
7.1 - Volumes,AWS EBS configuration example,"AWS EBS configuration example apiVersion: v1kind: Podmetadata:  name: test-ebsspec:  containers:  - image: registry.k8s.io/test-webserver    name: test-container    volumeMounts:    - mountPath: /test-ebs      name: test-volume  volumes:  - name: test-volume    # This AWS EBS volume must already exist.    awsElasticBlockStore:      volumeID: ""<volume id>""      fsType: ext4 If the EBS volume is partitioned, you can supply the optional field partition: ""<partition number>"" to specify which partition to mount on.",145
7.1 - Volumes,AWS EBS CSI migration,"AWS EBS CSI migration FEATURE STATE: Kubernetes v1.25 [stable] The CSIMigration feature for awsElasticBlockStore, when enabled, redirectsall plugin operations from the existing in-tree plugin to the ebs.csi.aws.com ContainerStorage Interface (CSI) driver. In order to use this feature, the AWS EBS CSIdrivermust be installed on the cluster.",90
7.1 - Volumes,AWS EBS CSI migration complete,"AWS EBS CSI migration complete FEATURE STATE: Kubernetes v1.17 [alpha] To disable the awsElasticBlockStore storage plugin from being loaded by the controller managerand the kubelet, set the InTreePluginAWSUnregister flag to true.",59
7.1 - Volumes,azureDisk (deprecated),"azureDisk (deprecated) FEATURE STATE: Kubernetes v1.19 [deprecated] The azureDisk volume type mounts a Microsoft Azure Data Disk into a pod. For more details, see the azureDisk volume plugin.",51
7.1 - Volumes,azureDisk CSI migration,"azureDisk CSI migration FEATURE STATE: Kubernetes v1.24 [stable] The CSIMigration feature for azureDisk, when enabled, redirects all plugin operationsfrom the existing in-tree plugin to the disk.csi.azure.com ContainerStorage Interface (CSI) Driver. In order to use this feature, theAzure Disk CSI Drivermust be installed on the cluster.",85
7.1 - Volumes,azureDisk CSI migration complete,"azureDisk CSI migration complete FEATURE STATE: Kubernetes v1.21 [alpha] To disable the azureDisk storage plugin from being loaded by the controller managerand the kubelet, set the InTreePluginAzureDiskUnregister flag to true.",56
7.1 - Volumes,azureFile (deprecated),"azureFile (deprecated) FEATURE STATE: Kubernetes v1.21 [deprecated] The azureFile volume type mounts a Microsoft Azure File volume (SMB 2.1 and 3.0)into a pod. For more details, see the azureFile volume plugin.",62
7.1 - Volumes,azureFile CSI migration,"azureFile CSI migration FEATURE STATE: Kubernetes v1.26 [stable] The CSIMigration feature for azureFile, when enabled, redirects all plugin operationsfrom the existing in-tree plugin to the file.csi.azure.com ContainerStorage Interface (CSI) Driver. In order to use this feature, the Azure File CSIDrivermust be installed on the cluster and the CSIMigrationAzureFilefeature gates must be enabled. Azure File CSI driver does not support using same volume with different fsgroups. IfCSIMigrationAzureFile is enabled, using same volume with different fsgroups won't be supported at all.",137
7.1 - Volumes,azureFile CSI migration complete,"azureFile CSI migration complete FEATURE STATE: Kubernetes v1.21 [alpha] To disable the azureFile storage plugin from being loaded by the controller managerand the kubelet, set the InTreePluginAzureFileUnregister flag to true.",56
7.1 - Volumes,cephfs,"cephfs A cephfs volume allows an existing CephFS volume to bemounted into your Pod. Unlike emptyDir, which is erased when a pod isremoved, the contents of a cephfs volume are preserved and the volume is merelyunmounted. This means that a cephfs volume can be pre-populated with data, andthat data can be shared between pods. The cephfs volume can be mounted by multiplewriters simultaneously. Note: You must have your own Ceph server running with the share exported before you can use it. See the CephFS example for more details.",124
7.1 - Volumes,cinder (deprecated),cinder (deprecated) FEATURE STATE: Kubernetes v1.18 [deprecated] Note: Kubernetes must be configured with the OpenStack cloud provider. The cinder volume type is used to mount the OpenStack Cinder volume into your pod.,57
7.1 - Volumes,Cinder volume configuration example,"Cinder volume configuration example apiVersion: v1kind: Podmetadata:  name: test-cinderspec:  containers:  - image: registry.k8s.io/test-webserver    name: test-cinder-container    volumeMounts:    - mountPath: /test-cinder      name: test-volume  volumes:  - name: test-volume    # This OpenStack volume must already exist.    cinder:      volumeID: ""<volume id>""      fsType: ext4",112
7.1 - Volumes,OpenStack CSI migration,"OpenStack CSI migration FEATURE STATE: Kubernetes v1.24 [stable] The CSIMigration feature for Cinder is enabled by default since Kubernetes 1.21.It redirects all plugin operations from the existing in-tree plugin to thecinder.csi.openstack.org Container Storage Interface (CSI) Driver.OpenStack Cinder CSI Drivermust be installed on the cluster. To disable the in-tree Cinder plugin from being loaded by the controller managerand the kubelet, you can enable the InTreePluginOpenStackUnregisterfeature gate.",123
7.1 - Volumes,configMap,"configMap A ConfigMapprovides a way to inject configuration data into pods.The data stored in a ConfigMap can be referenced in a volume of typeconfigMap and then consumed by containerized applications running in a pod. When referencing a ConfigMap, you provide the name of the ConfigMap in thevolume. You can customize the path to use for a specificentry in the ConfigMap. The following configuration shows how to mountthe log-config ConfigMap onto a Pod called configmap-pod: apiVersion: v1kind: Podmetadata:  name: configmap-podspec:  containers:    - name: test      image: busybox:1.28      volumeMounts:        - name: config-vol          mountPath: /etc/config  volumes:    - name: config-vol      configMap:        name: log-config        items:          - key: log_level            path: log_level The log-config ConfigMap is mounted as a volume, and all contents stored inits log_level entry are mounted into the Pod at path /etc/config/log_level.Note that this path is derived from the volume's mountPath and the pathkeyed with log_level. Note:You must create a ConfigMapbefore you can use it.A container using a ConfigMap as a subPath volume mount will notreceive ConfigMap updates.Text data is exposed as files using the UTF-8 character encoding. For other character encodings, use binaryData.",313
7.1 - Volumes,downwardAPI,"downwardAPI A downwardAPI volume makes downward APIdata available to applications. Within the volume, you can find the exposeddata as read-only files in plain text format. Note: A container using the downward API as a subPath volume mount does notreceive updates when field values change. See Expose Pod Information to Containers Through Filesto learn more.",75
7.1 - Volumes,emptyDir,"emptyDir An emptyDir volume is first created when a Pod is assigned to a node, andexists as long as that Pod is running on that node. As the name says, theemptyDir volume is initially empty. All containers in the Pod can read and write the samefiles in the emptyDir volume, though that volume can be mounted at the sameor different paths in each container. When a Pod is removed from a node forany reason, the data in the emptyDir is deleted permanently. Note: A container crashing does not remove a Pod from a node. The data in an emptyDir volumeis safe across container crashes. Some uses for an emptyDir are: scratch space, such as for a disk-based merge sortcheckpointing a long computation for recovery from crashesholding files that a content-manager container fetches while a webservercontainer serves the data The emptyDir.medium field controls where emptyDir volumes are stored. Bydefault emptyDir volumes are stored on whatever medium that backs the nodesuch as disk, SSD, or network storage, depending on your environment. If you setthe emptyDir.medium field to ""Memory"", Kubernetes mounts a tmpfs (RAM-backedfilesystem) for you instead. While tmpfs is very fast, be aware that unlikedisks, tmpfs is cleared on node reboot and any files you write count againstyour container's memory limit. A size limit can be specified for the default medium, which limits the capacityof the emptyDir volume. The storage is allocated from node ephemeralstorage.If that is filled up from another source (for example, log files or imageoverlays), the emptyDir may run out of capacity before this limit. Note: If the SizeMemoryBackedVolumes feature gate is enabled,you can specify a size for memory backed volumes. If no size is specified, memorybacked volumes are sized to 50% of the memory on a Linux host.",396
7.1 - Volumes,emptyDir configuration example,emptyDir configuration example apiVersion: v1kind: Podmetadata:  name: test-pdspec:  containers:  - image: registry.k8s.io/test-webserver    name: test-container    volumeMounts:    - mountPath: /cache      name: cache-volume  volumes:  - name: cache-volume    emptyDir:      sizeLimit: 500Mi,86
7.1 - Volumes,fc (fibre channel),"fc (fibre channel) An fc volume type allows an existing fibre channel block storage volumeto mount in a Pod. You can specify single or multiple target world wide names (WWNs)using the parameter targetWWNs in your Volume configuration. If multiple WWNs are specified,targetWWNs expect that those WWNs are from multi-path connections. Note: You must configure FC SAN Zoning to allocate and mask those LUNs (volumes) to the target WWNsbeforehand so that Kubernetes hosts can access them. See the fibre channel examplefor more details.",122
7.1 - Volumes,gcePersistentDisk (deprecated),"gcePersistentDisk (deprecated) FEATURE STATE: Kubernetes v1.17 [deprecated] A gcePersistentDisk volume mounts a Google Compute Engine (GCE)persistent disk (PD) into your Pod.Unlike emptyDir, which is erased when a pod is removed, the contents of a PD arepreserved and the volume is merely unmounted. This means that a PD can bepre-populated with data, and that data can be shared between pods. Note: You must create a PD using gcloud or the GCE API or UI before you can use it. There are some restrictions when using a gcePersistentDisk: the nodes on which Pods are running must be GCE VMsthose VMs need to be in the same GCE project and zone as the persistent disk One feature of GCE persistent disk is concurrent read-only access to a persistent disk.A gcePersistentDisk volume permits multiple consumers to simultaneouslymount a persistent disk as read-only. This means that you can pre-populate a PD with your datasetand then serve it in parallel from as many Pods as you need. Unfortunately,PDs can only be mounted by a single consumer in read-write mode. Simultaneouswriters are not allowed. Using a GCE persistent disk with a Pod controlled by a ReplicaSet will fail unlessthe PD is read-only or the replica count is 0 or 1.",301
7.1 - Volumes,Creating a GCE persistent disk,"Creating a GCE persistent disk Before you can use a GCE persistent disk with a Pod, you need to create it. gcloud compute disks create --size=500GB --zone=us-central1-a my-data-disk",49
7.1 - Volumes,GCE persistent disk configuration example,GCE persistent disk configuration example apiVersion: v1kind: Podmetadata:  name: test-pdspec:  containers:  - image: registry.k8s.io/test-webserver    name: test-container    volumeMounts:    - mountPath: /test-pd      name: test-volume  volumes:  - name: test-volume    # This GCE PD must already exist.    gcePersistentDisk:      pdName: my-data-disk      fsType: ext4,113
7.1 - Volumes,Regional persistent disks,"Regional persistent disks The Regional persistent disksfeature allows the creation of persistent disks that are available in two zoneswithin the same region. In order to use this feature, the volume must be provisionedas a PersistentVolume; referencing the volume directly from a pod is not supported.",56
7.1 - Volumes,Manually provisioning a Regional PD PersistentVolume,"Manually provisioning a Regional PD PersistentVolume Dynamic provisioning is possible using aStorageClass for GCE PD.Before creating a PersistentVolume, you must create the persistent disk: gcloud compute disks create --size=500GB my-data-disk  --region us-central1  --replica-zones us-central1-a,us-central1-b",80
7.1 - Volumes,Regional persistent disk configuration example,Regional persistent disk configuration example apiVersion: v1kind: PersistentVolumemetadata:  name: test-volumespec:  capacity:    storage: 400Gi  accessModes:  - ReadWriteOnce  gcePersistentDisk:    pdName: my-data-disk    fsType: ext4  nodeAffinity:    required:      nodeSelectorTerms:      - matchExpressions:        # failure-domain.beta.kubernetes.io/zone should be used prior to 1.21        - key: topology.kubernetes.io/zone          operator: In          values:          - us-central1-a          - us-central1-b,153
7.1 - Volumes,GCE CSI migration,"GCE CSI migration FEATURE STATE: Kubernetes v1.25 [stable] The CSIMigration feature for GCE PD, when enabled, redirects all plugin operationsfrom the existing in-tree plugin to the pd.csi.storage.gke.io ContainerStorage Interface (CSI) Driver. In order to use this feature, the GCE PD CSIDrivermust be installed on the cluster.",88
7.1 - Volumes,GCE CSI migration complete,"GCE CSI migration complete FEATURE STATE: Kubernetes v1.21 [alpha] To disable the gcePersistentDisk storage plugin from being loaded by the controller managerand the kubelet, set the InTreePluginGCEUnregister flag to true.",56
7.1 - Volumes,gitRepo (deprecated),"gitRepo (deprecated) Warning: The gitRepo volume type is deprecated. To provision a container with a git repo, mount anEmptyDir into an InitContainer that clones the repo using git, then mount theEmptyDir into the Pod's container. A gitRepo volume is an example of a volume plugin. This pluginmounts an empty directory and clones a git repository into this directoryfor your Pod to use. Here is an example of a gitRepo volume: apiVersion: v1kind: Podmetadata:  name: serverspec:  containers:  - image: nginx    name: nginx    volumeMounts:    - mountPath: /mypath      name: git-volume  volumes:  - name: git-volume    gitRepo:      repository: ""git@somewhere:me/my-git-repository.git""      revision: ""22f1d8406d464b0c0874075539c1f2e96c253775""",214
7.1 - Volumes,glusterfs (removed),glusterfs (removed) Kubernetes 1.26 does not include a glusterfs volume type. The GlusterFS in-tree storage driver was deprecated in the Kubernetes v1.25 releaseand then removed entirely in the v1.26 release.,58
7.1 - Volumes,hostPath,"hostPath Warning:HostPath volumes present many security risks, and it is a best practice to avoid the use ofHostPaths when possible. When a HostPath volume must be used, it should be scoped to only therequired file or directory, and mounted as ReadOnly.If restricting HostPath access to specific directories through AdmissionPolicy, volumeMounts MUSTbe required to use readOnly mounts for the policy to be effective. A hostPath volume mounts a file or directory from the host node's filesysteminto your Pod. This is not something that most Pods will need, but it offers apowerful escape hatch for some applications. For example, some uses for a hostPath are: running a container that needs access to Docker internals; use a hostPathof /var/lib/dockerrunning cAdvisor in a container; use a hostPath of /sysallowing a Pod to specify whether a given hostPath should exist prior to thePod running, whether it should be created, and what it should exist as In addition to the required path property, you can optionally specify a type for a hostPath volume. The supported values for field type are: ValueBehaviorEmpty string (default) is for backward compatibility, which means that no checks will be performed before mounting the hostPath volume.DirectoryOrCreateIf nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet.DirectoryA directory must exist at the given pathFileOrCreateIf nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet.FileA file must exist at the given pathSocketA UNIX socket must exist at the given pathCharDeviceA character device must exist at the given pathBlockDeviceA block device must exist at the given path Watch out when using this type of volume, because: HostPaths can expose privileged system credentials (such as for the Kubelet) or privileged APIs(such as container runtime socket), which can be used for container escape or to attack otherparts of the cluster.Pods with identical configuration (such as created from a PodTemplate) maybehave differently on different nodes due to different files on the nodesThe files or directories created on the underlying hosts are only writable by root. Youeither need to run your process as root in aprivileged Container or modify the filepermissions on the host to be able to write to a hostPath volume",522
7.1 - Volumes,hostPath configuration example,"hostPath configuration example apiVersion: v1kind: Podmetadata:  name: test-pdspec:  containers:  - image: registry.k8s.io/test-webserver    name: test-container    volumeMounts:    - mountPath: /test-pd      name: test-volume  volumes:  - name: test-volume    hostPath:      # directory location on host      path: /data      # this field is optional      type: Directory Caution: The FileOrCreate mode does not create the parent directory of the file. If the parent directoryof the mounted file does not exist, the pod fails to start. To ensure that this mode works,you can try to mount directories and files separately, as shown in theFileOrCreateconfiguration.",166
7.1 - Volumes,hostPath FileOrCreate configuration example,hostPath FileOrCreate configuration example apiVersion: v1kind: Podmetadata:  name: test-webserverspec:  containers:  - name: test-webserver    image: registry.k8s.io/test-webserver:latest    volumeMounts:    - mountPath: /var/local/aaa      name: mydir    - mountPath: /var/local/aaa/1.txt      name: myfile  volumes:  - name: mydir    hostPath:      # Ensure the file directory is created.      path: /var/local/aaa      type: DirectoryOrCreate  - name: myfile    hostPath:      path: /var/local/aaa/1.txt      type: FileOrCreate,165
7.1 - Volumes,iscsi,"iscsi An iscsi volume allows an existing iSCSI (SCSI over IP) volume to be mountedinto your Pod. Unlike emptyDir, which is erased when a Pod is removed, thecontents of an iscsi volume are preserved and the volume is merelyunmounted. This means that an iscsi volume can be pre-populated with data, andthat data can be shared between pods. Note: You must have your own iSCSI server running with the volume created before you can use it. A feature of iSCSI is that it can be mounted as read-only by multiple consumerssimultaneously. This means that you can pre-populate a volume with your datasetand then serve it in parallel from as many Pods as you need. Unfortunately,iSCSI volumes can only be mounted by a single consumer in read-write mode.Simultaneous writers are not allowed. See the iSCSI example for more details.",197
7.1 - Volumes,local,"local A local volume represents a mounted local storage device such as a disk,partition or directory. Local volumes can only be used as a statically created PersistentVolume. Dynamicprovisioning is not supported. Compared to hostPath volumes, local volumes are used in a durable andportable manner without manually scheduling pods to nodes. The system is awareof the volume's node constraints by looking at the node affinity on the PersistentVolume. However, local volumes are subject to the availability of the underlyingnode and are not suitable for all applications. If a node becomes unhealthy,then the local volume becomes inaccessible by the pod. The pod using this volumeis unable to run. Applications using local volumes must be able to tolerate thisreduced availability, as well as potential data loss, depending on thedurability characteristics of the underlying disk. The following example shows a PersistentVolume using a local volume andnodeAffinity: apiVersion: v1kind: PersistentVolumemetadata:  name: example-pvspec:  capacity:    storage: 100Gi  volumeMode: Filesystem  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Delete  storageClassName: local-storage  local:    path: /mnt/disks/ssd1  nodeAffinity:    required:      nodeSelectorTerms:      - matchExpressions:        - key: kubernetes.io/hostname          operator: In          values:          - example-node You must set a PersistentVolume nodeAffinity when using local volumes.The Kubernetes scheduler uses the PersistentVolume nodeAffinity to schedulethese Pods to the correct node. PersistentVolume volumeMode can be set to ""Block"" (instead of the defaultvalue ""Filesystem"") to expose the local volume as a raw block device. When using local volumes, it is recommended to create a StorageClass withvolumeBindingMode set to WaitForFirstConsumer. For more details, see thelocal StorageClass example.Delaying volume binding ensures that the PersistentVolumeClaim binding decisionwill also be evaluated with any other node constraints the Pod may have,such as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity. An external static provisioner can be run separately for improved management ofthe local volume lifecycle. Note that this provisioner does not support dynamicprovisioning yet. For an example on how to run an external local provisioner,see the local volume provisioner userguide. Note: The local PersistentVolume requires manual cleanup and deletion by theuser if the external static provisioner is not used to manage the volumelifecycle.",554
7.1 - Volumes,nfs,"nfs An nfs volume allows an existing NFS (Network File System) share to bemounted into a Pod. Unlike emptyDir, which is erased when a Pod isremoved, the contents of an nfs volume are preserved and the volume is merelyunmounted. This means that an NFS volume can be pre-populated with data, andthat data can be shared between pods. NFS can be mounted by multiplewriters simultaneously. apiVersion: v1kind: Podmetadata:  name: test-pdspec:  containers:  - image: registry.k8s.io/test-webserver    name: test-container    volumeMounts:    - mountPath: /my-nfs-data      name: test-volume  volumes:  - name: test-volume    nfs:      server: my-nfs-server.example.com      path: /my-nfs-volume      readOnly: true Note:You must have your own NFS server running with the share exported before you can use it.Also note that you can't specify NFS mount options in a Pod spec. You can either set mount options server-side oruse /etc/nfsmount.conf.You can also mount NFS volumes via PersistentVolumes which do allow you to set mount options. See the NFS examplefor an example of mounting NFS volumes with PersistentVolumes.",297
7.1 - Volumes,persistentVolumeClaim,"persistentVolumeClaim A persistentVolumeClaim volume is used to mount aPersistentVolume into a Pod. PersistentVolumeClaimsare a way for users to ""claim"" durable storage (such as a GCE PersistentDisk or aniSCSI volume) without knowing the details of the particular cloud environment. See the information about PersistentVolumes for moredetails.",75
7.1 - Volumes,portworxVolume (deprecated),"portworxVolume (deprecated) FEATURE STATE: Kubernetes v1.25 [deprecated] A portworxVolume is an elastic block storage layer that runs hyperconverged withKubernetes. Portworx fingerprints storagein a server, tiers based on capabilities, and aggregates capacity across multiple servers.Portworx runs in-guest in virtual machines or on bare metal Linux nodes. A portworxVolume can be dynamically created through Kubernetes or it can alsobe pre-provisioned and referenced inside a Pod.Here is an example Pod referencing a pre-provisioned Portworx volume: apiVersion: v1kind: Podmetadata:  name: test-portworx-volume-podspec:  containers:  - image: registry.k8s.io/test-webserver    name: test-container    volumeMounts:    - mountPath: /mnt      name: pxvol  volumes:  - name: pxvol    # This Portworx volume must already exist.    portworxVolume:      volumeID: ""pxvol""      fsType: ""<fs-type>"" Note: Make sure you have an existing PortworxVolume with name pxvolbefore using it in the Pod. For more details, see the Portworx volume examples.",294
7.1 - Volumes,Portworx CSI migration,"Portworx CSI migration FEATURE STATE: Kubernetes v1.25 [beta] The CSIMigration feature for Portworx has been added but disabled by default in Kubernetes 1.23 since it's in alpha state.It has been beta now since v1.25 but it is still turned off by default.It redirects all plugin operations from the existing in-tree plugin to thepxd.portworx.com Container Storage Interface (CSI) Driver.Portworx CSI Drivermust be installed on the cluster.To enable the feature, set CSIMigrationPortworx=true in kube-controller-manager and kubelet.",145
7.1 - Volumes,rbd,"rbd An rbd volume allows aRados Block Device (RBD) volume to mountinto your Pod. Unlike emptyDir, which is erased when a pod is removed, thecontents of an rbd volume are preserved and the volume is unmounted. Thismeans that a RBD volume can be pre-populated with data, and that data can beshared between pods. Note: You must have a Ceph installation running before you can use RBD. A feature of RBD is that it can be mounted as read-only by multiple consumerssimultaneously. This means that you can pre-populate a volume with your datasetand then serve it in parallel from as many pods as you need. Unfortunately,RBD volumes can only be mounted by a single consumer in read-write mode.Simultaneous writers are not allowed. See the RBD examplefor more details.",183
7.1 - Volumes,RBD CSI migration,"RBD CSI migration FEATURE STATE: Kubernetes v1.23 [alpha] The CSIMigration feature for RBD, when enabled, redirects all pluginoperations from the existing in-tree plugin to the rbd.csi.ceph.com CSI driver. In order to use thisfeature, theCeph CSI drivermust be installed on the cluster and the CSIMigrationRBDfeature gatemust be enabled. (Note that the csiMigrationRBD flag has been removed andreplaced with CSIMigrationRBD in release v1.24) Note:As a Kubernetes cluster operator that administers storage, here are theprerequisites that you must complete before you attempt migration to theRBD CSI driver:You must install the Ceph CSI driver (rbd.csi.ceph.com), v3.5.0 or above,into your Kubernetes cluster.considering the clusterID field is a required parameter for CSI driver forits operations, but in-tree StorageClass has monitors field as a requiredparameter, a Kubernetes storage admin has to create a clusterID based on themonitors hash ( ex:#echo -n '<monitors_string>' | md5sum) in the CSI config map and keep the monitorsunder this clusterID configuration.Also, if the value of adminId in the in-tree Storageclass is different fromadmin, the adminSecretName mentioned in the in-tree Storageclass has to bepatched with the base64 value of the adminId parameter value, otherwise thisstep can be skipped.",334
7.1 - Volumes,secret,"secret A secret volume is used to pass sensitive information, such as passwords, toPods. You can store secrets in the Kubernetes API and mount them as files foruse by pods without coupling to Kubernetes directly. secret volumes arebacked by tmpfs (a RAM-backed filesystem) so they are never written tonon-volatile storage. Note: You must create a Secret in the Kubernetes API before you can use it. Note: A container using a Secret as a subPath volume mount will notreceive Secret updates. For more details, see Configuring Secrets.",123
7.1 - Volumes,vsphereVolume (deprecated),"vsphereVolume (deprecated) Note: We recommend to use vSphere CSI out-of-tree driver instead. A vsphereVolume is used to mount a vSphere VMDK volume into your Pod. The contentsof a volume are preserved when it is unmounted. It supports both VMFS and VSAN datastore. For more information, see the vSphere volume examples.",82
7.1 - Volumes,vSphere CSI migration,"vSphere CSI migration FEATURE STATE: Kubernetes v1.26 [stable] In Kubernetes 1.26, all operations for the in-tree vsphereVolume typeare redirected to the csi.vsphere.vmware.com CSI driver. vSphere CSI drivermust be installed on the cluster. You can find additional advice on how to migrate in-tree vsphereVolume in VMware's documentation pageMigrating In-Tree vSphere Volumes to vSphere Container Storage lug-in.If vSphere CSI Driver is not installed volume operations can not be performed on the PV created with the in-tree vsphereVolume type. You must run vSphere 7.0u2 or later in order to migrate to the vSphere CSI driver. If you are running a version of Kubernetes other than v1.26, consultthe documentation for that version of Kubernetes. Note:The following StorageClass parameters from the built-in vsphereVolume plugin are not supported by the vSphere CSI driver:diskformathostfailurestotolerateforceprovisioningcachereservationdiskstripesobjectspacereservationiopslimitExisting volumes created using these parameters will be migrated to the vSphere CSI driver,but new volumes created by the vSphere CSI driver will not be honoring these parameters.",279
7.1 - Volumes,vSphere CSI migration complete,"vSphere CSI migration complete FEATURE STATE: Kubernetes v1.19 [beta] To turn off the vsphereVolume plugin from being loaded by the controller manager and the kubelet, you need to set InTreePluginvSphereUnregister feature flag to true. You must install a csi.vsphere.vmware.com CSI driver on all worker nodes.",80
7.1 - Volumes,Using subPath,"Using subPath Sometimes, it is useful to share one volume for multiple uses in a single pod.The volumeMounts.subPath property specifies a sub-path inside the referenced volumeinstead of its root. The following example shows how to configure a Pod with a LAMP stack (Linux Apache MySQL PHP)using a single, shared volume. This sample subPath configuration is not recommendedfor production use. The PHP application's code and assets map to the volume's html folder andthe MySQL database is stored in the volume's mysql folder. For example: apiVersion: v1kind: Podmetadata:  name: my-lamp-sitespec:    containers:    - name: mysql      image: mysql      env:      - name: MYSQL_ROOT_PASSWORD        value: ""rootpasswd""      volumeMounts:      - mountPath: /var/lib/mysql        name: site-data        subPath: mysql    - name: php      image: php:7.0-apache      volumeMounts:      - mountPath: /var/www/html        name: site-data        subPath: html    volumes:    - name: site-data      persistentVolumeClaim:        claimName: my-lamp-site-data",266
7.1 - Volumes,Using subPath with expanded environment variables,"Using subPath with expanded environment variables FEATURE STATE: Kubernetes v1.17 [stable] Use the subPathExpr field to construct subPath directory names fromdownward API environment variables.The subPath and subPathExpr properties are mutually exclusive. In this example, a Pod uses subPathExpr to create a directory pod1 withinthe hostPath volume /var/log/pods.The hostPath volume takes the Pod name from the downwardAPI.The host directory /var/log/pods/pod1 is mounted at /logs in the container. apiVersion: v1kind: Podmetadata:  name: pod1spec:  containers:  - name: container1    env:    - name: POD_NAME      valueFrom:        fieldRef:          apiVersion: v1          fieldPath: metadata.name    image: busybox:1.28    command: [ ""sh"", ""-c"", ""while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt"" ]    volumeMounts:    - name: workdir1      mountPath: /logs      # The variable expansion uses round brackets (not curly brackets).      subPathExpr: $(POD_NAME)  restartPolicy: Never  volumes:  - name: workdir1    hostPath:      path: /var/log/pods",299
7.1 - Volumes,Resources,"Resources The storage media (such as Disk or SSD) of an emptyDir volume is determined by themedium of the filesystem holding the kubelet root dir (typically/var/lib/kubelet). There is no limit on how much space an emptyDir orhostPath volume can consume, and no isolation between containers or betweenpods. To learn about requesting space using a resource specification, seehow to manage resources.",86
7.1 - Volumes,Out-of-tree volume plugins,"Out-of-tree volume plugins The out-of-tree volume plugins includeContainer Storage Interface (CSI), and also FlexVolume (which is deprecated). These plugins enable storage vendors to create custom storage pluginswithout adding their plugin source code to the Kubernetes repository. Previously, all volume plugins were ""in-tree"". The ""in-tree"" plugins were built, linked, compiled,and shipped with the core Kubernetes binaries. This meant that adding a new storage system toKubernetes (a volume plugin) required checking code into the core Kubernetes code repository. Both CSI and FlexVolume allow volume plugins to be developed independent ofthe Kubernetes code base, and deployed (installed) on Kubernetes clusters asextensions. For storage vendors looking to create an out-of-tree volume plugin, please referto the volume plugin FAQ.",183
7.1 - Volumes,csi,"csi Container Storage Interface(CSI) defines a standard interface for container orchestration systems (likeKubernetes) to expose arbitrary storage systems to their container workloads. Please read the CSI design proposal for more information. Note: Support for CSI spec versions 0.2 and 0.3 are deprecated in Kubernetesv1.13 and will be removed in a future release. Note: CSI drivers may not be compatible across all Kubernetes releases.Please check the specific CSI driver's documentation for supporteddeployments steps for each Kubernetes release and a compatibility matrix. Once a CSI compatible volume driver is deployed on a Kubernetes cluster, usersmay use the csi volume type to attach or mount the volumes exposed by theCSI driver. A csi volume can be used in a Pod in three different ways: through a reference to a PersistentVolumeClaimwith a generic ephemeral volumewith a CSI ephemeral volume if the driver supports that The following fields are available to storage administrators to configure a CSIpersistent volume: driver: A string value that specifies the name of the volume driver to use.This value must correspond to the value returned in the GetPluginInfoResponseby the CSI driver as defined in the CSI spec.It is used by Kubernetes to identify which CSI driver to call out to, and byCSI driver components to identify which PV objects belong to the CSI driver.volumeHandle: A string value that uniquely identifies the volume. This valuemust correspond to the value returned in the volume.id field of theCreateVolumeResponse by the CSI driver as defined in the CSI spec.The value is passed as volume_id on all calls to the CSI volume driver whenreferencing the volume.readOnly: An optional boolean value indicating whether the volume is to be""ControllerPublished"" (attached) as read only. Default is false. This value ispassed to the CSI driver via the readonly field in theControllerPublishVolumeRequest.fsType: If the PV's VolumeMode is Filesystem then this field may be usedto specify the filesystem that should be used to mount the volume. If thevolume has not been formatted and formatting is supported, this value will beused to format the volume.This value is passed to the CSI driver via the VolumeCapability field ofControllerPublishVolumeRequest, NodeStageVolumeRequest, andNodePublishVolumeRequest.volumeAttributes: A map of string to string that specifies static propertiesof a volume. This map must correspond to the map returned in thevolume.attributes field of the CreateVolumeResponse by the CSI driver asdefined in the CSI spec.The map is passed to the CSI driver via the volume_context field in theControllerPublishVolumeRequest, NodeStageVolumeRequest, andNodePublishVolumeRequest.controllerPublishSecretRef: A reference to the secret object containingsensitive information to pass to the CSI driver to complete the CSIControllerPublishVolume and ControllerUnpublishVolume calls. This field isoptional, and may be empty if no secret is required. If the Secretcontains more than one secret, all secrets are passed.nodeExpandSecretRef: A reference to the secret containing sensitiveinformation to pass to the CSI driver to complete the CSINodeExpandVolume call. This field is optional, and may be empty if nosecret is required. If the object contains more than one secret, allsecrets are passed. When you have configured secret data for node-initiatedvolume expansion, the kubelet passes that data via the NodeExpandVolume()call to the CSI driver. In order to use the nodeExpandSecretRef field, yourcluster should be running Kubernetes version 1.25 or later and you must enablethe feature gatenamed CSINodeExpandSecret for each kube-apiserver and for the kubelet on everynode. You must also be using a CSI driver that supports or requires secret data duringnode-initiated storage resize operations.nodePublishSecretRef: A reference to the secret object containingsensitive information to pass to the CSI driver to complete the CSINodePublishVolume call. This field is optional, and may be empty if nosecret is required. If the secret object contains more than one secret, allsecrets are passed.nodeStageSecretRef: A reference to the secret object containingsensitive information to pass to the CSI driver to complete the CSINodeStageVolume call. This field is optional, and may be empty if no secretis required. If the Secret contains more than one secret, all secretsare passed.",960
7.1 - Volumes,CSI raw block volume support,"CSI raw block volume support FEATURE STATE: Kubernetes v1.18 [stable] Vendors with external CSI drivers can implement raw block volume supportin Kubernetes workloads. You can set up yourPersistentVolume/PersistentVolumeClaim with raw block volume support as usual, without any CSI specific changes.",68
7.1 - Volumes,CSI ephemeral volumes,"CSI ephemeral volumes FEATURE STATE: Kubernetes v1.25 [stable] You can directly configure CSI volumes within the Podspecification. Volumes specified in this way are ephemeral and do notpersist across pod restarts. See EphemeralVolumesfor more information. For more information on how to develop a CSI driver, refer to thekubernetes-csi documentation",85
7.1 - Volumes,Windows CSI proxy,"Windows CSI proxy FEATURE STATE: Kubernetes v1.22 [stable] CSI node plugins need to perform various privilegedoperations like scanning of disk devices and mounting of file systems. These operationsdiffer for each host operating system. For Linux worker nodes, containerized CSI nodenode plugins are typically deployed as privileged containers. For Windows worker nodes,privileged operations for containerized CSI node plugins is supported usingcsi-proxy, a community-managed,stand-alone binary that needs to be pre-installed on each Windows node. For more details, refer to the deployment guide of the CSI plugin you wish to deploy.",129
7.1 - Volumes,Migrating to CSI drivers from in-tree plugins,"Migrating to CSI drivers from in-tree plugins FEATURE STATE: Kubernetes v1.25 [stable] The CSIMigration feature directs operations against existing in-treeplugins to corresponding CSI plugins (which are expected to be installed and configured).As a result, operators do not have to make anyconfiguration changes to existing Storage Classes, PersistentVolumes or PersistentVolumeClaims(referring to in-tree plugins) when transitioning to a CSI driver that supersedes an in-tree plugin. The operations and features that are supported include:provisioning/delete, attach/detach, mount/unmount and resizing of volumes. In-tree plugins that support CSIMigration and have a corresponding CSI driver implementedare listed in Types of Volumes. The following in-tree plugins support persistent storage on Windows nodes: awsElasticBlockStoreazureDiskazureFilegcePersistentDiskvsphereVolume",196
7.1 - Volumes,flexVolume (deprecated),"flexVolume (deprecated) FEATURE STATE: Kubernetes v1.23 [deprecated] FlexVolume is an out-of-tree plugin interface that uses an exec-based model to interfacewith storage drivers. The FlexVolume driver binaries must be installed in a pre-definedvolume plugin path on each node and in some cases the control plane nodes as well. Pods interact with FlexVolume drivers through the flexVolume in-tree volume plugin.For more details, see the FlexVolume README document. The following FlexVolume plugins,deployed as PowerShell scripts on the host, support Windows nodes: SMBiSCSI Note:FlexVolume is deprecated. Using an out-of-tree CSI driver is the recommended way to integrate external storage with Kubernetes.Maintainers of FlexVolume driver should implement a CSI Driver and help to migrate users of FlexVolume drivers to CSI.Users of FlexVolume should move their workloads to use the equivalent CSI Driver.",202
7.1 - Volumes,Mount propagation,"Mount propagation Mount propagation allows for sharing volumes mounted by a container toother containers in the same pod, or even to other pods on the same node. Mount propagation of a volume is controlled by the mountPropagation fieldin Container.volumeMounts. Its values are: None - This volume mount will not receive any subsequent mountsthat are mounted to this volume or any of its subdirectories by the host.In similar fashion, no mounts created by the container will be visible onthe host. This is the default mode.This mode is equal to rprivate mount propagation as described inmount(8)However, the CRI runtime may choose rslave mount propagation (i.e.,HostToContainer) instead, when rprivate propagation is not applicable.cri-dockerd (Docker) is known to choose rslave mount propagation when themount source contains the Docker daemon's root directory (/var/lib/docker).HostToContainer - This volume mount will receive all subsequent mountsthat are mounted to this volume or any of its subdirectories.In other words, if the host mounts anything inside the volume mount, thecontainer will see it mounted there.Similarly, if any Pod with Bidirectional mount propagation to the samevolume mounts anything there, the container with HostToContainer mountpropagation will see it.This mode is equal to rslave mount propagation as described in themount(8)Bidirectional - This volume mount behaves the same the HostToContainer mount.In addition, all volume mounts created by the container will be propagatedback to the host and to all containers of all pods that use the same volume.A typical use case for this mode is a Pod with a FlexVolume or CSI driver ora Pod that needs to mount something on the host using a hostPath volume.This mode is equal to rshared mount propagation as described in themount(8)Warning: Bidirectional mount propagation can be dangerous. It can damagethe host operating system and therefore it is allowed only in privilegedcontainers. Familiarity with Linux kernel behavior is strongly recommended.In addition, any volume mounts created by containers in pods must be destroyed(unmounted) by the containers on termination.",452
7.1 - Volumes,Configuration,"Configuration Before mount propagation can work properly on some deployments (CoreOS,RedHat/Centos, Ubuntu) mount share must be configured correctly inDocker as shown below. Edit your Docker's systemd service file. Set MountFlags as follows: MountFlags=shared Or, remove MountFlags=slave if present. Then restart the Docker daemon: sudo systemctl daemon-reloadsudo systemctl restart docker Follow an example of deploying WordPress and MySQL with Persistent Volumes.",95
7.2 - Persistent Volumes,Introduction,"Introduction Managing storage is a distinct problem from managing compute instances.The PersistentVolume subsystem provides an API for users and administratorsthat abstracts details of how storage is provided from how it is consumed.To do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim. A PersistentVolume (PV) is a piece of storage in the cluster that has beenprovisioned by an administrator or dynamically provisioned usingStorage Classes. It is a resource inthe cluster just like a node is a cluster resource. PVs are volume plugins likeVolumes, but have a lifecycle independent of any individual Pod that uses the PV.This API object captures the details of the implementation of the storage, be thatNFS, iSCSI, or a cloud-provider-specific storage system. A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similarto a Pod. Pods consume node resources and PVCs consume PV resources. Pods canrequest specific levels of resources (CPU and Memory). Claims can request specificsize and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany orReadWriteMany, see AccessModes). While PersistentVolumeClaims allow a user to consume abstract storage resources,it is common that users need PersistentVolumes with varying properties, such asperformance, for different problems. Cluster administrators need to be able tooffer a variety of PersistentVolumes that differ in more ways than size and accessmodes, without exposing users to the details of how those volumes are implemented.For these needs, there is the StorageClass resource. See the detailed walkthrough with working examples.",349
7.2 - Persistent Volumes,Lifecycle of a volume and claim,Lifecycle of a volume and claim PVs are resources in the cluster. PVCs are requests for those resources and also actas claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:,46
7.2 - Persistent Volumes,Static,"Static A cluster administrator creates a number of PVs. They carry the details of thereal storage, which is available for use by cluster users. They exist in theKubernetes API and are available for consumption.",44
7.2 - Persistent Volumes,Dynamic,"Dynamic When none of the static PVs the administrator created match a user's PersistentVolumeClaim,the cluster may try to dynamically provision a volume specially for the PVC.This provisioning is based on StorageClasses: the PVC must request astorage class andthe administrator must have created and configured that class for dynamicprovisioning to occur. Claims that request the class """" effectively disabledynamic provisioning for themselves. To enable dynamic storage provisioning based on storage class, the cluster administratorneeds to enable the DefaultStorageClassadmission controlleron the API server. This can be done, for example, by ensuring that DefaultStorageClass isamong the comma-delimited, ordered list of values for the --enable-admission-plugins flag ofthe API server component. For more information on API server command-line flags,check kube-apiserver documentation.",175
7.2 - Persistent Volumes,Binding,"Binding A user creates, or in the case of dynamic provisioning, has already created,a PersistentVolumeClaim with a specific amount of storage requested and withcertain access modes. A control loop in the master watches for new PVCs, findsa matching PV (if possible), and binds them together. If a PV was dynamicallyprovisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise,the user will always get at least what they asked for, but the volume may be inexcess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive,regardless of how they were bound. A PVC to PV binding is a one-to-one mapping,using a ClaimRef which is a bi-directional binding between the PersistentVolumeand the PersistentVolumeClaim. Claims will remain unbound indefinitely if a matching volume does not exist.Claims will be bound as matching volumes become available. For example, acluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi.The PVC can be bound when a 100Gi PV is added to the cluster.",233
7.2 - Persistent Volumes,Using,"Using Pods use claims as volumes. The cluster inspects the claim to find the boundvolume and mounts that volume for a Pod. For volumes that support multipleaccess modes, the user specifies which mode is desired when using their claimas a volume in a Pod. Once a user has a claim and that claim is bound, the bound PV belongs to theuser for as long as they need it. Users schedule Pods and access their claimedPVs by including a persistentVolumeClaim section in a Pod's volumes block.See Claims As Volumes for more details on this.",115
7.2 - Persistent Volumes,Storage Object in Use Protection,"Storage Object in Use Protection The purpose of the Storage Object in Use Protection feature is to ensure thatPersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs)that are bound to PVCs are not removed from the system, as this may result in data loss. Note: PVC is in active use by a Pod when a Pod object exists that is using the PVC. If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately.PVC removal is postponed until the PVC is no longer actively used by any Pods. Also,if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately.PV removal is postponed until the PV is no longer bound to a PVC. You can see that a PVC is protected when the PVC's status is Terminating and theFinalizers list includes kubernetes.io/pvc-protection: kubectl describe pvc hostpathName:          hostpathNamespace:     defaultStorageClass:  example-hostpathStatus:        TerminatingVolume:Labels:        <none>Annotations:   volume.beta.kubernetes.io/storage-class=example-hostpath               volume.beta.kubernetes.io/storage-provisioner=example.com/hostpathFinalizers:    [kubernetes.io/pvc-protection]... You can see that a PV is protected when the PV's status is Terminating andthe Finalizers list includes kubernetes.io/pv-protection too: kubectl describe pv task-pv-volumeName:            task-pv-volumeLabels:          type=localAnnotations:     <none>Finalizers:      [kubernetes.io/pv-protection]StorageClass:    standardStatus:          TerminatingClaim:Reclaim Policy:  DeleteAccess Modes:    RWOCapacity:        1GiMessage:Source:    Type:          HostPath (bare host directory volume)    Path:          /tmp/data    HostPathType:Events:            <none>",456
7.2 - Persistent Volumes,Reclaiming,"Reclaiming When a user is done with their volume, they can delete the PVC objects from theAPI that allows reclamation of the resource. The reclaim policy for a PersistentVolumetells the cluster what to do with the volume after it has been released of its claim.Currently, volumes can either be Retained, Recycled, or Deleted.",74
7.2 - Persistent Volumes,Retain,"Retain The Retain reclaim policy allows for manual reclamation of the resource.When the PersistentVolumeClaim is deleted, the PersistentVolume still existsand the volume is considered ""released"". But it is not yet available foranother claim because the previous claimant's data remains on the volume.An administrator can manually reclaim the volume with the following steps. Delete the PersistentVolume. The associated storage asset in external infrastructure(such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume) still exists after the PV is deleted.Manually clean up the data on the associated storage asset accordingly.Manually delete the associated storage asset. If you want to reuse the same storage asset, create a new PersistentVolume withthe same storage asset definition.",156
7.2 - Persistent Volumes,Delete,"Delete For volume plugins that support the Delete reclaim policy, deletion removesboth the PersistentVolume object from Kubernetes, as well as the associatedstorage asset in the external infrastructure, such as an AWS EBS, GCE PD,Azure Disk, or Cinder volume. Volumes that were dynamically provisionedinherit the reclaim policy of their StorageClass, whichdefaults to Delete. The administrator should configure the StorageClassaccording to users' expectations; otherwise, the PV must be edited orpatched after it is created. SeeChange the Reclaim Policy of a PersistentVolume.",120
7.2 - Persistent Volumes,Recycle,"Recycle Warning: The Recycle reclaim policy is deprecated. Instead, the recommended approachis to use dynamic provisioning. If supported by the underlying volume plugin, the Recycle reclaim policy performsa basic scrub (rm -rf /thevolume/*) on the volume and makes it available again for a new claim. However, an administrator can configure a custom recycler Pod template usingthe Kubernetes controller manager command line arguments as described in thereference.The custom recycler Pod template must contain a volumes specification, asshown in the example below: apiVersion: v1kind: Podmetadata:  name: pv-recycler  namespace: defaultspec:  restartPolicy: Never  volumes:  - name: vol    hostPath:      path: /any/path/it/will/be/replaced  containers:  - name: pv-recycler    image: ""registry.k8s.io/busybox""    command: [""/bin/sh"", ""-c"", ""test -e /scrub && rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  && test -z \""$(ls -A /scrub)\"" || exit 1""]    volumeMounts:    - name: vol      mountPath: /scrub However, the particular path specified in the custom recycler Pod template in thevolumes part is replaced with the particular path of the volume that is being recycled.",311
7.2 - Persistent Volumes,PersistentVolume deletion protection finalizer,"PersistentVolume deletion protection finalizer FEATURE STATE: Kubernetes v1.23 [alpha] Finalizers can be added on a PersistentVolume to ensure that PersistentVolumeshaving Delete reclaim policy are deleted only after the backing storage are deleted. The newly introduced finalizers kubernetes.io/pv-controller andexternal-provisioner.volume.kubernetes.io/finalizerare only added to dynamically provisioned volumes. The finalizer kubernetes.io/pv-controller is added to in-tree plugin volumes.The following is an example kubectl describe pv pvc-74a498d6-3929-47e8-8c02-078c1ece4d78Name:            pvc-74a498d6-3929-47e8-8c02-078c1ece4d78Labels:          <none>Annotations:     kubernetes.io/createdby: vsphere-volume-dynamic-provisioner                 pv.kubernetes.io/bound-by-controller: yes                 pv.kubernetes.io/provisioned-by: kubernetes.io/vsphere-volumeFinalizers:      [kubernetes.io/pv-protection kubernetes.io/pv-controller]StorageClass:    vcp-scStatus:          BoundClaim:           default/vcp-pvc-1Reclaim Policy:  DeleteAccess Modes:    RWOVolumeMode:      FilesystemCapacity:        1GiNode Affinity:   <none>Message:         Source:    Type:               vSphereVolume (a Persistent Disk resource in vSphere)    VolumePath:         [vsanDatastore] d49c4a62-166f-ce12-c464-020077ba5d46/kubernetes-dynamic-pvc-74a498d6-3929-47e8-8c02-078c1ece4d78.vmdk    FSType:             ext4    StoragePolicyName:  vSAN Default Storage PolicyEvents:                 <none> The finalizer external-provisioner.volume.kubernetes.io/finalizer is added for CSI volumes.The following is an example: Name:            pvc-2f0bab97-85a8-4552-8044-eb8be45cf48dLabels:          <none>Annotations:     pv.kubernetes.io/provisioned-by: csi.vsphere.vmware.comFinalizers:      [kubernetes.io/pv-protection external-provisioner.volume.kubernetes.io/finalizer]StorageClass:    fastStatus:          BoundClaim:           demo-app/nginx-logsReclaim Policy:  DeleteAccess Modes:    RWOVolumeMode:      FilesystemCapacity:        200MiNode Affinity:   <none>Message:         Source:    Type:              CSI (a Container Storage Interface (CSI) volume source)    Driver:            csi.vsphere.vmware.com    FSType:            ext4    VolumeHandle:      44830fa8-79b4-406b-8b58-621ba25353fd    ReadOnly:          false    VolumeAttributes:      storage.kubernetes.io/csiProvisionerIdentity=1648442357185-8081-csi.vsphere.vmware.com                           type=vSphere CNS Block VolumeEvents:                <none> When the CSIMigration{provider} feature flag is enabled for a specific in-tree volume plugin,the kubernetes.io/pv-controller finalizer is replaced by theexternal-provisioner.volume.kubernetes.io/finalizer finalizer.",865
7.2 - Persistent Volumes,Reserving a PersistentVolume,"Reserving a PersistentVolume The control plane can bind PersistentVolumeClaims to matching PersistentVolumesin the cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them. By specifying a PersistentVolume in a PersistentVolumeClaim, you declare a bindingbetween that specific PV and PVC. If the PersistentVolume exists and has not reservedPersistentVolumeClaims through its claimRef field, then the PersistentVolume andPersistentVolumeClaim will be bound. The binding happens regardless of some volume matching criteria, including node affinity.The control plane still checks that storage class,access modes, and requested storage size are valid. apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: foo-pvc  namespace: foospec:  storageClassName: """" # Empty string must be explicitly set otherwise default StorageClass will be set  volumeName: foo-pv  ... This method does not guarantee any binding privileges to the PersistentVolume.If other PersistentVolumeClaims could use the PV that you specify, you firstneed to reserve that storage volume. Specify the relevant PersistentVolumeClaimin the claimRef field of the PV so that other PVCs can not bind to it. apiVersion: v1kind: PersistentVolumemetadata:  name: foo-pvspec:  storageClassName: """"  claimRef:    name: foo-pvc    namespace: foo  ... This is useful if you want to consume PersistentVolumes that have their claimPolicy setto Retain, including cases where you are reusing an existing PV.",341
7.2 - Persistent Volumes,Expanding Persistent Volumes Claims,"Expanding Persistent Volumes Claims FEATURE STATE: Kubernetes v1.24 [stable] Support for expanding PersistentVolumeClaims (PVCs) is enabled by default. You can expandthe following types of volumes: azureDiskazureFileawsElasticBlockStorecinder (deprecated)csiflexVolume (deprecated)gcePersistentDiskrbdportworxVolume You can only expand a PVC if its storage class's allowVolumeExpansion field is set to true. apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: example-vol-defaultprovisioner: vendor-name.example/magicstorageparameters:  resturl: ""http://192.168.10.100:8080""  restuser: """"  secretNamespace: """"  secretName: """"allowVolumeExpansion: true To request a larger volume for a PVC, edit the PVC object and specify a largersize. This triggers expansion of the volume that backs the underlying PersistentVolume. Anew PersistentVolume is never created to satisfy the claim. Instead, an existing volume is resized. Warning: Directly editing the size of a PersistentVolume can prevent an automatic resize of that volume.If you edit the capacity of a PersistentVolume, and then edit the .spec of a matchingPersistentVolumeClaim to make the size of the PersistentVolumeClaim match the PersistentVolume,then no storage resize happens.The Kubernetes control plane will see that the desired state of both resources matches,conclude that the backing volume size has been manuallyincreased and that no resize is necessary.",349
7.2 - Persistent Volumes,CSI Volume expansion,CSI Volume expansion FEATURE STATE: Kubernetes v1.24 [stable] Support for expanding CSI volumes is enabled by default but it also requires aspecific CSI driver to support volume expansion. Refer to documentation of thespecific CSI driver for more information.,54
7.2 - Persistent Volumes,Resizing a volume containing a file system,"Resizing a volume containing a file system You can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4. When a volume contains a file system, the file system is only resized when a new Pod is usingthe PersistentVolumeClaim in ReadWrite mode. File system expansion is either done when a Pod is starting upor when a Pod is running and the underlying file system supports online expansion. FlexVolumes (deprecated since Kubernetes v1.23) allow resize if the driver is configured with theRequiresFSResize capability to true. The FlexVolume can be resized on Pod restart.",134
7.2 - Persistent Volumes,Resizing an in-use PersistentVolumeClaim,"Resizing an in-use PersistentVolumeClaim FEATURE STATE: Kubernetes v1.24 [stable] In this case, you don't need to delete and recreate a Pod or deployment that is using an existing PVC.Any in-use PVC automatically becomes available to its Pod as soon as its file system has been expanded.This feature has no effect on PVCs that are not in use by a Pod or deployment. You must create a Pod thatuses the PVC before the expansion can complete. Similar to other volume types - FlexVolume volumes can also be expanded when in-use by a Pod. Note: FlexVolume resize is possible only when the underlying driver supports resize. Note: Expanding EBS volumes is a time-consuming operation.Also, there is a per-volume quota of one modification every 6 hours.",169
7.2 - Persistent Volumes,Recovering from Failure when Expanding Volumes,"Recovering from Failure when Expanding Volumes If a user specifies a new size that is too big to be satisfied by underlyingstorage system, expansion of PVC will be continuously retried until user orcluster administrator takes some action. This can be undesirable and henceKubernetes provides following methods of recovering from such failures. Manually with Cluster Administrator accessBy requesting expansion to smaller size If expanding underlying storage fails, the cluster administrator can manuallyrecover the Persistent Volume Claim (PVC) state and cancel the resize requests.Otherwise, the resize requests are continuously retried by the controller withoutadministrator intervention.Mark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC)with Retain reclaim policy.Delete the PVC. Since PV has Retain reclaim policy - we will not lose any datawhen we recreate the PVC.Delete the claimRef entry from PV specs, so as new PVC can bind to it.This should make the PV Available.Re-create the PVC with smaller size than PV and set volumeName field of thePVC to the name of the PV. This should bind new PVC to existing PV.Don't forget to restore the reclaim policy of the PV.FEATURE STATE: Kubernetes v1.23 [alpha]Note: Recovery from failing PVC expansion by users is available as an alpha featuresince Kubernetes 1.23. The RecoverVolumeExpansionFailure feature must beenabled for this feature to work. Refer to thefeature gatedocumentation for more information.If the feature gates RecoverVolumeExpansionFailure isenabled in your cluster, and expansion has failed for a PVC, you can retry expansion with asmaller size than the previously requested value. To request a new expansion attempt with asmaller proposed size, edit .spec.resources for that PVC and choose a value that is less than thevalue you previously tried.This is useful if expansion to a higher value did not succeed because of capacity constraint.If that has happened, or you suspect that it might have, you can retry expansion by specifying asize that is within the capacity limits of underlying storage provider. You can monitor status ofresize operation by watching .status.resizeStatus and events on the PVC.Note that,although you can specify a lower amount of storage than what was requested previously,the new value must still be higher than .status.capacity.Kubernetes does not support shrinking a PVC to less than its current size.",512
7.2 - Persistent Volumes,Types of Persistent Volumes,Types of Persistent Volumes PersistentVolume types are implemented as plugins. Kubernetes currently supports the following plugins: cephfs - CephFS volumecsi - Container Storage Interface (CSI)fc - Fibre Channel (FC) storagehostPath - HostPath volume(for single node testing only; WILL NOT WORK in a multi-node cluster;consider using local volume instead)iscsi - iSCSI (SCSI over IP) storagelocal - local storage devicesmounted on nodes.nfs - Network File System (NFS) storagerbd - Rados Block Device (RBD) volume The following types of PersistentVolume are deprecated.This means that support is still available but will be removed in a future Kubernetes release. awsElasticBlockStore - AWS Elastic Block Store (EBS)(deprecated in v1.17)azureDisk - Azure Disk(deprecated in v1.19)azureFile - Azure File(deprecated in v1.21)cinder - Cinder (OpenStack block storage)(deprecated in v1.18)flexVolume - FlexVolume(deprecated in v1.23)gcePersistentDisk - GCE Persistent Disk(deprecated in v1.17)portworxVolume - Portworx volume(deprecated in v1.25)vsphereVolume - vSphere VMDK volume(deprecated in v1.19) Older versions of Kubernetes also supported the following in-tree PersistentVolume types: photonPersistentDisk - Photon controller persistent disk.(not available starting v1.15)scaleIO - ScaleIO volume(not available starting v1.21)flocker - Flocker storage(not available starting v1.25)quobyte - Quobyte volume(not available starting v1.25)storageos - StorageOS volume(not available starting v1.25),407
7.2 - Persistent Volumes,Persistent Volumes,"Persistent Volumes Each PV contains a spec and status, which is the specification and status of the volume.The name of a PersistentVolume object must be a validDNS subdomain name. apiVersion: v1kind: PersistentVolumemetadata:  name: pv0003spec:  capacity:    storage: 5Gi  volumeMode: Filesystem  accessModes:    - ReadWriteOnce  persistentVolumeReclaimPolicy: Recycle  storageClassName: slow  mountOptions:    - hard    - nfsvers=4.1  nfs:    path: /tmp    server: 172.17.0.2 Note: Helper programs relating to the volume type may be required for consumption ofa PersistentVolume within a cluster. In this example, the PersistentVolume isof type NFS and the helper program /sbin/mount.nfs is required to support themounting of NFS filesystems.",199
7.2 - Persistent Volumes,Capacity,"Capacity Generally, a PV will have a specific storage capacity. This is set using the PV'scapacity attribute. Read the glossary termQuantity to understand the unitsexpected by capacity. Currently, storage size is the only resource that can be set or requested.Future attributes may include IOPS, throughput, etc.",64
7.2 - Persistent Volumes,Volume Mode,"Volume Mode FEATURE STATE: Kubernetes v1.18 [stable] Kubernetes supports two volumeModes of PersistentVolumes: Filesystem and Block. volumeMode is an optional API parameter.Filesystem is the default mode used when volumeMode parameter is omitted. A volume with volumeMode: Filesystem is mounted into Pods into a directory. If the volumeis backed by a block device and the device is empty, Kubernetes creates a filesystemon the device before mounting it for the first time. You can set the value of volumeMode to Block to use a volume as a raw block device.Such volume is presented into a Pod as a block device, without any filesystem on it.This mode is useful to provide a Pod the fastest possible way to access a volume, withoutany filesystem layer between the Pod and the volume. On the other hand, the applicationrunning in the Pod must know how to handle a raw block device.See Raw Block Volume Supportfor an example on how to use a volume with volumeMode: Block in a Pod.",219
7.2 - Persistent Volumes,Access Modes,"Access Modes A PersistentVolume can be mounted on a host in any way supported by the resourceprovider. As shown in the table below, providers will have different capabilitiesand each PV's access modes are set to the specific modes supported by that particularvolume. For example, NFS can support multiple read/write clients, but a specificNFS PV might be exported on the server as read-only. Each PV gets its own set ofaccess modes describing that specific PV's capabilities. The access modes are: ReadWriteOncethe volume can be mounted as read-write by a single node. ReadWriteOnce accessmode still can allow multiple pods to access the volume when the pods are running on the same node.ReadOnlyManythe volume can be mounted as read-only by many nodes.ReadWriteManythe volume can be mounted as read-write by many nodes.ReadWriteOncePodthe volume can be mounted as read-write by a single Pod. Use ReadWriteOncePodaccess mode if you want to ensure that only one pod across whole cluster canread that PVC or write to it. This is only supported for CSI volumes andKubernetes version 1.22+. The blog articleIntroducing Single Pod Access Mode for PersistentVolumescovers this in more detail. In the CLI, the access modes are abbreviated to: RWO - ReadWriteOnceROX - ReadOnlyManyRWX - ReadWriteManyRWOP - ReadWriteOncePod Note: Kubernetes uses volume access modes to match PersistentVolumeClaims and PersistentVolumes.In some cases, the volume access modes also constrain where the PersistentVolume can be mounted.Volume access modes do not enforce write protection once the storage has been mounted.Even if the access modes are specified as ReadWriteOnce, ReadOnlyMany, or ReadWriteMany,they don't set any constraints on the volume. For example, even if a PersistentVolume iscreated as ReadOnlyMany, it is no guarantee that it will be read-only. If the access modesare specified as ReadWriteOncePod, the volume is constrained and can be mounted on only a single Pod. Important! A volume can only be mounted using one access mode at a time,even if it supports many. For example, a GCEPersistentDisk can be mounted asReadWriteOnce by a single node or ReadOnlyMany by many nodes, but not at the same time. Volume PluginReadWriteOnceReadOnlyManyReadWriteManyReadWriteOncePodAWSElasticBlockStore✓---AzureFile✓✓✓-AzureDisk✓---CephFS✓✓✓-Cinder✓-(if multi-attach volumes are available)-CSIdepends on the driverdepends on the driverdepends on the driverdepends on the driverFC✓✓--FlexVolume✓✓depends on the driver-GCEPersistentDisk✓✓--Glusterfs✓✓✓-HostPath✓---iSCSI✓✓--NFS✓✓✓-RBD✓✓--VsphereVolume✓-- (works when Pods are collocated)-PortworxVolume✓-✓- Access Modes Claims use the same conventions as volumes when requestingstorage with specific access modes.",695
7.2 - Persistent Volumes,Class,"Class A PV can have a class, which is specified by setting thestorageClassName attribute to the name of aStorageClass.A PV of a particular class can only be bound to PVCs requestingthat class. A PV with no storageClassName has no class and can only be boundto PVCs that request no particular class. In the past, the annotation volume.beta.kubernetes.io/storage-class was used insteadof the storageClassName attribute. This annotation is still working; however,it will become fully deprecated in a future Kubernetes release. Class A claim can request a particular class by specifying the name of aStorageClassusing the attribute storageClassName.Only PVs of the requested class, ones with the same storageClassName as the PVC, canbe bound to the PVC. PVCs don't necessarily have to request a class. A PVC with its storageClassName setequal to """" is always interpreted to be requesting a PV with no class, so itcan only be bound to PVs with no class (no annotation or one set equal to""""). A PVC with no storageClassName is not quite the same and is treated differentlyby the cluster, depending on whether theDefaultStorageClass admission pluginis turned on. If the admission plugin is turned on, the administrator may specify adefault StorageClass. All PVCs that have no storageClassName can be bound only toPVs of that default. Specifying a default StorageClass is done by setting theannotation storageclass.kubernetes.io/is-default-class equal to true ina StorageClass object. If the administrator does not specify a default, thecluster responds to PVC creation as if the admission plugin were turned off. Ifmore than one default is specified, the admission plugin forbids the creation ofall PVCs.If the admission plugin is turned off, there is no notion of a defaultStorageClass. All PVCs that have storageClassName set to """" can bebound only to PVs that have storageClassName also set to """".However, PVCs with missing storageClassName can be updated later oncedefault StorageClass becomes available. If the PVC gets updated it will nolonger bind to PVs that have storageClassName also set to """". See retroactive default StorageClass assignment for more details. Depending on installation method, a default StorageClass may be deployedto a Kubernetes cluster by addon manager during installation. When a PVC specifies a selector in addition to requesting a StorageClass,the requirements are ANDed together: only a PV of the requested class and withthe requested labels may be bound to the PVC. Note: Currently, a PVC with a non-empty selector can't have a PV dynamically provisioned for it. In the past, the annotation volume.beta.kubernetes.io/storage-class was used insteadof storageClassName attribute. This annotation is still working; however,it won't be supported in a future Kubernetes release.",619
7.2 - Persistent Volumes,Reclaim Policy,"Reclaim Policy Current reclaim policies are: Retain -- manual reclamationRecycle -- basic scrub (rm -rf /thevolume/*)Delete -- associated storage asset such as AWS EBS, GCE PD, Azure Disk,or OpenStack Cinder volume is deleted Currently, only NFS and HostPath support recycling. AWS EBS, GCE PD, Azure Disk,and Cinder volumes support deletion.",83
7.2 - Persistent Volumes,Mount Options,"Mount Options A Kubernetes administrator can specify additional mount options for when aPersistent Volume is mounted on a node. Note: Not all Persistent Volume types support mount options. The following volume types support mount options: awsElasticBlockStoreazureDiskazureFilecephfscinder (deprecated in v1.18)gcePersistentDiskiscsinfsrbdvsphereVolume Mount options are not validated. If a mount option is invalid, the mount fails. In the past, the annotation volume.beta.kubernetes.io/mount-options was used insteadof the mountOptions attribute. This annotation is still working; however,it will become fully deprecated in a future Kubernetes release.",153
7.2 - Persistent Volumes,Node Affinity,"Node Affinity Note: For most volume types, you do not need to set this field. It is automaticallypopulated for AWS EBS,GCE PD andAzure Disk volume block types. Youneed to explicitly set this for local volumes. A PV can specify node affinity to define constraints that limit what nodes thisvolume can be accessed from. Pods that use a PV will only be scheduled to nodesthat are selected by the node affinity. To specify node affinity, setnodeAffinity in the .spec of a PV. ThePersistentVolumeAPI reference has more details on this field.",121
7.2 - Persistent Volumes,Phase,"Phase A volume will be in one of the following phases: Available -- a free resource that is not yet bound to a claimBound -- the volume is bound to a claimReleased -- the claim has been deleted, but the resource is not yet reclaimed by the clusterFailed -- the volume has failed its automatic reclamation The CLI will show the name of the PVC bound to the PV.",77
7.2 - Persistent Volumes,PersistentVolumeClaims,"PersistentVolumeClaims Each PVC contains a spec and status, which is the specification and status of the claim.The name of a PersistentVolumeClaim object must be a validDNS subdomain name. apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: myclaimspec:  accessModes:    - ReadWriteOnce  volumeMode: Filesystem  resources:    requests:      storage: 8Gi  storageClassName: slow  selector:    matchLabels:      release: ""stable""    matchExpressions:      - {key: environment, operator: In, values: [dev]}",130
7.2 - Persistent Volumes,Selector,"Selector Claims can specify alabel selectorto further filter the set of volumes. Only the volumes whose labels match the selectorcan be bound to the claim. The selector can consist of two fields: matchLabels - the volume must have a label with this valuematchExpressions - a list of requirements made by specifying key, list of values,and operator that relates the key and values. Valid operators include In, NotIn,Exists, and DoesNotExist. All of the requirements, from both matchLabels and matchExpressions, areANDed together – they must all be satisfied in order to match.",128
7.2 - Persistent Volumes,Retroactive default StorageClass assignment,"Retroactive default StorageClass assignment FEATURE STATE: Kubernetes v1.26 [beta] You can create a PersistentVolumeClaim without specifying a storageClassNamefor the new PVC, and you can do so even when no default StorageClass existsin your cluster. In this case, the new PVC creates as you defined it, and thestorageClassName of that PVC remains unset until default becomes available. When a default StorageClass becomes available, the control plane identifies anyexisting PVCs without storageClassName. For the PVCs that either have an emptyvalue for storageClassName or do not have this key, the control plane thenupdates those PVCs to set storageClassName to match the new default StorageClass.If you have an existing PVC where the storageClassName is """", and you configurea default StorageClass, then this PVC will not get updated. In order to keep binding to PVs with storageClassName set to """"(while a default StorageClass is present), you need to set the storageClassNameof the associated PVC to """". This behavior helps administrators change default StorageClass by removing theold one first and then creating or setting another one. This brief window whilethere is no default causes PVCs without storageClassName created at that timeto not have any default, but due to the retroactive default StorageClassassignment this way of changing defaults is safe.",285
7.2 - Persistent Volumes,Claims As Volumes,"Claims As Volumes Pods access storage by using the claim as a volume. Claims must exist in thesame namespace as the Pod using the claim. The cluster finds the claim in thePod's namespace and uses it to get the PersistentVolume backing the claim.The volume is then mounted to the host and into the Pod. apiVersion: v1kind: Podmetadata:  name: mypodspec:  containers:    - name: myfrontend      image: nginx      volumeMounts:      - mountPath: ""/var/www/html""        name: mypd  volumes:    - name: mypd      persistentVolumeClaim:        claimName: myclaim",141
7.2 - Persistent Volumes,A Note on Namespaces,"A Note on Namespaces PersistentVolumes binds are exclusive, and since PersistentVolumeClaims arenamespaced objects, mounting claims with ""Many"" modes (ROX, RWX) is onlypossible within one namespace.",47
7.2 - Persistent Volumes,PersistentVolumes typed hostPath,PersistentVolumes typed hostPath A hostPath PersistentVolume uses a file or directory on the Node to emulatenetwork-attached storage. Seean example of hostPath typed volume.,41
7.2 - Persistent Volumes,Raw Block Volume Support,"Raw Block Volume Support FEATURE STATE: Kubernetes v1.18 [stable] The following volume plugins support raw block volumes, including dynamic provisioning whereapplicable: AWSElasticBlockStoreAzureDiskCSIFC (Fibre Channel)GCEPersistentDiskiSCSILocal volumeOpenStack CinderRBD (Ceph Block Device)VsphereVolume",82
7.2 - Persistent Volumes,PersistentVolume using a Raw Block Volume,"PersistentVolume using a Raw Block Volume apiVersion: v1kind: PersistentVolumemetadata:  name: block-pvspec:  capacity:    storage: 10Gi  accessModes:    - ReadWriteOnce  volumeMode: Block  persistentVolumeReclaimPolicy: Retain  fc:    targetWWNs: [""50060e801049cfd1""]    lun: 0    readOnly: false",92
7.2 - Persistent Volumes,PersistentVolumeClaim requesting a Raw Block Volume,PersistentVolumeClaim requesting a Raw Block Volume apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: block-pvcspec:  accessModes:    - ReadWriteOnce  volumeMode: Block  resources:    requests:      storage: 10Gi,57
7.2 - Persistent Volumes,Pod specification adding Raw Block Device path in container,"Pod specification adding Raw Block Device path in container apiVersion: v1kind: Podmetadata:  name: pod-with-block-volumespec:  containers:    - name: fc-container      image: fedora:26      command: [""/bin/sh"", ""-c""]      args: [ ""tail -f /dev/null"" ]      volumeDevices:        - name: data          devicePath: /dev/xvda  volumes:    - name: data      persistentVolumeClaim:        claimName: block-pvc Note: When adding a raw block device for a Pod, you specify the device path in thecontainer instead of a mount path.",143
7.2 - Persistent Volumes,Binding Block Volumes,"Binding Block Volumes If a user requests a raw block volume by indicating this using the volumeModefield in the PersistentVolumeClaim spec, the binding rules differ slightly fromprevious releases that didn't consider this mode as part of the spec.Listed is a table of possible combinations the user and admin might specify forrequesting a raw block device. The table indicates if the volume will be bound ornot given the combinations: Volume binding matrix for statically provisioned volumes: PV volumeModePVC volumeModeResultunspecifiedunspecifiedBINDunspecifiedBlockNO BINDunspecifiedFilesystemBINDBlockunspecifiedNO BINDBlockBlockBINDBlockFilesystemNO BINDFilesystemFilesystemBINDFilesystemBlockNO BINDFilesystemunspecifiedBIND Note: Only statically provisioned volumes are supported for alpha release. Administratorsshould take care to consider these values when working with raw block devices.",189
7.2 - Persistent Volumes,Volume Snapshot and Restore Volume from Snapshot Support,"Volume Snapshot and Restore Volume from Snapshot Support FEATURE STATE: Kubernetes v1.20 [stable] Volume snapshots only support the out-of-tree CSI volume plugins.For details, see Volume Snapshots.In-tree volume plugins are deprecated. You can read about the deprecated volumeplugins in theVolume Plugin FAQ.",69
7.2 - Persistent Volumes,Create a PersistentVolumeClaim from a Volume Snapshot,Create a PersistentVolumeClaim from a Volume Snapshot apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: restore-pvcspec:  storageClassName: csi-hostpath-sc  dataSource:    name: new-snapshot-test    kind: VolumeSnapshot    apiGroup: snapshot.storage.k8s.io  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 10Gi,98
7.2 - Persistent Volumes,Create PersistentVolumeClaim from an existing PVC,Create PersistentVolumeClaim from an existing PVC apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: cloned-pvcspec:  storageClassName: my-csi-plugin  dataSource:    name: existing-src-pvc-name    kind: PersistentVolumeClaim  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 10Gi,86
7.2 - Persistent Volumes,Volume populators and data sources,"Volume populators and data sources FEATURE STATE: Kubernetes v1.24 [beta] Kubernetes supports custom volume populators.To use custom volume populators, you must enable the AnyVolumeDataSourcefeature gate forthe kube-apiserver and kube-controller-manager. Volume populators take advantage of a PVC spec field called dataSourceRef. Unlike thedataSource field, which can only contain either a reference to another PersistentVolumeClaimor to a VolumeSnapshot, the dataSourceRef field can contain a reference to any object in thesame namespace, except for core objects other than PVCs. For clusters that have the featuregate enabled, use of the dataSourceRef is preferred over dataSource.",152
7.2 - Persistent Volumes,Cross namespace data sources,"Cross namespace data sources FEATURE STATE: Kubernetes v1.26 [alpha] Kubernetes supports cross namespace volume data sources.To use cross namespace volume data sources, you must enable the AnyVolumeDataSourceand CrossNamespaceVolumeDataSourcefeature gates forthe kube-apiserver, kube-controller-manager.Also, you must enable the CrossNamespaceVolumeDataSource feature gate for the csi-provisioner. Enabling the CrossNamespaceVolumeDataSource feature gate allow you to specifya namespace in the dataSourceRef field. Note: When you specify a namespace for a volume data source, Kubernetes checks for aReferenceGrant in the other namespace before accepting the reference.ReferenceGrant is part of the gateway.networking.k8s.io extension APIs.See ReferenceGrantin the Gateway API documentation for details.This means that you must extend your Kubernetes cluster with at least ReferenceGrant from theGateway API before you can use this mechanism.",208
7.2 - Persistent Volumes,Data source references,"Data source references The dataSourceRef field behaves almost the same as the dataSource field. If either one isspecified while the other is not, the API server will give both fields the same value. Neitherfield can be changed after creation, and attempting to specify different values for the twofields will result in a validation error. Therefore the two fields will always have the samecontents. There are two differences between the dataSourceRef field and the dataSource field thatusers should be aware of: The dataSource field ignores invalid values (as if the field was blank) while thedataSourceRef field never ignores values and will cause an error if an invalid value isused. Invalid values are any core object (objects with no apiGroup) except for PVCs.The dataSourceRef field may contain different types of objects, while the dataSource fieldonly allows PVCs and VolumeSnapshots. When the CrossNamespaceVolumeDataSource feature is enabled, there are additional differences: The dataSource field only allows local objects, while the dataSourceRef field allowsobjects in any namespaces.When namespace is specified, dataSource and dataSourceRef are not synced. Users should always use dataSourceRef on clusters that have the feature gate enabled, andfall back to dataSource on clusters that do not. It is not necessary to look at both fieldsunder any circumstance. The duplicated values with slightly different semantics exist only forbackwards compatibility. In particular, a mixture of older and newer controllers are able tointeroperate because the fields are the same.",318
7.2 - Persistent Volumes,Using volume populators,"Using volume populators Volume populators are controllers that cancreate non-empty volumes, where the contents of the volume are determined by a Custom Resource.Users create a populated volume by referring to a Custom Resource using the dataSourceRef field: apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: populated-pvcspec:  dataSourceRef:    name: example-name    kind: ExampleDataSource    apiGroup: example.storage.k8s.io  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 10Gi Because volume populators are external components, attempts to create a PVC that uses onecan fail if not all the correct components are installed. External controllers should generateevents on the PVC to provide feedback on the status of the creation, including warnings ifthe PVC cannot be created due to some missing component. You can install the alpha volume data source validatorcontroller into your cluster. That controller generates warning Events on a PVC in the case that no populatoris registered to handle that kind of data source. When a suitable populator is installed for a PVC, it's theresponsibility of that populator controller to report Events that relate to volume creation and issues duringthe process.",255
7.2 - Persistent Volumes,Using a cross-namespace volume data source,"Using a cross-namespace volume data source FEATURE STATE: Kubernetes v1.26 [alpha] Create a ReferenceGrant to allow the namespace owner to accept the reference.You define a populated volume by specifying a cross namespace volume data sourceusing the dataSourceRef field. You must already have a valid ReferenceGrantin the source namespace: apiVersion: gateway.networking.k8s.io/v1beta1kind: ReferenceGrantmetadata:  name: allow-ns1-pvc  namespace: defaultspec:  from:  - group: """"    kind: PersistentVolumeClaim    namespace: ns1  to:  - group: snapshot.storage.k8s.io    kind: VolumeSnapshot    name: new-snapshot-demo apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: foo-pvc  namespace: ns1spec:  storageClassName: example  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 1Gi  dataSourceRef:    apiGroup: snapshot.storage.k8s.io    kind: VolumeSnapshot    name: new-snapshot-demo    namespace: default  volumeMode: Filesystem",261
7.2 - Persistent Volumes,Writing Portable Configuration,"Writing Portable Configuration If you're writing configuration templates or examples that run on a wide range of clustersand need persistent storage, it is recommended that you use the following pattern: Include PersistentVolumeClaim objects in your bundle of config (alongsideDeployments, ConfigMaps, etc).Do not include PersistentVolume objects in the config, since the user instantiatingthe config may not have permission to create PersistentVolumes.Give the user the option of providing a storage class name when instantiatingthe template.If the user provides a storage class name, put that value into thepersistentVolumeClaim.storageClassName field.This will cause the PVC to match the right storageclass if the cluster has StorageClasses enabled by the admin.If the user does not provide a storage class name, leave thepersistentVolumeClaim.storageClassName field as nil. This will cause aPV to be automatically provisioned for the user with the default StorageClassin the cluster. Many cluster environments have a default StorageClass installed,or administrators can create their own default StorageClass.In your tooling, watch for PVCs that are not getting bound after some timeand surface this to the user, as this may indicate that the cluster has nodynamic storage support (in which case the user should create a matching PV)or the cluster has no storage system (in which case the user cannot deployconfig requiring PVCs). Learn more about Creating a PersistentVolume.Learn more about Creating a PersistentVolumeClaim.Read the Persistent Storage design document.",312
7.3 - Projected Volumes,Introduction,"Introduction A projected volume maps several existing volume sources into the same directory. Currently, the following types of volume sources can be projected: secretdownwardAPIconfigMapserviceAccountToken All sources are required to be in the same namespace as the Pod. For more details,see the all-in-one volume design document.",65
7.3 - Projected Volumes,"Example configuration with a secret, a downwardAPI, and a configMap","Example configuration with a secret, a downwardAPI, and a configMap pods/storage/projected-secret-downwardapi-configmap.yamlapiVersion: v1kind: Podmetadata:  name: volume-testspec:  containers:  - name: container-test    image: busybox:1.28    volumeMounts:    - name: all-in-one      mountPath: ""/projected-volume""      readOnly: true  volumes:  - name: all-in-one    projected:      sources:      - secret:          name: mysecret          items:            - key: username              path: my-group/my-username      - downwardAPI:          items:            - path: ""labels""              fieldRef:                fieldPath: metadata.labels            - path: ""cpu_limit""              resourceFieldRef:                containerName: container-test                resource: limits.cpu      - configMap:          name: myconfigmap          items:            - key: config              path: my-group/my-config",227
7.3 - Projected Volumes,Example configuration: secrets with a non-default permission mode set,"Example configuration: secrets with a non-default permission mode set pods/storage/projected-secrets-nondefault-permission-mode.yamlapiVersion: v1kind: Podmetadata:  name: volume-testspec:  containers:  - name: container-test    image: busybox:1.28    volumeMounts:    - name: all-in-one      mountPath: ""/projected-volume""      readOnly: true  volumes:  - name: all-in-one    projected:      sources:      - secret:          name: mysecret          items:            - key: username              path: my-group/my-username      - secret:          name: mysecret2          items:            - key: password              path: my-group/my-password              mode: 511 Each projected volume source is listed in the spec under sources. Theparameters are nearly the same with two exceptions: For secrets, the secretName field has been changed to name to be consistentwith ConfigMap naming.The defaultMode can only be specified at the projected level and not for eachvolume source. However, as illustrated above, you can explicitly set the modefor each individual projection.",257
7.3 - Projected Volumes,serviceAccountToken projected volumes,"serviceAccountToken projected volumes You can inject the token for the current service accountinto a Pod at a specified path. For example: pods/storage/projected-service-account-token.yamlapiVersion: v1kind: Podmetadata:  name: sa-token-testspec:  containers:  - name: container-test    image: busybox:1.28    volumeMounts:    - name: token-vol      mountPath: ""/service-account""      readOnly: true  serviceAccountName: default  volumes:  - name: token-vol    projected:      sources:      - serviceAccountToken:          audience: api          expirationSeconds: 3600          path: token The example Pod has a projected volume containing the injected service accounttoken. Containers in this Pod can use that token to access the Kubernetes APIserver, authenticating with the identity of the pod's ServiceAccount.The audience field contains the intended audience of thetoken. A recipient of the token must identify itself with an identifier specifiedin the audience of the token, and otherwise should reject the token. This fieldis optional and it defaults to the identifier of the API server. The expirationSeconds is the expected duration of validity of the service accounttoken. It defaults to 1 hour and must be at least 10 minutes (600 seconds). An administratorcan also limit its maximum value by specifying the --service-account-max-token-expirationoption for the API server. The path field specifies a relative path to the mount pointof the projected volume. Note: A container using a projected volume source as a subPathvolume mount will not receive updates for those volume sources.",346
7.3 - Projected Volumes,Linux,"Linux In Linux pods that have a projected volume and RunAsUser set in the PodSecurityContext,the projected files have the correct ownership set including container userownership. When all containers in a pod have the same runAsUser set in theirPodSecurityContextor containerSecurityContext,then the kubelet ensures that the contents of the serviceAccountToken volume are owned by that user,and the token file has its permission mode set to 0600. Note:Ephemeral containersadded to a Pod after it is created do not change volume permissions that wereset when the pod was created.If a Pod's serviceAccountToken volume permissions were set to 0600 becauseall other containers in the Pod have the same runAsUser, ephemeralcontainers must use the same runAsUser to be able to read the token.",167
7.3 - Projected Volumes,Windows,"Windows In Windows pods that have a projected volume and RunAsUsername set in thePod SecurityContext, the ownership is not enforced due to the way useraccounts are managed in Windows. Windows stores and manages local user and groupaccounts in a database file called Security Account Manager (SAM). Eachcontainer maintains its own instance of the SAM database, to which the host hasno visibility into while the container is running. Windows containers aredesigned to run the user mode portion of the OS in isolation from the host,hence the maintenance of a virtual SAM database. As a result, the kubelet runningon the host does not have the ability to dynamically configure host fileownership for virtualized container accounts. It is recommended that if files onthe host machine are to be shared with the container then they should be placedinto their own volume mount outside of C:\. By default, the projected files will have the following ownership as shown foran example projected volume file: PS C:\> Get-Acl C:\var\run\secrets\kubernetes.io\serviceaccount\..2021_08_31_22_22_18.318230061\ca.crt | Format-ListPath   : Microsoft.PowerShell.Core\FileSystem::C:\var\run\secrets\kubernetes.io\serviceaccount\..2021_08_31_22_22_18.318230061\ca.crtOwner  : BUILTIN\AdministratorsGroup  : NT AUTHORITY\SYSTEMAccess : NT AUTHORITY\SYSTEM Allow  FullControl         BUILTIN\Administrators Allow  FullControl         BUILTIN\Users Allow  ReadAndExecute, SynchronizeAudit  :Sddl   : O:BAG:SYD:AI(A;ID;FA;;;SY)(A;ID;FA;;;BA)(A;ID;0x1200a9;;;BU) This implies all administrator users like ContainerAdministrator will haveread, write and execute access while, non-administrator users will have read andexecute access. Note:In general, granting the container access to the host is discouraged as it canopen the door for potential security exploits.Creating a Windows Pod with RunAsUser in it's SecurityContext will result inthe Pod being stuck at ContainerCreating forever. So it is advised to not usethe Linux only RunAsUser option with Windows Pods.",514
7.4 - Ephemeral Volumes,default,"This document describes ephemeral volumes in Kubernetes. Familiaritywith volumes is suggested, inparticular PersistentVolumeClaim and PersistentVolume. Some application need additional storage but don't care whether thatdata is stored persistently across restarts. For example, cachingservices are often limited by memory size and can move infrequentlyused data into storage that is slower than memory with little impacton overall performance. Other applications expect some read-only input data to be present infiles, like configuration data or secret keys. Ephemeral volumes are designed for these use cases. Because volumesfollow the Pod's lifetime and get created and deleted along with thePod, Pods can be stopped and restarted without being limited to wheresome persistent volume is available. Ephemeral volumes are specified inline in the Pod spec, whichsimplifies application deployment and management.",179
7.4 - Ephemeral Volumes,Types of ephemeral volumes,"Types of ephemeral volumes Kubernetes supports several different kinds of ephemeral volumes fordifferent purposes: emptyDir: empty at Pod startup,with storage coming locally from the kubelet base directory (usuallythe root disk) or RAMconfigMap,downwardAPI,secret: inject differentkinds of Kubernetes data into a PodCSI ephemeral volumes:similar to the previous volume kinds, but provided by specialCSI driverswhich specifically support this featuregeneric ephemeral volumes, whichcan be provided by all storage drivers that also support persistent volumes emptyDir, configMap, downwardAPI, secret are provided aslocal ephemeralstorage.They are managed by kubelet on each node. CSI ephemeral volumes must be provided by third-party CSI storagedrivers. Generic ephemeral volumes can be provided by third-party CSI storagedrivers, but also by any other storage driver that supports dynamicprovisioning. Some CSI drivers are written specifically for CSIephemeral volumes and do not support dynamic provisioning: those thencannot be used for generic ephemeral volumes. The advantage of using third-party drivers is that they can offerfunctionality that Kubernetes itself does not support, for examplestorage with different performance characteristics than the disk thatis managed by kubelet, or injecting different data.",282
7.4 - Ephemeral Volumes,CSI ephemeral volumes,"CSI ephemeral volumes FEATURE STATE: Kubernetes v1.25 [stable] Note: CSI ephemeral volumes are only supported by a subset of CSI drivers.The Kubernetes CSI Drivers listshows which drivers support ephemeral volumes. Conceptually, CSI ephemeral volumes are similar to configMap,downwardAPI and secret volume types: the storage is managed locally on eachnode and is created together with other local resources after a Pod has beenscheduled onto a node. Kubernetes has no concept of rescheduling Podsanymore at this stage. Volume creation has to be unlikely to fail,otherwise Pod startup gets stuck. In particular, storage capacityaware Pod scheduling is notsupported for these volumes. They are currently also not covered bythe storage resource usage limits of a Pod, because that is somethingthat kubelet can only enforce for storage that it manages itself. Here's an example manifest for a Pod that uses CSI ephemeral storage: kind: PodapiVersion: v1metadata:  name: my-csi-appspec:  containers:    - name: my-frontend      image: busybox:1.28      volumeMounts:      - mountPath: ""/data""        name: my-csi-inline-vol      command: [ ""sleep"", ""1000000"" ]  volumes:    - name: my-csi-inline-vol      csi:        driver: inline.storage.kubernetes.io        volumeAttributes:          foo: bar The volumeAttributes determine what volume is prepared by thedriver. These attributes are specific to each driver and notstandardized. See the documentation of each CSI driver for furtherinstructions. CSI ephemeral volumes For more information on the design, see theEphemeral Inline CSI volumes KEP.For more information on further development of this feature, see theenhancement tracking issue #596.",404
7.4 - Ephemeral Volumes,CSI driver restrictions,"CSI driver restrictions CSI ephemeral volumes allow users to provide volumeAttributesdirectly to the CSI driver as part of the Pod spec. A CSI driverallowing volumeAttributes that are typically restricted toadministrators is NOT suitable for use in an inline ephemeral volume.For example, parameters that are normally defined in the StorageClassshould not be exposed to users through the use of inline ephemeral volumes. Cluster administrators who need to restrict the CSI drivers that areallowed to be used as inline volumes within a Pod spec may do so by: Removing Ephemeral from volumeLifecycleModes in the CSIDriver spec, which prevents thedriver from being used as an inline ephemeral volume.Using an admission webhookto restrict how this driver is used.",157
7.4 - Ephemeral Volumes,Generic ephemeral volumes,"Generic ephemeral volumes FEATURE STATE: Kubernetes v1.23 [stable] Generic ephemeral volumes are similar to emptyDir volumes in thesense that they provide a per-pod directory for scratch data that isusually empty after provisioning. But they may also have additionalfeatures: Storage can be local or network-attached.Volumes can have a fixed size that Pods are not able to exceed.Volumes may have some initial data, depending on the driver andparameters.Typical operations on volumes are supported assuming that the driversupports them, includingsnapshotting,cloning,resizing,and storage capacity tracking. Example: kind: PodapiVersion: v1metadata:  name: my-appspec:  containers:    - name: my-frontend      image: busybox:1.28      volumeMounts:      - mountPath: ""/scratch""        name: scratch-volume      command: [ ""sleep"", ""1000000"" ]  volumes:    - name: scratch-volume      ephemeral:        volumeClaimTemplate:          metadata:            labels:              type: my-frontend-volume          spec:            accessModes: [ ""ReadWriteOnce"" ]            storageClassName: ""scratch-storage-class""            resources:              requests:                storage: 1Gi Generic ephemeral volumes For more information on the design, see theGeneric ephemeral inline volumes KEP.",306
7.4 - Ephemeral Volumes,Lifecycle and PersistentVolumeClaim,"Lifecycle and PersistentVolumeClaim The key design idea is that theparameters for a volume claimare allowed inside a volume source of the Pod. Labels, annotations andthe whole set of fields for a PersistentVolumeClaim are supported. When such a Pod getscreated, the ephemeral volume controller then creates an actual PersistentVolumeClaimobject in the same namespace as the Pod and ensures that the PersistentVolumeClaimgets deleted when the Pod gets deleted. That triggers volume binding and/or provisioning, either immediately ifthe StorageClass uses immediate volume binding or when the Pod istentatively scheduled onto a node (WaitForFirstConsumer volumebinding mode). The latter is recommended for generic ephemeral volumesbecause then the scheduler is free to choose a suitable node forthe Pod. With immediate binding, the scheduler is forced to select a node that hasaccess to the volume once it is available. In terms of resource ownership,a Pod that has generic ephemeral storage is the owner of the PersistentVolumeClaim(s)that provide that ephemeral storage. When the Pod is deleted,the Kubernetes garbage collector deletes the PVC, which then usuallytriggers deletion of the volume because the default reclaim policy ofstorage classes is to delete volumes. You can create quasi-ephemeral local storageusing a StorageClass with a reclaim policy of retain: the storage outlives the Pod,and in this case you need to ensure that volume clean up happens separately. While these PVCs exist, they can be used like any other PVC. Inparticular, they can be referenced as data source in volume cloning orsnapshotting. The PVC object also holds the current status of thevolume.",354
7.4 - Ephemeral Volumes,PersistentVolumeClaim naming,"PersistentVolumeClaim naming Naming of the automatically created PVCs is deterministic: the name isa combination of Pod name and volume name, with a hyphen (-) in themiddle. In the example above, the PVC name will bemy-app-scratch-volume. This deterministic naming makes it easier tointeract with the PVC because one does not have to search for it oncethe Pod name and volume name are known. The deterministic naming also introduces a potential conflict between differentPods (a Pod ""pod-a"" with volume ""scratch"" and another Pod with name""pod"" and volume ""a-scratch"" both end up with the same PVC name""pod-a-scratch"") and between Pods and manually created PVCs. Such conflicts are detected: a PVC is only used for an ephemeralvolume if it was created for the Pod. This check is based on theownership relationship. An existing PVC is not overwritten ormodified. But this does not resolve the conflict because without theright PVC, the Pod cannot start. Caution: Take care when naming Pods and volumes inside thesame namespace, so that these conflicts can't occur.",246
7.4 - Ephemeral Volumes,Security,"Security Enabling the GenericEphemeralVolume feature allows users to createPVCs indirectly if they can create Pods, even if they do not havepermission to create PVCs directly. Cluster administrators must beaware of this. If this does not fit their security model, they shoulduse an admission webhookthat rejects objects like Pods that have a generic ephemeral volume. The normal namespace quota for PVCsstill applies, so even if users are allowed to use this new mechanism, they cannot useit to circumvent other policies.",109
7.5 - Storage Classes,Introduction,"Introduction A StorageClass provides a way for administrators to describe the ""classes"" ofstorage they offer. Different classes might map to quality-of-service levels,or to backup policies, or to arbitrary policies determined by the clusteradministrators. Kubernetes itself is unopinionated about what classesrepresent. This concept is sometimes called ""profiles"" in other storagesystems.",80
7.5 - Storage Classes,The StorageClass Resource,"The StorageClass Resource Each StorageClass contains the fields provisioner, parameters, andreclaimPolicy, which are used when a PersistentVolume belonging to theclass needs to be dynamically provisioned. The name of a StorageClass object is significant, and is how users canrequest a particular class. Administrators set the name and other parametersof a class when first creating StorageClass objects, and the objects cannotbe updated once they are created. Administrators can specify a default StorageClass only for PVCs that don'trequest any particular class to bind to: see thePersistentVolumeClaim sectionfor details. apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: standardprovisioner: kubernetes.io/aws-ebsparameters:  type: gp2reclaimPolicy: RetainallowVolumeExpansion: truemountOptions:  - debugvolumeBindingMode: Immediate",193
7.5 - Storage Classes,Provisioner,"Provisioner Each StorageClass has a provisioner that determines what volume plugin is usedfor provisioning PVs. This field must be specified. Volume PluginInternal ProvisionerConfig ExampleAWSElasticBlockStore✓AWS EBSAzureFile✓Azure FileAzureDisk✓Azure DiskCephFS--Cinder✓OpenStack CinderFC--FlexVolume--GCEPersistentDisk✓GCE PDiSCSI--NFS-NFSRBD✓Ceph RBDVsphereVolume✓vSpherePortworxVolume✓Portworx VolumeLocal-Local You are not restricted to specifying the ""internal"" provisionerslisted here (whose names are prefixed with ""kubernetes.io"" and shippedalongside Kubernetes). You can also run and specify external provisioners,which are independent programs that follow a specificationdefined by Kubernetes. Authors of external provisioners have full discretionover where their code lives, how the provisioner is shipped, how it needs to berun, what volume plugin it uses (including Flex), etc. The repositorykubernetes-sigs/sig-storage-lib-external-provisionerhouses a library for writing external provisioners that implements the bulk ofthe specification. Some external provisioners are listed under the repositorykubernetes-sigs/sig-storage-lib-external-provisioner. For example, NFS doesn't provide an internal provisioner, but an externalprovisioner can be used. There are also cases when 3rd party storagevendors provide their own external provisioner.",344
7.5 - Storage Classes,Reclaim Policy,"Reclaim Policy PersistentVolumes that are dynamically created by a StorageClass will have thereclaim policy specified in the reclaimPolicy field of the class, which can beeither Delete or Retain. If no reclaimPolicy is specified when aStorageClass object is created, it will default to Delete. PersistentVolumes that are created manually and managed via a StorageClass will havewhatever reclaim policy they were assigned at creation.",84
7.5 - Storage Classes,Allow Volume Expansion,"Allow Volume Expansion FEATURE STATE: Kubernetes v1.11 [beta] PersistentVolumes can be configured to be expandable. This feature when set to true,allows the users to resize the volume by editing the corresponding PVC object. The following types of volumes support volume expansion, when the underlyingStorageClass has the field allowVolumeExpansion set to true. Table of Volume types and the version of Kubernetes they requireVolume typeRequired Kubernetes versiongcePersistentDisk1.11awsElasticBlockStore1.11Cinder1.11rbd1.11Azure File1.11Azure Disk1.11Portworx1.11FlexVolume1.13CSI1.14 (alpha), 1.16 (beta) Note: You can only use the volume expansion feature to grow a Volume, not to shrink it.",183
7.5 - Storage Classes,Mount Options,"Mount Options PersistentVolumes that are dynamically created by a StorageClass will have themount options specified in the mountOptions field of the class. If the volume plugin does not support mount options but mount options arespecified, provisioning will fail. Mount options are not validated on eitherthe class or PV. If a mount option is invalid, the PV mount fails.",73
7.5 - Storage Classes,Volume Binding Mode,"Volume Binding Mode The volumeBindingMode field controls when volume binding and dynamicprovisioning should occur. When unset, ""Immediate"" mode is used by default. The Immediate mode indicates that volume binding and dynamicprovisioning occurs once the PersistentVolumeClaim is created. For storagebackends that are topology-constrained and not globally accessible from all Nodesin the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod's schedulingrequirements. This may result in unschedulable Pods. A cluster administrator can address this issue by specifying the WaitForFirstConsumer mode whichwill delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.PersistentVolumes will be selected or provisioned conforming to the topology that isspecified by the Pod's scheduling constraints. These include, but are not limited to, resourcerequirements,node selectors,pod affinity andanti-affinity,and taints and tolerations. The following plugins support WaitForFirstConsumer with dynamic provisioning: AWSElasticBlockStoreGCEPersistentDiskAzureDisk The following plugins support WaitForFirstConsumer with pre-created PersistentVolume binding: All of the aboveLocal FEATURE STATE: Kubernetes v1.17 [stable]CSI volumes are also supported with dynamic provisioningand pre-created PVs, but you'll need to look at the documentation for a specific CSI driverto see its supported topology keys and examples. Note:If you choose to use WaitForFirstConsumer, do not use nodeName in the Pod specto specify node affinity. If nodeName is used in this case, the scheduler will be bypassed and PVC will remain in pending state.Instead, you can use node selector for hostname in this case as shown below. apiVersion: v1kind: Podmetadata:  name: task-pv-podspec:  nodeSelector:    kubernetes.io/hostname: kube-01  volumes:    - name: task-pv-storage      persistentVolumeClaim:        claimName: task-pv-claim  containers:    - name: task-pv-container      image: nginx      ports:        - containerPort: 80          name: ""http-server""      volumeMounts:        - mountPath: ""/usr/share/nginx/html""          name: task-pv-storage",517
7.5 - Storage Classes,Allowed Topologies,"Allowed Topologies When a cluster operator specifies the WaitForFirstConsumer volume binding mode, it is no longer necessaryto restrict provisioning to specific topologies in most situations. However,if still required, allowedTopologies can be specified. This example demonstrates how to restrict the topology of provisioned volumes to specificzones and should be used as a replacement for the zone and zones parameters for thesupported plugins. apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: standardprovisioner: kubernetes.io/gce-pdparameters:  type: pd-standardvolumeBindingMode: WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:  - key: failure-domain.beta.kubernetes.io/zone    values:    - us-central-1a    - us-central-1b",186
7.5 - Storage Classes,Parameters,"Parameters Storage Classes have parameters that describe volumes belonging to the storageclass. Different parameters may be accepted depending on the provisioner. Forexample, the value io1, for the parameter type, and the parameteriopsPerGB are specific to EBS. When a parameter is omitted, some default isused. There can be at most 512 parameters defined for a StorageClass.The total length of the parameters object including its keys and values cannotexceed 256 KiB.",95
7.5 - Storage Classes,AWS EBS,"AWS EBS apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: slowprovisioner: kubernetes.io/aws-ebsparameters:  type: io1  iopsPerGB: ""10""  fsType: ext4 type: io1, gp2, sc1, st1. SeeAWS docsfor details. Default: gp2.zone (Deprecated): AWS zone. If neither zone nor zones is specified, volumes aregenerally round-robin-ed across all active zones where Kubernetes clusterhas a node. zone and zones parameters must not be used at the same time.zones (Deprecated): A comma separated list of AWS zone(s). If neither zone nor zonesis specified, volumes are generally round-robin-ed across all active zoneswhere Kubernetes cluster has a node. zone and zones parameters must notbe used at the same time.iopsPerGB: only for io1 volumes. I/O operations per second per GiB. AWSvolume plugin multiplies this with size of requested volume to compute IOPSof the volume and caps it at 20 000 IOPS (maximum supported by AWS, seeAWS docs).A string is expected here, i.e. ""10"", not 10.fsType: fsType that is supported by kubernetes. Default: ""ext4"".encrypted: denotes whether the EBS volume should be encrypted or not.Valid values are ""true"" or ""false"". A string is expected here,i.e. ""true"", not true.kmsKeyId: optional. The full Amazon Resource Name of the key to use whenencrypting the volume. If none is supplied but encrypted is true, a key isgenerated by AWS. See AWS docs for valid ARN value. Note: zone and zones parameters are deprecated and replaced withallowedTopologies",401
7.5 - Storage Classes,GCE PD,"GCE PD apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: slowprovisioner: kubernetes.io/gce-pdparameters:  type: pd-standard  fstype: ext4  replication-type: none type: pd-standard or pd-ssd. Default: pd-standardzone (Deprecated): GCE zone. If neither zone nor zones is specified, volumes aregenerally round-robin-ed across all active zones where Kubernetes cluster hasa node. zone and zones parameters must not be used at the same time.zones (Deprecated): A comma separated list of GCE zone(s). If neither zone nor zonesis specified, volumes are generally round-robin-ed across all active zoneswhere Kubernetes cluster has a node. zone and zones parameters must notbe used at the same time.fstype: ext4 or xfs. Default: ext4. The defined filesystem type must be supported by the host operating system.replication-type: none or regional-pd. Default: none. If replication-type is set to none, a regular (zonal) PD will be provisioned. If replication-type is set to regional-pd, aRegional Persistent Diskwill be provisioned. It's highly recommended to havevolumeBindingMode: WaitForFirstConsumer set, in which case when you createa Pod that consumes a PersistentVolumeClaim which uses this StorageClass, aRegional Persistent Disk is provisioned with two zones. One zone is the sameas the zone that the Pod is scheduled in. The other zone is randomly pickedfrom the zones available to the cluster. Disk zones can be further constrainedusing allowedTopologies. Note: zone and zones parameters are deprecated and replaced withallowedTopologies",389
7.5 - Storage Classes,NFS,"NFS apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: example-nfsprovisioner: example.com/external-nfsparameters:  server: nfs-server.example.com  path: /share  readOnly: ""false"" server: Server is the hostname or IP address of the NFS server.path: Path that is exported by the NFS server.readOnly: A flag indicating whether the storage will be mounted as read only (default false). Kubernetes doesn't include an internal NFS provisioner. You need to use an external provisioner to create a StorageClass for NFS.Here are some examples: NFS Ganesha server and external provisionerNFS subdir external provisioner",166
7.5 - Storage Classes,OpenStack Cinder,"OpenStack Cinder apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: goldprovisioner: kubernetes.io/cinderparameters:  availability: nova availability: Availability Zone. If not specified, volumes are generallyround-robin-ed across all active zones where Kubernetes cluster has a node. Note:FEATURE STATE: Kubernetes v1.11 [deprecated]This internal provisioner of OpenStack is deprecated. Please use the external cloud provider for OpenStack.",118
7.5 - Storage Classes,vSphere,"vSphere There are two types of provisioners for vSphere storage classes: CSI provisioner: csi.vsphere.vmware.comvCP provisioner: kubernetes.io/vsphere-volume In-tree provisioners are deprecated. For more information on the CSI provisioner, see Kubernetes vSphere CSI Driver and vSphereVolume CSI migration.",80
7.5 - Storage Classes,vCP Provisioner,"vCP Provisioner The following examples use the VMware Cloud Provider (vCP) StorageClass provisioner. Create a StorageClass with a user specified disk format.apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: fastprovisioner: kubernetes.io/vsphere-volumeparameters:  diskformat: zeroedthickdiskformat: thin, zeroedthick and eagerzeroedthick. Default: ""thin"".Create a StorageClass with a disk format on a user specified datastore.apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: fastprovisioner: kubernetes.io/vsphere-volumeparameters:  diskformat: zeroedthick  datastore: VSANDatastoredatastore: The user can also specify the datastore in the StorageClass.The volume will be created on the datastore specified in the StorageClass,which in this case is VSANDatastore. This field is optional. If thedatastore is not specified, then the volume will be created on the datastorespecified in the vSphere config file used to initialize the vSphere CloudProvider.Storage Policy Management inside kubernetesUsing existing vCenter SPBM policyOne of the most important features of vSphere for Storage Management ispolicy based Management. Storage Policy Based Management (SPBM) is astorage policy framework that provides a single unified control planeacross a broad range of data services and storage solutions. SPBM enablesvSphere administrators to overcome upfront storage provisioning challenges,such as capacity planning, differentiated service levels and managingcapacity headroom.The SPBM policies can be specified in the StorageClass using thestoragePolicyName parameter.Virtual SAN policy support inside KubernetesVsphere Infrastructure (VI) Admins will have the ability to specify customVirtual SAN Storage Capabilities during dynamic volume provisioning. Youcan now define storage requirements, such as performance and availability,in the form of storage capabilities during dynamic volume provisioning.The storage capability requirements are converted into a Virtual SANpolicy which are then pushed down to the Virtual SAN layer when apersistent volume (virtual disk) is being created. The virtual disk isdistributed across the Virtual SAN datastore to meet the requirements.You can see Storage Policy Based Management for dynamic provisioning of volumesfor more details on how to use storage policies for persistent volumesmanagement. There are fewvSphere exampleswhich you try out for persistent volume management inside Kubernetes for vSphere.",543
7.5 - Storage Classes,Ceph RBD,"Ceph RBD apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: fastprovisioner: kubernetes.io/rbdparameters:  monitors: 10.16.153.105:6789  adminId: kube  adminSecretName: ceph-secret  adminSecretNamespace: kube-system  pool: kube  userId: kube  userSecretName: ceph-secret-user  userSecretNamespace: default  fsType: ext4  imageFormat: ""2""  imageFeatures: ""layering"" monitors: Ceph monitors, comma delimited. This parameter is required.adminId: Ceph client ID that is capable of creating images in the pool.Default is ""admin"".adminSecretName: Secret Name for adminId. This parameter is required.The provided secret must have type ""kubernetes.io/rbd"".adminSecretNamespace: The namespace for adminSecretName. Default is ""default"".pool: Ceph RBD pool. Default is ""rbd"".userId: Ceph client ID that is used to map the RBD image. Default is thesame as adminId.userSecretName: The name of Ceph Secret for userId to map RBD image. Itmust exist in the same namespace as PVCs. This parameter is required.The provided secret must have type ""kubernetes.io/rbd"", for example created in thisway:kubectl create secret generic ceph-secret --type=""kubernetes.io/rbd"" \  --from-literal=key='QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==' \  --namespace=kube-systemuserSecretNamespace: The namespace for userSecretName.fsType: fsType that is supported by kubernetes. Default: ""ext4"".imageFormat: Ceph RBD image format, ""1"" or ""2"". Default is ""2"".imageFeatures: This parameter is optional and should only be used if youset imageFormat to ""2"". Currently supported features are layering only.Default is """", and no features are turned on.",496
7.5 - Storage Classes,Azure Unmanaged Disk storage class,"Azure Unmanaged Disk storage class apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: slowprovisioner: kubernetes.io/azure-diskparameters:  skuName: Standard_LRS  location: eastus  storageAccount: azure_storage_account_name skuName: Azure storage account Sku tier. Default is empty.location: Azure storage account location. Default is empty.storageAccount: Azure storage account name. If a storage account is provided,it must reside in the same resource group as the cluster, and location isignored. If a storage account is not provided, a new storage account will becreated in the same resource group as the cluster.",157
7.5 - Storage Classes,Azure Disk storage class (starting from v1.7.2),"Azure Disk storage class (starting from v1.7.2) apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: slowprovisioner: kubernetes.io/azure-diskparameters:  storageaccounttype: Standard_LRS  kind: managed storageaccounttype: Azure storage account Sku tier. Default is empty.kind: Possible values are shared, dedicated, and managed (default).When kind is shared, all unmanaged disks are created in a few sharedstorage accounts in the same resource group as the cluster. When kind isdedicated, a new dedicated storage account will be created for the newunmanaged disk in the same resource group as the cluster. When kind ismanaged, all managed disks are created in the same resource group asthe cluster.resourceGroup: Specify the resource group in which the Azure disk will be created.It must be an existing resource group name. If it is unspecified, the disk will beplaced in the same resource group as the current Kubernetes cluster. Premium VM can attach both Standard_LRS and Premium_LRS disks, while StandardVM can only attach Standard_LRS disks.Managed VM can only attach managed disks and unmanaged VM can only attachunmanaged disks.",269
7.5 - Storage Classes,Azure File,"Azure File apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: azurefileprovisioner: kubernetes.io/azure-fileparameters:  skuName: Standard_LRS  location: eastus  storageAccount: azure_storage_account_name skuName: Azure storage account Sku tier. Default is empty.location: Azure storage account location. Default is empty.storageAccount: Azure storage account name. Default is empty. If a storageaccount is not provided, all storage accounts associated with the resourcegroup are searched to find one that matches skuName and location. If astorage account is provided, it must reside in the same resource group as thecluster, and skuName and location are ignored.secretNamespace: the namespace of the secret that contains the Azure StorageAccount Name and Key. Default is the same as the Pod.secretName: the name of the secret that contains the Azure Storage Account Name andKey. Default is azure-storage-account-<accountName>-secretreadOnly: a flag indicating whether the storage will be mounted as read only.Defaults to false which means a read/write mount. This setting will impact theReadOnly setting in VolumeMounts as well. During storage provisioning, a secret named by secretName is created for themounting credentials. If the cluster has enabled bothRBAC andController Roles,add the create permission of resource secret for clusterrolesystem:controller:persistent-volume-binder. In a multi-tenancy context, it is strongly recommended to set the value forsecretNamespace explicitly, otherwise the storage account credentials maybe read by other users.",359
7.5 - Storage Classes,Portworx Volume,"Portworx Volume apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: portworx-io-priority-highprovisioner: kubernetes.io/portworx-volumeparameters:  repl: ""1""  snap_interval: ""70""  priority_io: ""high"" fs: filesystem to be laid out: none/xfs/ext4 (default: ext4).block_size: block size in Kbytes (default: 32).repl: number of synchronous replicas to be provided in the form ofreplication factor 1..3 (default: 1) A string is expected here i.e.""1"" and not 1.priority_io: determines whether the volume will be created from higherperformance or a lower priority storage high/medium/low (default: low).snap_interval: clock/time interval in minutes for when to trigger snapshots.Snapshots are incremental based on difference with the prior snapshot, 0disables snaps (default: 0). A string is expected here i.e.""70"" and not 70.aggregation_level: specifies the number of chunks the volume would bedistributed into, 0 indicates a non-aggregated volume (default: 0). A stringis expected here i.e. ""0"" and not 0ephemeral: specifies whether the volume should be cleaned-up after unmountor should be persistent. emptyDir use case can set this value to true andpersistent volumes use case such as for databases like Cassandra should setto false, true/false (default false). A string is expected here i.e.""true"" and not true.",357
7.5 - Storage Classes,Local,"Local FEATURE STATE: Kubernetes v1.14 [stable] apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: local-storageprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumer Local volumes do not currently support dynamic provisioning, however a StorageClassshould still be created to delay volume binding until Pod scheduling. This isspecified by the WaitForFirstConsumer volume binding mode. Delaying volume binding allows the scheduler to consider all of a Pod'sscheduling constraints when choosing an appropriate PersistentVolume for aPersistentVolumeClaim.",140
7.6 - Dynamic Volume Provisioning,default,"Dynamic volume provisioning allows storage volumes to be created on-demand.Without dynamic provisioning, cluster administrators have to manually makecalls to their cloud or storage provider to create new storage volumes, andthen create PersistentVolume objectsto represent them in Kubernetes. The dynamic provisioning feature eliminatesthe need for cluster administrators to pre-provision storage. Instead, itautomatically provisions storage when it is requested by users.",92
7.6 - Dynamic Volume Provisioning,Background,"Background The implementation of dynamic volume provisioning is based on the API object StorageClassfrom the API group storage.k8s.io. A cluster administrator can define as manyStorageClass objects as needed, each specifying a volume plugin (akaprovisioner) that provisions a volume and the set of parameters to pass tothat provisioner when provisioning.A cluster administrator can define and expose multiple flavors of storage (fromthe same or different storage systems) within a cluster, each with a custom setof parameters. This design also ensures that end users don't have to worryabout the complexity and nuances of how storage is provisioned, but stillhave the ability to select from multiple storage options. More information on storage classes can be foundhere.",150
7.6 - Dynamic Volume Provisioning,Enabling Dynamic Provisioning,"Enabling Dynamic Provisioning To enable dynamic provisioning, a cluster administrator needs to pre-createone or more StorageClass objects for users.StorageClass objects define which provisioner should be used and what parametersshould be passed to that provisioner when dynamic provisioning is invoked.The name of a StorageClass object must be a validDNS subdomain name. The following manifest creates a storage class ""slow"" which provisions standarddisk-like persistent disks. apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: slowprovisioner: kubernetes.io/gce-pdparameters:  type: pd-standard The following manifest creates a storage class ""fast"" which provisionsSSD-like persistent disks. apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: fastprovisioner: kubernetes.io/gce-pdparameters:  type: pd-ssd",207
7.6 - Dynamic Volume Provisioning,Using Dynamic Provisioning,"Using Dynamic Provisioning Users request dynamically provisioned storage by including a storage class intheir PersistentVolumeClaim. Before Kubernetes v1.6, this was done via thevolume.beta.kubernetes.io/storage-class annotation. However, this annotationis deprecated since v1.9. Users now can and should instead use thestorageClassName field of the PersistentVolumeClaim object. The value ofthis field must match the name of a StorageClass configured by theadministrator (see below). To select the ""fast"" storage class, for example, a user would create thefollowing PersistentVolumeClaim: apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: claim1spec:  accessModes:    - ReadWriteOnce  storageClassName: fast  resources:    requests:      storage: 30Gi This claim results in an SSD-like Persistent Disk being automaticallyprovisioned. When the claim is deleted, the volume is destroyed.",207
7.6 - Dynamic Volume Provisioning,Defaulting Behavior,"Defaulting Behavior Dynamic provisioning can be enabled on a cluster such that all claims aredynamically provisioned if no storage class is specified. A cluster administratorcan enable this behavior by: Marking one StorageClass object as default;Making sure that the DefaultStorageClass admission controlleris enabled on the API server. An administrator can mark a specific StorageClass as default by adding thestorageclass.kubernetes.io/is-default-class annotation to it.When a default StorageClass exists in a cluster and a user creates aPersistentVolumeClaim with storageClassName unspecified, theDefaultStorageClass admission controller automatically adds thestorageClassName field pointing to the default storage class. Note that there can be at most one default storage class on a cluster, ora PersistentVolumeClaim without storageClassName explicitly specified cannotbe created.",170
7.6 - Dynamic Volume Provisioning,Topology Awareness,"Topology Awareness In Multi-Zone clusters, Pods can be spread acrossZones in a Region. Single-Zone storage backends should be provisioned in the Zones wherePods are scheduled. This can be accomplished by setting theVolume Binding Mode.",52
7.7 - Volume Snapshots,Introduction,"Introduction Similar to how API resources PersistentVolume and PersistentVolumeClaim areused to provision volumes for users and administrators, VolumeSnapshotContentand VolumeSnapshot API resources are provided to create volume snapshots forusers and administrators. A VolumeSnapshotContent is a snapshot taken from a volume in the cluster thathas been provisioned by an administrator. It is a resource in the cluster justlike a PersistentVolume is a cluster resource. A VolumeSnapshot is a request for snapshot of a volume by a user. It is similarto a PersistentVolumeClaim. VolumeSnapshotClass allows you to specify different attributes belonging to aVolumeSnapshot. These attributes may differ among snapshots taken from the samevolume on the storage system and therefore cannot be expressed by using the sameStorageClass of a PersistentVolumeClaim. Volume snapshots provide Kubernetes users with a standardized way to copy a volume'scontents at a particular point in time without creating an entirely new volume. Thisfunctionality enables, for example, database administrators to backup databases beforeperforming edit or delete modifications. Users need to be aware of the following when using this feature: API Objects VolumeSnapshot, VolumeSnapshotContent, and VolumeSnapshotClassare CRDs, notpart of the core API.VolumeSnapshot support is only available for CSI drivers.As part of the deployment process of VolumeSnapshot, the Kubernetes team providesa snapshot controller to be deployed into the control plane, and a sidecar helpercontainer called csi-snapshotter to be deployed together with the CSI driver.The snapshot controller watches VolumeSnapshot and VolumeSnapshotContent objectsand is responsible for the creation and deletion of VolumeSnapshotContent object.The sidecar csi-snapshotter watches VolumeSnapshotContent objects and triggersCreateSnapshot and DeleteSnapshot operations against a CSI endpoint.There is also a validating webhook server which provides tightened validation onsnapshot objects. This should be installed by the Kubernetes distros along withthe snapshot controller and CRDs, not CSI drivers. It should be installed in allKubernetes clusters that has the snapshot feature enabled.CSI drivers may or may not have implemented the volume snapshot functionality.The CSI drivers that have provided support for volume snapshot will likely usethe csi-snapshotter. See CSI Driver documentation for details.The CRDs and snapshot controller installations are the responsibility of the Kubernetes distribution.",498
7.7 - Volume Snapshots,Lifecycle of a volume snapshot and volume snapshot content,Lifecycle of a volume snapshot and volume snapshot content VolumeSnapshotContents are resources in the cluster. VolumeSnapshots are requestsfor those resources. The interaction between VolumeSnapshotContents and VolumeSnapshotsfollow this lifecycle:,46
7.7 - Volume Snapshots,Pre-provisioned,Pre-provisioned A cluster administrator creates a number of VolumeSnapshotContents. They carry the detailsof the real volume snapshot on the storage system which is available for use by cluster users.They exist in the Kubernetes API and are available for consumption.,55
7.7 - Volume Snapshots,Dynamic,"Dynamic Instead of using a pre-existing snapshot, you can request that a snapshot to be dynamicallytaken from a PersistentVolumeClaim. The VolumeSnapshotClassspecifies storage provider-specific parameters to use when taking a snapshot.",47
7.7 - Volume Snapshots,Binding,"Binding The snapshot controller handles the binding of a VolumeSnapshot object with an appropriateVolumeSnapshotContent object, in both pre-provisioned and dynamically provisioned scenarios.The binding is a one-to-one mapping. In the case of pre-provisioned binding, the VolumeSnapshot will remain unbound until therequested VolumeSnapshotContent object is created.",78
7.7 - Volume Snapshots,Persistent Volume Claim as Snapshot Source Protection,"Persistent Volume Claim as Snapshot Source Protection The purpose of this protection is to ensure that in-usePersistentVolumeClaimAPI objects are not removed from the system while a snapshot is being taken from it(as this may result in data loss). While a snapshot is being taken of a PersistentVolumeClaim, that PersistentVolumeClaimis in-use. If you delete a PersistentVolumeClaim API object in active use as a snapshotsource, the PersistentVolumeClaim object is not removed immediately. Instead, removal ofthe PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.",124
7.7 - Volume Snapshots,Delete,"Delete Deletion is triggered by deleting the VolumeSnapshot object, and the DeletionPolicywill be followed. If the DeletionPolicy is Delete, then the underlying storage snapshotwill be deleted along with the VolumeSnapshotContent object. If the DeletionPolicy isRetain, then both the underlying snapshot and VolumeSnapshotContent remain.",72
7.7 - Volume Snapshots,VolumeSnapshots,"VolumeSnapshots Each VolumeSnapshot contains a spec and a status. apiVersion: snapshot.storage.k8s.io/v1kind: VolumeSnapshotmetadata:  name: new-snapshot-testspec:  volumeSnapshotClassName: csi-hostpath-snapclass  source:    persistentVolumeClaimName: pvc-test persistentVolumeClaimName is the name of the PersistentVolumeClaim data sourcefor the snapshot. This field is required for dynamically provisioning a snapshot. A volume snapshot can request a particular class by specifying the name of aVolumeSnapshotClassusing the attribute volumeSnapshotClassName. If nothing is set, then thedefault class is used if available. For pre-provisioned snapshots, you need to specify a volumeSnapshotContentNameas the source for the snapshot as shown in the following example. ThevolumeSnapshotContentName source field is required for pre-provisioned snapshots. apiVersion: snapshot.storage.k8s.io/v1kind: VolumeSnapshotmetadata:  name: test-snapshotspec:  source:    volumeSnapshotContentName: test-content",239
7.7 - Volume Snapshots,Volume Snapshot Contents,"Volume Snapshot Contents Each VolumeSnapshotContent contains a spec and status. In dynamic provisioning,the snapshot common controller creates VolumeSnapshotContent objects. Here is an example: apiVersion: snapshot.storage.k8s.io/v1kind: VolumeSnapshotContentmetadata:  name: snapcontent-72d9a349-aacd-42d2-a240-d775650d2455spec:  deletionPolicy: Delete  driver: hostpath.csi.k8s.io  source:    volumeHandle: ee0cfb94-f8d4-11e9-b2d8-0242ac110002  sourceVolumeMode: Filesystem  volumeSnapshotClassName: csi-hostpath-snapclass  volumeSnapshotRef:    name: new-snapshot-test    namespace: default    uid: 72d9a349-aacd-42d2-a240-d775650d2455 volumeHandle is the unique identifier of the volume created on the storagebackend and returned by the CSI driver during the volume creation. This fieldis required for dynamically provisioning a snapshot.It specifies the volume source of the snapshot. For pre-provisioned snapshots, you (as cluster administrator) are responsiblefor creating the VolumeSnapshotContent object as follows. apiVersion: snapshot.storage.k8s.io/v1kind: VolumeSnapshotContentmetadata:  name: new-snapshot-content-testspec:  deletionPolicy: Delete  driver: hostpath.csi.k8s.io  source:    snapshotHandle: 7bdd0de3-aaeb-11e8-9aae-0242ac110002  sourceVolumeMode: Filesystem  volumeSnapshotRef:    name: new-snapshot-test    namespace: default snapshotHandle is the unique identifier of the volume snapshot created onthe storage backend. This field is required for the pre-provisioned snapshots.It specifies the CSI snapshot id on the storage system that thisVolumeSnapshotContent represents. sourceVolumeMode is the mode of the volume whose snapshot is taken. The valueof the sourceVolumeMode field can be either Filesystem or Block. If thesource volume mode is not specified, Kubernetes treats the snapshot as if thesource volume's mode is unknown. volumeSnapshotRef is the reference of the corresponding VolumeSnapshot. Note thatwhen the VolumeSnapshotContent is being created as a pre-provisioned snapshot, theVolumeSnapshot referenced in volumeSnapshotRef might not exist yet.",546
7.7 - Volume Snapshots,Converting the volume mode of a Snapshot,"Converting the volume mode of a Snapshot If the VolumeSnapshots API installed on your cluster supports the sourceVolumeModefield, then the API has the capability to prevent unauthorized users from convertingthe mode of a volume. To check if your cluster has capability for this feature, run the following command: $ kubectl get crd volumesnapshotcontent -o yaml If you want to allow users to create a PersistentVolumeClaim from an existingVolumeSnapshot, but with a different volume mode than the source, the annotationsnapshot.storage.kubernetes.io/allow-volume-mode-change: ""true""needs to be added tothe VolumeSnapshotContent that corresponds to the VolumeSnapshot. For pre-provisioned snapshots, spec.sourceVolumeMode needs to be populatedby the cluster administrator. An example VolumeSnapshotContent resource with this feature enabled would look like: apiVersion: snapshot.storage.k8s.io/v1kind: VolumeSnapshotContentmetadata:  name: new-snapshot-content-test  annotations:    - snapshot.storage.kubernetes.io/allow-volume-mode-change: ""true""spec:  deletionPolicy: Delete  driver: hostpath.csi.k8s.io  source:    snapshotHandle: 7bdd0de3-aaeb-11e8-9aae-0242ac110002  sourceVolumeMode: Filesystem  volumeSnapshotRef:    name: new-snapshot-test    namespace: default",327
7.7 - Volume Snapshots,Provisioning Volumes from Snapshots,"Provisioning Volumes from Snapshots You can provision a new volume, pre-populated with data from a snapshot, by usingthe dataSource field in the PersistentVolumeClaim object. For more details, seeVolume Snapshot and Restore Volume from Snapshot.",54
7.8 - Volume Snapshot Classes,Introduction,"Introduction Just like StorageClass provides a way for administrators to describe the ""classes""of storage they offer when provisioning a volume, VolumeSnapshotClass provides away to describe the ""classes"" of storage when provisioning a volume snapshot.",47
7.8 - Volume Snapshot Classes,The VolumeSnapshotClass Resource,"The VolumeSnapshotClass Resource Each VolumeSnapshotClass contains the fields driver, deletionPolicy, and parameters,which are used when a VolumeSnapshot belonging to the class needs to bedynamically provisioned. The name of a VolumeSnapshotClass object is significant, and is how users canrequest a particular class. Administrators set the name and other parametersof a class when first creating VolumeSnapshotClass objects, and the objects cannotbe updated once they are created. Note: Installation of the CRDs is the responsibility of the Kubernetes distribution. Without the required CRDs present, the creation of a VolumeSnapshotClass fails. apiVersion: snapshot.storage.k8s.io/v1kind: VolumeSnapshotClassmetadata:  name: csi-hostpath-snapclassdriver: hostpath.csi.k8s.iodeletionPolicy: Deleteparameters: Administrators can specify a default VolumeSnapshotClass for VolumeSnapshotsthat don't request any particular class to bind to by adding thesnapshot.storage.kubernetes.io/is-default-class: ""true"" annotation: apiVersion: snapshot.storage.k8s.io/v1kind: VolumeSnapshotClassmetadata:  name: csi-hostpath-snapclass  annotations:    snapshot.storage.kubernetes.io/is-default-class: ""true""driver: hostpath.csi.k8s.iodeletionPolicy: Deleteparameters:",321
7.8 - Volume Snapshot Classes,DeletionPolicy,"DeletionPolicy Volume snapshot classes have a deletionPolicy. It enables you to configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object it is bound to is to be deleted. The deletionPolicy of a volume snapshot class can either be Retain or Delete. This field must be specified. If the deletionPolicy is Delete, then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remain.",106
7.9 - CSI Volume Cloning,Introduction,"Introduction The CSI Volume Cloning feature adds support for specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume. A Clone is defined as a duplicate of an existing Kubernetes Volume that can be consumed as any standard Volume would be. The only difference is that upon provisioning, rather than creating a ""new"" empty Volume, the back end device creates an exact duplicate of the specified Volume. The implementation of cloning, from the perspective of the Kubernetes API, adds the ability to specify an existing PVC as a dataSource during new PVC creation. The source PVC must be bound and available (not in use). Users need to be aware of the following when using this feature: Cloning support (VolumePVCDataSource) is only available for CSI drivers.Cloning support is only available for dynamic provisioners.CSI drivers may or may not have implemented the volume cloning functionality.You can only clone a PVC when it exists in the same namespace as the destination PVC (source and destination must be in the same namespace).Cloning is supported with a different Storage Class.Destination volume can be the same or a different storage class as the source.Default storage class can be used and storageClassName omitted in the spec.Cloning can only be performed between two volumes that use the same VolumeMode setting (if you request a block mode volume, the source MUST also be block mode)",291
7.9 - CSI Volume Cloning,Provisioning,"Provisioning Clones are provisioned like any other PVC with the exception of adding a dataSource that references an existing PVC in the same namespace. apiVersion: v1kind: PersistentVolumeClaimmetadata:    name: clone-of-pvc-1    namespace: mynsspec:  accessModes:  - ReadWriteOnce  storageClassName: cloning  resources:    requests:      storage: 5Gi  dataSource:    kind: PersistentVolumeClaim    name: pvc-1 Note: You must specify a capacity value for spec.resources.requests.storage, and the value you specify must be the same or larger than the capacity of the source volume. The result is a new PVC with the name clone-of-pvc-1 that has the exact same content as the specified source pvc-1.",176
7.9 - CSI Volume Cloning,Usage,"Usage Upon availability of the new PVC, the cloned PVC is consumed the same as other PVC. It's also expected at this point that the newly created PVC is an independent object. It can be consumed, cloned, snapshotted, or deleted independently and without consideration for it's original dataSource PVC. This also implies that the source is not linked in any way to the newly created clone, it may also be modified or deleted without affecting the newly created clone.",95
7.10 - Storage Capacity,default,"Storage capacity is limited and may vary depending on the node onwhich a pod runs: network-attached storage might not be accessible byall nodes, or storage is local to a node to begin with. FEATURE STATE: Kubernetes v1.24 [stable] This page describes how Kubernetes keeps track of storage capacity andhow the scheduler uses that information to schedule Pods onto nodesthat have access to enough storage capacity for the remaining missingvolumes. Without storage capacity tracking, the scheduler may choose anode that doesn't have enough capacity to provision a volume andmultiple scheduling retries will be needed.",129
7.10 - Storage Capacity,Before you begin,"Before you begin Kubernetes v1.26 includes cluster-level API support forstorage capacity tracking. To use this you must also be using a CSI driver thatsupports capacity tracking. Consult the documentation for the CSI drivers thatyou use to find out whether this support is available and, if so, how to useit. If you are not running Kubernetes v1.26, check thedocumentation for that version of Kubernetes.",94
7.10 - Storage Capacity,API,"API There are two API extensions for this feature: CSIStorageCapacity objects:these get produced by a CSI driver in the namespacewhere the driver is installed. Each object contains capacityinformation for one storage class and defines which nodes haveaccess to that storage.The CSIDriverSpec.StorageCapacity field:when set to true, the Kubernetes scheduler will consider storagecapacity for volumes that use the CSI driver.",88
7.10 - Storage Capacity,Scheduling,"Scheduling Storage capacity information is used by the Kubernetes scheduler if: a Pod uses a volume that has not been created yet,that volume uses a StorageClass which references a CSI driver anduses WaitForFirstConsumer volume bindingmode,andthe CSIDriver object for the driver has StorageCapacity set totrue. In that case, the scheduler only considers nodes for the Pod whichhave enough storage available to them. This check is verysimplistic and only compares the size of the volume against thecapacity listed in CSIStorageCapacity objects with a topology thatincludes the node. For volumes with Immediate volume binding mode, the storage driverdecides where to create the volume, independently of Pods that willuse the volume. The scheduler then schedules Pods onto nodes where thevolume is available after the volume has been created. For CSI ephemeral volumes,scheduling always happens without considering storage capacity. Thisis based on the assumption that this volume type is only used byspecial CSI drivers which are local to a node and do not needsignificant resources there.",223
7.10 - Storage Capacity,Rescheduling,"Rescheduling When a node has been selected for a Pod with WaitForFirstConsumervolumes, that decision is still tentative. The next step is that theCSI storage driver gets asked to create the volume with a hint that thevolume is supposed to be available on the selected node. Because Kubernetes might have chosen a node based on out-datedcapacity information, it is possible that the volume cannot really becreated. The node selection is then reset and the Kubernetes schedulertries again to find a node for the Pod.",114
7.10 - Storage Capacity,Limitations,"Limitations Storage capacity tracking increases the chance that scheduling workson the first try, but cannot guarantee this because the scheduler hasto decide based on potentially out-dated information. Usually, thesame retry mechanism as for scheduling without any storage capacityinformation handles scheduling failures. One situation where scheduling can fail permanently is when a Pod usesmultiple volumes: one volume might have been created already in atopology segment which then does not have enough capacity left foranother volume. Manual intervention is necessary to recover from this,for example by increasing capacity or deleting the volume that wasalready created. For more information on the design, see theStorage Capacity Constraints for Pod Scheduling KEP.",137
7.11 - Node-specific Volume Limits,default,"This page describes the maximum number of volumes that can be attachedto a Node for various cloud providers. Cloud providers like Google, Amazon, and Microsoft typically have a limit onhow many volumes can be attached to a Node. It is important for Kubernetes torespect those limits. Otherwise, Pods scheduled on a Node could get stuckwaiting for volumes to attach.",75
7.11 - Node-specific Volume Limits,Kubernetes default limits,Kubernetes default limits The Kubernetes scheduler has default limits on the number of volumesthat can be attached to a Node: Cloud serviceMaximum volumes per NodeAmazon Elastic Block Store (EBS)39Google Persistent Disk16Microsoft Azure Disk Storage16,56
7.11 - Node-specific Volume Limits,Custom limits,"Custom limits You can change these limits by setting the value of theKUBE_MAX_PD_VOLS environment variable, and then starting the scheduler.CSI drivers might have a different procedure, see their documentationon how to customize their limits. Use caution if you set a limit that is higher than the default limit. Consultthe cloud provider's documentation to make sure that Nodes can actually supportthe limit you set. The limit applies to the entire cluster, so it affects all Nodes.",102
7.11 - Node-specific Volume Limits,Dynamic volume limits,"Dynamic volume limits FEATURE STATE: Kubernetes v1.17 [stable] Dynamic volume limits are supported for following volume types. Amazon EBSGoogle Persistent DiskAzure DiskCSI For volumes managed by in-tree volume plugins, Kubernetes automatically determines the Nodetype and enforces the appropriate maximum number of volumes for the node. For example: OnGoogle Compute Engine,up to 127 volumes can be attached to a node, depending on the nodetype.For Amazon EBS disks on M5,C5,R5,T3 and Z1D instance types, Kubernetes allows only 25volumes to be attached to a Node. For other instance types onAmazon Elastic Compute Cloud (EC2),Kubernetes allows 39 volumes to be attached to a Node.On Azure, up to 64 disks can be attached to a node, depending on the node type. For more details, refer to Sizes for virtual machines in Azure.If a CSI storage driver advertises a maximum number of volumes for a Node (using NodeGetInfo), the kube-scheduler honors that limit.Refer to the CSI specifications for details.For volumes managed by in-tree plugins that have been migrated to a CSI driver, the maximum number of volumes will be the one reported by the CSI driver.",275
7.12 - Volume Health Monitoring,default,FEATURE STATE: Kubernetes v1.21 [alpha] CSI volume health monitoring allows CSI Drivers to detect abnormal volume conditions from the underlying storage systems and report them as events on PVCs or Pods.,44
7.12 - Volume Health Monitoring,Volume health monitoring,"Volume health monitoring Kubernetes volume health monitoring is part of how Kubernetes implements the Container Storage Interface (CSI). Volume health monitoring feature is implemented in two components: an External Health Monitor controller, and the kubelet. If a CSI Driver supports Volume Health Monitoring feature from the controller side, an event will be reported on the related PersistentVolumeClaim (PVC) when an abnormal volume condition is detected on a CSI volume. The External Health Monitor controller also watches for node failure events. You can enable node failure monitoring by setting the enable-node-watcher flag to true. When the external health monitor detects a node failure event, the controller reports an Event will be reported on the PVC to indicate that pods using this PVC are on a failed node. If a CSI Driver supports Volume Health Monitoring feature from the node side, an Event will be reported on every Pod using the PVC when an abnormal volume condition is detected on a CSI volume. In addition, Volume Health information is exposed as Kubelet VolumeStats metrics. A new metric kubelet_volume_stats_health_status_abnormal is added. This metric includes two labels: namespace and persistentvolumeclaim. The count is either 1 or 0. 1 indicates the volume is unhealthy, 0 indicates volume is healthy. For more information, please check KEP. Note: You need to enable the CSIVolumeHealth feature gate to use this feature from the node side. See the CSI driver documentation to find out which CSI drivers have implemented this feature.",311
7.13 - Windows Storage,Persistent storage,"Persistent storage Windows has a layered filesystem driver to mount container layers and create a copyfilesystem based on NTFS. All file paths in the container are resolved only withinthe context of that container. With Docker, volume mounts can only target a directory in the container, and notan individual file. This limitation does not apply to containerd.Volume mounts cannot project files or directories back to the host filesystem.Read-only filesystems are not supported because write access is always requiredfor the Windows registry and SAM database. However, read-only volumes are supported.Volume user-masks and permissions are not available. Because the SAM is not sharedbetween the host & container, there's no mapping between them. All permissions areresolved within the context of the container. As a result, the following storage functionality is not supported on Windows nodes: Volume subpath mounts: only the entire volume can be mounted in a Windows containerSubpath volume mounting for SecretsHost mount projectionRead-only root filesystem (mapped volumes still support readOnly)Block device mappingMemory as the storage medium (for example, emptyDir.medium set to Memory)File system features like uid/gid; per-user Linux filesystem permissionsSetting secret permissions with DefaultMode (due to UID/GID dependency)NFS based storage/volume supportExpanding the mounted volume (resizefs) Kubernetes volumes enable complexapplications, with data persistence and Pod volume sharing requirements, to be deployedon Kubernetes. Management of persistent volumes associated with a specific storageback-end or protocol includes actions such as provisioning/de-provisioning/resizingof volumes, attaching/detaching a volume to/from a Kubernetes node andmounting/dismounting a volume to/from individual containers in a pod that needs topersist data. Volume management components are shipped as Kubernetes volumeplugin.The following broad classes of Kubernetes volume plugins are supported on Windows: FlexVolume pluginsPlease note that FlexVolumes have been deprecated as of 1.23CSI Plugins In-tree volume plugins The following in-tree plugins support persistent storage on Windows nodes: awsElasticBlockStoreazureDiskazureFilegcePersistentDiskvsphereVolume",470
8.1 - Configuration Best Practices,default,"This document highlights and consolidates configuration best practices that are introducedthroughout the user guide, Getting Started documentation, and examples. This is a living document. If you think of something that is not on this list but might be usefulto others, please don't hesitate to file an issue or submit a PR.",64
8.1 - Configuration Best Practices,General Configuration Tips,"General Configuration Tips When defining configurations, specify the latest stable API version.Configuration files should be stored in version control before being pushed to the cluster. Thisallows you to quickly roll back a configuration change if necessary. It also aids clusterre-creation and restoration.Write your configuration files using YAML rather than JSON. Though these formats can be usedinterchangeably in almost all scenarios, YAML tends to be more user-friendly.Group related objects into a single file whenever it makes sense. One file is often easier tomanage than several. See theguestbook-all-in-one.yamlfile as an example of this syntax.Note also that many kubectl commands can be called on a directory. For example, you can callkubectl apply on a directory of config files.Don't specify default values unnecessarily: simple, minimal configuration will make errors less likely.Put object descriptions in annotations, to allow better introspection.",198
8.1 - Configuration Best Practices,"""Naked"" Pods versus ReplicaSets, Deployments, and Jobs","""Naked"" Pods versus ReplicaSets, Deployments, and Jobs Don't use naked Pods (that is, Pods not bound to a ReplicaSet orDeployment) if you can avoid it. Naked Podswill not be rescheduled in the event of a node failure.A Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods isalways available, and specifies a strategy to replace Pods (such asRollingUpdate), isalmost always preferable to creating Pods directly, except for some explicitrestartPolicy: Never scenarios.A Job may also be appropriate.",130
8.1 - Configuration Best Practices,Services,"Services Create a Service before its corresponding backendworkloads (Deployments or ReplicaSets), and before any workloads that need to access it.When Kubernetes starts a container, it provides environment variables pointing to all the Serviceswhich were running when the container was started. For example, if a Service named foo exists,all containers will get the following variables in their initial environment:FOO_SERVICE_HOST=<the host the Service is running on>FOO_SERVICE_PORT=<the port the Service is running on>This does imply an ordering requirement - any Service that a Pod wants to access must becreated before the Pod itself, or else the environment variables will not be populated.DNS does not have this restriction.An optional (though strongly recommended) cluster add-onis a DNS server. The DNS server watches the Kubernetes API for new Services and creates a setof DNS records for each. If DNS has been enabled throughout the cluster then all Pods should beable to do name resolution of Services automatically.Don't specify a hostPort for a Pod unless it is absolutely necessary. When you bind a Pod to ahostPort, it limits the number of places the Pod can be scheduled, because each <hostIP,hostPort, protocol> combination must be unique. If you don't specify the hostIP andprotocol explicitly, Kubernetes will use 0.0.0.0 as the default hostIP and TCP as thedefault protocol.If you only need access to the port for debugging purposes, you can use theapiserver proxyor kubectl port-forward.If you explicitly need to expose a Pod's port on the node, consider using aNodePort Service before resorting tohostPort.Avoid using hostNetwork, for the same reasons as hostPort.Use headless Services(which have a ClusterIP of None) for service discovery when you don't need kube-proxyload balancing.",404
8.1 - Configuration Best Practices,Using Labels,"Using Labels Define and use labels that identifysemantic attributes of your application or Deployment, such as { app.kubernetes.io/name: MyApp, tier: frontend, phase: test, deployment: v3 }. You can use these labels to select theappropriate Pods for other resources; for example, a Service that selects all tier: frontendPods, or all phase: test components of app.kubernetes.io/name: MyApp.See the guestbook appfor examples of this approach.A Service can be made to span multiple Deployments by omitting release-specific labels from itsselector. When you need to update a running service without downtime, use aDeployment.A desired state of an object is described by a Deployment, and if changes to that spec areapplied, the deployment controller changes the actual state to the desired state at a controlledrate.Use the Kubernetes common labelsfor common use cases. These standardized labels enrich the metadata in a way that allows tools,including kubectl and dashboard, towork in an interoperable way.You can manipulate labels for debugging. Because Kubernetes controllers (such as ReplicaSet) andServices match to Pods using selector labels, removing the relevant labels from a Pod will stopit from being considered by a controller or from being served traffic by a Service. If you removethe labels of an existing Pod, its controller will create a new Pod to take its place. This is auseful way to debug a previously ""live"" Pod in a ""quarantine"" environment. To interactively removeor add labels, use kubectl label.",347
8.1 - Configuration Best Practices,Using kubectl,"Using kubectl Use kubectl apply -f <directory>. This looks for Kubernetes configuration in all .yaml,.yml, and .json files in <directory> and passes it to apply.Use label selectors for get and delete operations instead of specific object names. See thesections on label selectorsand using labels effectively.Use kubectl create deployment and kubectl expose to quickly create single-containerDeployments and Services.See Use a Service to Access an Application in a Clusterfor an example.",113
8.2 - ConfigMaps,default,"A ConfigMap is an API object used to store non-confidential data in key-value pairs.Pods can consume ConfigMaps asenvironment variables, command-line arguments, or as configuration files in avolume. A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable. Caution: ConfigMap does not provide secrecy or encryption.If the data you want to store are confidential, use aSecret rather than a ConfigMap,or use additional (third party) tools to keep your data private.",115
8.2 - ConfigMaps,Motivation,"Motivation Use a ConfigMap for setting configuration data separately from application code. For example, imagine that you are developing an application that you can run on yourown computer (for development) and in the cloud (to handle real traffic).You write the code to look in an environment variable named DATABASE_HOST.Locally, you set that variable to localhost. In the cloud, you set it torefer to a Kubernetes Servicethat exposes the database component to your cluster.This lets you fetch a container image running in the cloud anddebug the exact same code locally if needed. A ConfigMap is not designed to hold large chunks of data. The data stored in aConfigMap cannot exceed 1 MiB. If you need to store settings that arelarger than this limit, you may want to consider mounting a volume or use aseparate database or file service.",183
8.2 - ConfigMaps,ConfigMap object,"ConfigMap object A ConfigMap is an API objectthat lets you store configuration for other objects to use. Unlike mostKubernetes objects that have a spec, a ConfigMap has data and binaryDatafields. These fields accept key-value pairs as their values. Both the datafield and the binaryData are optional. The data field is designed tocontain UTF-8 strings while the binaryData field is designed tocontain binary data as base64-encoded strings. The name of a ConfigMap must be a validDNS subdomain name. Each key under the data or the binaryData field must consist ofalphanumeric characters, -, _ or .. The keys stored in data must notoverlap with the keys in the binaryData field. Starting from v1.19, you can add an immutable field to a ConfigMapdefinition to create an immutable ConfigMap.",179
8.2 - ConfigMaps,ConfigMaps and Pods,"ConfigMaps and Pods You can write a Pod spec that refers to a ConfigMap and configures the container(s)in that Pod based on the data in the ConfigMap. The Pod and the ConfigMap must be inthe same namespace. Note: The spec of a static Pod cannot refer to a ConfigMapor any other API objects. Here's an example ConfigMap that has some keys with single values,and other keys where the value looks like a fragment of a configurationformat. apiVersion: v1kind: ConfigMapmetadata:  name: game-demodata:  # property-like keys; each key maps to a simple value  player_initial_lives: ""3""  ui_properties_file_name: ""user-interface.properties""  # file-like keys  game.properties: |    enemy.types=aliens,monsters    player.maximum-lives=5      user-interface.properties: |    color.good=purple    color.bad=yellow    allow.textmode=true There are four different ways that you can use a ConfigMap to configurea container inside a Pod: Inside a container command and argsEnvironment variables for a containerAdd a file in read-only volume, for the application to readWrite code to run inside the Pod that uses the Kubernetes API to read a ConfigMap These different methods lend themselves to different ways of modelingthe data being consumed.For the first three methods, thekubelet uses the data fromthe ConfigMap when it launches container(s) for a Pod. The fourth method means you have to write code to read the ConfigMap and its data.However, because you're using the Kubernetes API directly, your application cansubscribe to get updates whenever the ConfigMap changes, and reactwhen that happens. By accessing the Kubernetes API directly, thistechnique also lets you access a ConfigMap in a different namespace. Here's an example Pod that uses values from game-demo to configure a Pod: configmap/configure-pod.yamlapiVersion: v1kind: Podmetadata:  name: configmap-demo-podspec:  containers:    - name: demo      image: alpine      command: [""sleep"", ""3600""]      env:        # Define the environment variable        - name: PLAYER_INITIAL_LIVES # Notice that the case is different here                                     # from the key name in the ConfigMap.          valueFrom:            configMapKeyRef:              name: game-demo           # The ConfigMap this value comes from.              key: player_initial_lives # The key to fetch.        - name: UI_PROPERTIES_FILE_NAME          valueFrom:            configMapKeyRef:              name: game-demo              key: ui_properties_file_name      volumeMounts:      - name: config        mountPath: ""/config""        readOnly: true  volumes:  # You set volumes at the Pod level, then mount them into containers inside that Pod  - name: config    configMap:      # Provide the name of the ConfigMap you want to mount.      name: game-demo      # An array of keys from the ConfigMap to create as files      items:      - key: ""game.properties""        path: ""game.properties""      - key: ""user-interface.properties""        path: ""user-interface.properties"" A ConfigMap doesn't differentiate between single line property values andmulti-line file-like values.What matters is how Pods and other objects consume those values. For this example, defining a volume and mounting it inside the democontainer as /config creates two files,/config/game.properties and /config/user-interface.properties,even though there are four keys in the ConfigMap. This is because the Poddefinition specifies an items array in the volumes section.If you omit the items array entirely, every key in the ConfigMap becomesa file with the same name as the key, and you get 4 files.",863
8.2 - ConfigMaps,Using ConfigMaps,"Using ConfigMaps ConfigMaps can be mounted as data volumes. ConfigMaps can also be used by otherparts of the system, without being directly exposed to the Pod. For example,ConfigMaps can hold data that other parts of the system should use for configuration. The most common way to use ConfigMaps is to configure settings forcontainers running in a Pod in the same namespace. You can also use aConfigMap separately. For example, youmight encounter addonsor operators thatadjust their behavior based on a ConfigMap.",105
8.2 - ConfigMaps,Using ConfigMaps as files from a Pod,"Using ConfigMaps as files from a Pod To consume a ConfigMap in a volume in a Pod: Create a ConfigMap or use an existing one. Multiple Pods can reference thesame ConfigMap.Modify your Pod definition to add a volume under .spec.volumes[]. Namethe volume anything, and have a .spec.volumes[].configMap.name field setto reference your ConfigMap object.Add a .spec.containers[].volumeMounts[] to each container that needs theConfigMap. Specify .spec.containers[].volumeMounts[].readOnly = true and.spec.containers[].volumeMounts[].mountPath to an unused directory namewhere you would like the ConfigMap to appear.Modify your image or command line so that the program looks for files inthat directory. Each key in the ConfigMap data map becomes the filenameunder mountPath. This is an example of a Pod that mounts a ConfigMap in a volume: apiVersion: v1kind: Podmetadata:  name: mypodspec:  containers:  - name: mypod    image: redis    volumeMounts:    - name: foo      mountPath: ""/etc/foo""      readOnly: true  volumes:  - name: foo    configMap:      name: myconfigmap Each ConfigMap you want to use needs to be referred to in .spec.volumes. If there are multiple containers in the Pod, then each container needs itsown volumeMounts block, but only one .spec.volumes is needed per ConfigMap.",332
8.2 - ConfigMaps,Mounted ConfigMaps are updated automatically,"Mounted ConfigMaps are updated automatically When a ConfigMap currently consumed in a volume is updated, projected keys are eventually updated as well.The kubelet checks whether the mounted ConfigMap is fresh on every periodic sync.However, the kubelet uses its local cache for getting the current value of the ConfigMap.The type of the cache is configurable using the ConfigMapAndSecretChangeDetectionStrategy field inthe KubeletConfiguration struct.A ConfigMap can be either propagated by watch (default), ttl-based, or by redirectingall requests directly to the API server.As a result, the total delay from the moment when the ConfigMap is updated to the momentwhen new keys are projected to the Pod can be as long as the kubelet sync period + cachepropagation delay, where the cache propagation delay depends on the chosen cache type(it equals to watch propagation delay, ttl of cache, or zero correspondingly). ConfigMaps consumed as environment variables are not updated automatically and require a pod restart. Note: A container using a ConfigMap as a subPath volume mount will not receive ConfigMap updates.",233
8.2 - ConfigMaps,Immutable ConfigMaps,"Immutable ConfigMaps FEATURE STATE: Kubernetes v1.21 [stable] The Kubernetes feature Immutable Secrets and ConfigMaps provides an option to setindividual Secrets and ConfigMaps as immutable. For clusters that extensively use ConfigMaps(at least tens of thousands of unique ConfigMap to Pod mounts), preventing changes to theirdata has the following advantages: protects you from accidental (or unwanted) updates that could cause applications outagesimproves performance of your cluster by significantly reducing load on kube-apiserver, byclosing watches for ConfigMaps marked as immutable. This feature is controlled by the ImmutableEphemeralVolumesfeature gate.You can create an immutable ConfigMap by setting the immutable field to true.For example: apiVersion: v1kind: ConfigMapmetadata:  ...data:  ...immutable: true Once a ConfigMap is marked as immutable, it is not possible to revert this changenor to mutate the contents of the data or the binaryData field. You canonly delete and recreate the ConfigMap. Because existing Pods maintain a mount pointto the deleted ConfigMap, it is recommended to recreate these pods. Read about Secrets.Read Configure a Pod to Use a ConfigMap.Read about changing a ConfigMap (or any other Kubernetes object)Read The Twelve-Factor App to understand the motivation forseparating code from configuration.",287
8.3 - Secrets,default,"A Secret is an object that contains a small amount of sensitive data such asa password, a token, or a key. Such information might otherwise be put in aPod specification or in acontainer image. Using aSecret means that you don't need to include confidential data in yourapplication code. Because Secrets can be created independently of the Pods that use them, thereis less risk of the Secret (and its data) being exposed during the workflow ofcreating, viewing, and editing Pods. Kubernetes, and applications that run inyour cluster, can also take additional precautions with Secrets, such as avoidingwriting secret data to nonvolatile storage. Secrets are similar to ConfigMapsbut are specifically intended to hold confidential data. Caution:Kubernetes Secrets are, by default, stored unencrypted in the API server's underlying data store(etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd.Additionally, anyone who is authorized to create a Pod in a namespace can use that access to readany Secret in that namespace; this includes indirect access such as the ability to create aDeployment.In order to safely use Secrets, take at least the following steps:Enable Encryption at Rest for Secrets.Enable or configure RBAC rules withleast-privilege access to Secrets.Restrict Secret access to specific containers.Consider using external Secret store providers.For more guidelines to manage and improve the security of your Secrets, refer toGood practices for Kubernetes Secrets. See Information security for Secrets for more details.",326
8.3 - Secrets,Uses for Secrets,"Uses for Secrets There are three main ways for a Pod to use a Secret: As files in avolume mounted on one or more ofits containers.As container environment variable.By the kubelet when pulling images for the Pod. The Kubernetes control plane also uses Secrets; for example,bootstrap token Secrets are a mechanism tohelp automate node registration.",76
8.3 - Secrets,Alternatives to Secrets,"Alternatives to Secrets Rather than using a Secret to protect confidential data, you can pick from alternatives. Here are some of your options: if your cloud-native component needs to authenticate to another application that youknow is running within the same Kubernetes cluster, you can use aServiceAccountand its tokens to identify your client.there are third-party tools that you can run, either within or outside your cluster,that provide secrets management. For example, a service that Pods access over HTTPS,that reveals a secret if the client correctly authenticates (for example, with a ServiceAccounttoken).for authentication, you can implement a custom signer for X.509 certificates, and useCertificateSigningRequeststo let that custom signer issue certificates to Pods that need them.you can use a device pluginto expose node-local encryption hardware to a specific Pod. For example, you can scheduletrusted Pods onto nodes that provide a Trusted Platform Module, configured out-of-band. You can also combine two or more of those options, including the option to use Secret objects themselves. For example: implement (or deploy) an operatorthat fetches short-lived session tokens from an external service, and then creates Secrets basedon those short-lived session tokens. Pods running in your cluster can make use of the session tokens,and operator ensures they are valid. This separation means that you can run Pods that are unaware ofthe exact mechanisms for issuing and refreshing those session tokens.",310
8.3 - Secrets,Constraints on Secret names and data,"Constraints on Secret names and data The name of a Secret object must be a validDNS subdomain name. You can specify the data and/or the stringData field when creating aconfiguration file for a Secret. The data and the stringData fields are optional.The values for all keys in the data field have to be base64-encoded strings.If the conversion to base64 string is not desirable, you can choose to specifythe stringData field instead, which accepts arbitrary strings as values. The keys of data and stringData must consist of alphanumeric characters,-, _ or .. All key-value pairs in the stringData field are internallymerged into the data field. If a key appears in both the data and thestringData field, the value specified in the stringData field takesprecedence.",170
8.3 - Secrets,Size limit,"Size limit Individual secrets are limited to 1MiB in size. This is to discourage creationof very large secrets that could exhaust the API server and kubelet memory.However, creation of many smaller secrets could also exhaust memory. You canuse a resource quota to limit thenumber of Secrets (or other resources) in a namespace.",67
8.3 - Secrets,Editing a Secret,"Editing a Secret You can edit an existing Secret unless it is immutable. Toedit a Secret, use one of the following methods: Use kubectlUse a configuration file You can also edit the data in a Secret using the Kustomize tool. However, thismethod creates a new Secret object with the edited data. Depending on how you created the Secret, as well as how the Secret is used inyour Pods, updates to existing Secret objects are propagated automatically toPods that use the data. For more information, refer to Mounted Secrets are updated automatically.",117
8.3 - Secrets,Using a Secret,"Using a Secret Secrets can be mounted as data volumes or exposed asenvironment variablesto be used by a container in a Pod. Secrets can also be used by other parts of thesystem, without being directly exposed to the Pod. For example, Secrets can holdcredentials that other parts of the system should use to interact with externalsystems on your behalf. Secret volume sources are validated to ensure that the specified objectreference actually points to an object of type Secret. Therefore, a Secretneeds to be created before any Pods that depend on it. If the Secret cannot be fetched (perhaps because it does not exist, ordue to a temporary lack of connection to the API server) the kubeletperiodically retries running that Pod. The kubelet also reports an Eventfor that Pod, including details of the problem fetching the Secret.",176
8.3 - Secrets,Optional Secrets,"Optional Secrets When you reference a Secret in a Pod, you can mark the Secret as optional,such as in the following example. If an optional Secret doesn't exist,Kubernetes ignores it. apiVersion: v1kind: Podmetadata:  name: mypodspec:  containers:  - name: mypod    image: redis    volumeMounts:    - name: foo      mountPath: ""/etc/foo""      readOnly: true  volumes:  - name: foo    secret:      secretName: mysecret      optional: true By default, Secrets are required. None of a Pod's containers will start untilall non-optional Secrets are available. If a Pod references a specific key in a non-optional Secret and that Secretdoes exist, but is missing the named key, the Pod fails during startup.",173
8.3 - Secrets,Using Secrets as files from a Pod,"Using Secrets as files from a Pod If you want to access data from a Secret in a Pod, one way to do that is tohave Kubernetes make the value of that Secret be available as a file insidethe filesystem of one or more of the Pod's containers. For instructions, refer toDistribute credentials securely using Secrets. When a volume contains data from a Secret, and that Secret is updated, Kubernetes tracksthis and updates the data in the volume, using an eventually-consistent approach. Note: A container using a Secret as asubPath volume mount does not receiveautomated Secret updates. The kubelet keeps a cache of the current keys and values for the Secrets that are used involumes for pods on that node.You can configure the way that the kubelet detects changes from the cached values. TheconfigMapAndSecretChangeDetectionStrategy field in thekubelet configuration controlswhich strategy the kubelet uses. The default strategy is Watch. Updates to Secrets can be either propagated by an API watch mechanism (the default), based ona cache with a defined time-to-live, or polled from the cluster API server on each kubeletsynchronisation loop. As a result, the total delay from the moment when the Secret is updated to the momentwhen new keys are projected to the Pod can be as long as the kubelet sync period + cachepropagation delay, where the cache propagation delay depends on the chosen cache type(following the same order listed in the previous paragraph, these are:watch propagation delay, the configured cache TTL, or zero for direct polling).",336
8.3 - Secrets,Using Secrets as environment variables,"Using Secrets as environment variables To use a Secret in an environment variablein a Pod: For each container in your Pod specification, add an environment variablefor each Secret key that you want to use to theenv[].valueFrom.secretKeyRef field.Modify your image and/or command line so that the program looks for valuesin the specified environment variables. For instructions, refer toDefine container environment variables using Secret data.",87
8.3 - Secrets,Invalid environment variables,"Invalid environment variables If your environment variable definitions in your Pod specification areconsidered to be invalid environment variable names, those keys aren't madeavailable to your container. The Pod is allowed to start. Kubernetes adds an Event with the reason set to InvalidVariableNames and amessage that lists the skipped invalid keys. The following example shows a Pod that refers to a Secret named mysecret, where mysecret contains 2 invalid keys: 1badkey and 2alsobad. kubectl get events The output is similar to: LASTSEEN   FIRSTSEEN   COUNT     NAME            KIND      SUBOBJECT                         TYPE      REASON0s         0s          1         dapi-test-pod   Pod                                         Warning   InvalidEnvironmentVariableNames   kubelet, 127.0.0.1      Keys [1badkey, 2alsobad] from the EnvFrom secret default/mysecret were skipped since they are considered invalid environment variable names.",202
8.3 - Secrets,Container image pull secrets,"Container image pull secrets If you want to fetch container images from a private repository, you need a way forthe kubelet on each node to authenticate to that repository. You can configureimage pull secrets to make this possible. These secrets are configured at the Podlevel.",55
8.3 - Secrets,Using imagePullSecrets,"Using imagePullSecrets The imagePullSecrets field is a list of references to secrets in the same namespace.You can use an imagePullSecrets to pass a secret that contains a Docker (or other) image registrypassword to the kubelet. The kubelet uses this information to pull a private image on behalf of your Pod.See the PodSpec APIfor more information about the imagePullSecrets field. Manually specifying an imagePullSecret You can learn how to specify imagePullSecrets from thecontainer imagesdocumentation. Arranging for imagePullSecrets to be automatically attached You can manually create imagePullSecrets, and reference these from a ServiceAccount. Any Podscreated with that ServiceAccount or created with that ServiceAccount by default, will get theirimagePullSecrets field set to that of the service account.See Add ImagePullSecrets to a service accountfor a detailed explanation of that process.",190
8.3 - Secrets,Use case: Pod with SSH keys,"Use case: Pod with SSH keys Create a Secret containing some SSH keys: kubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/path/to/.ssh/id_rsa --from-file=ssh-publickey=/path/to/.ssh/id_rsa.pub The output is similar to: secret ""ssh-key-secret"" created You can also create a kustomization.yaml with a secretGenerator field containing ssh keys. Caution:Think carefully before sending your own SSH keys: other users of the cluster may have accessto the Secret.You could instead create an SSH private key representing a service identity that you want to beaccessible to all the users with whom you share the Kubernetes cluster, and that you can revokeif the credentials are compromised. Now you can create a Pod which references the secret with the SSH key andconsumes it in a volume: apiVersion: v1kind: Podmetadata:  name: secret-test-pod  labels:    name: secret-testspec:  volumes:  - name: secret-volume    secret:      secretName: ssh-key-secret  containers:  - name: ssh-test-container    image: mySshImage    volumeMounts:    - name: secret-volume      readOnly: true      mountPath: ""/etc/secret-volume"" When the container's command runs, the pieces of the key will be available in: /etc/secret-volume/ssh-publickey/etc/secret-volume/ssh-privatekey The container is then free to use the secret data to establish an SSH connection.",350
8.3 - Secrets,Use case: Pods with prod / test credentials,"Use case: Pods with prod / test credentials This example illustrates a Pod which consumes a secret containing production credentials andanother Pod which consumes a secret with test environment credentials. You can create a kustomization.yaml with a secretGenerator field or runkubectl create secret. kubectl create secret generic prod-db-secret --from-literal=username=produser --from-literal=password=Y4nys7f11 The output is similar to: secret ""prod-db-secret"" created You can also create a secret for test environment credentials. kubectl create secret generic test-db-secret --from-literal=username=testuser --from-literal=password=iluvtests The output is similar to: secret ""test-db-secret"" created Note:Special characters such as $, \, *, =, and ! will be interpreted by yourshell and require escaping.In most shells, the easiest way to escape the password is to surround it with single quotes (').For example, if your actual password is S!B\*d$zDsb=, you should execute the command this way:kubectl create secret generic dev-db-secret --from-literal=username=devuser --from-literal=password='S!B\*d$zDsb='You do not need to escape special characters in passwords from files (--from-file). Now make the Pods: cat <<EOF > pod.yamlapiVersion: v1kind: Listitems:- kind: Pod  apiVersion: v1  metadata:    name: prod-db-client-pod    labels:      name: prod-db-client  spec:    volumes:    - name: secret-volume      secret:        secretName: prod-db-secret    containers:    - name: db-client-container      image: myClientImage      volumeMounts:      - name: secret-volume        readOnly: true        mountPath: ""/etc/secret-volume""- kind: Pod  apiVersion: v1  metadata:    name: test-db-client-pod    labels:      name: test-db-client  spec:    volumes:    - name: secret-volume      secret:        secretName: test-db-secret    containers:    - name: db-client-container      image: myClientImage      volumeMounts:      - name: secret-volume        readOnly: true        mountPath: ""/etc/secret-volume""EOF Add the pods to the same kustomization.yaml: cat <<EOF >> kustomization.yamlresources:- pod.yamlEOF Apply all those objects on the API server by running: kubectl apply -k . Both containers will have the following files present on their filesystems with the valuesfor each container's environment: /etc/secret-volume/username/etc/secret-volume/password Note how the specs for the two Pods differ only in one field; this facilitatescreating Pods with different capabilities from a common Pod template. You could further simplify the base Pod specification by using two service accounts: prod-user with the prod-db-secrettest-user with the test-db-secret The Pod specification is shortened to: apiVersion: v1kind: Podmetadata:  name: prod-db-client-pod  labels:    name: prod-db-clientspec:  serviceAccount: prod-db-client  containers:  - name: db-client-container    image: myClientImage",761
8.3 - Secrets,Use case: dotfiles in a secret volume,"Use case: dotfiles in a secret volume You can make your data ""hidden"" by defining a key that begins with a dot.This key represents a dotfile or ""hidden"" file. For example, when the following secretis mounted into a volume, secret-volume: apiVersion: v1kind: Secretmetadata:  name: dotfile-secretdata:  .secret-file: dmFsdWUtMg0KDQo=---apiVersion: v1kind: Podmetadata:  name: secret-dotfiles-podspec:  volumes:  - name: secret-volume    secret:      secretName: dotfile-secret  containers:  - name: dotfile-test-container    image: registry.k8s.io/busybox    command:    - ls    - ""-l""    - ""/etc/secret-volume""    volumeMounts:    - name: secret-volume      readOnly: true      mountPath: ""/etc/secret-volume"" The volume will contain a single file, called .secret-file, andthe dotfile-test-container will have this file present at the path/etc/secret-volume/.secret-file. Note: Files beginning with dot characters are hidden from the output of ls -l;you must use ls -la to see them when listing directory contents.",285
8.3 - Secrets,Use case: Secret visible to one container in a Pod,"Use case: Secret visible to one container in a Pod Consider a program that needs to handle HTTP requests, do some complex businesslogic, and then sign some messages with an HMAC. Because it has complexapplication logic, there might be an unnoticed remote file reading exploit inthe server, which could expose the private key to an attacker. This could be divided into two processes in two containers: a frontend containerwhich handles user interaction and business logic, but which cannot see theprivate key; and a signer container that can see the private key, and respondsto simple signing requests from the frontend (for example, over localhost networking). With this partitioned approach, an attacker now has to trick the applicationserver into doing something rather arbitrary, which may be harder than gettingit to read a file.",164
8.3 - Secrets,Types of Secret,"Types of Secret When creating a Secret, you can specify its type using the type field ofthe Secretresource, or certain equivalent kubectl command line flags (if available).The Secret type is used to facilitate programmatic handling of the Secret data. Kubernetes provides several built-in types for some common usage scenarios.These types vary in terms of the validations performed and the constraintsKubernetes imposes on them. Built-in TypeUsageOpaquearbitrary user-defined datakubernetes.io/service-account-tokenServiceAccount tokenkubernetes.io/dockercfgserialized ~/.dockercfg filekubernetes.io/dockerconfigjsonserialized ~/.docker/config.json filekubernetes.io/basic-authcredentials for basic authenticationkubernetes.io/ssh-authcredentials for SSH authenticationkubernetes.io/tlsdata for a TLS client or serverbootstrap.kubernetes.io/tokenbootstrap token data You can define and use your own Secret type by assigning a non-empty string as thetype value for a Secret object (an empty string is treated as an Opaque type). Kubernetes doesn't impose any constraints on the type name. However, if youare using one of the built-in types, you must meet all the requirements definedfor that type. If you are defining a type of secret that's for public use, follow the conventionand structure the secret type to have your domain name before the name, separatedby a /. For example: cloud-hosting.example.net/cloud-api-credentials.",351
8.3 - Secrets,Opaque secrets,"Opaque secrets Opaque is the default Secret type if omitted from a Secret configuration file.When you create a Secret using kubectl, you will use the genericsubcommand to indicate an Opaque Secret type. For example, the followingcommand creates an empty Secret of type Opaque. kubectl create secret generic empty-secretkubectl get secret empty-secret The output looks like: NAME           TYPE     DATA   AGEempty-secret   Opaque   0      2m6s The DATA column shows the number of data items stored in the Secret.In this case, 0 means you have created an empty Secret.",133
8.3 - Secrets,Service account token Secrets,"Service account token Secrets A kubernetes.io/service-account-token type of Secret is used to store atoken credential that identifies aservice account. Note:Versions of Kubernetes before v1.22 automatically created credentials foraccessing the Kubernetes API. This older mechanism was based on creating tokenSecrets that could then be mounted into running Pods.In more recent versions, including Kubernetes v1.26, APIcredentials are obtained directly by using theTokenRequestAPI, and are mounted into Pods using aprojected volume.The tokens obtained using this method have bounded lifetimes, and areautomatically invalidated when the Pod they are mounted into is deleted.You can stillmanually createa service account token Secret; for example, if you need a token that neverexpires. However, using theTokenRequestsubresource to obtain a token to access the API is recommended instead.You can use thekubectl create tokencommand to obtain a token from the TokenRequest API. You should only create a service account token Secret objectif you can't use the TokenRequest API to obtain a token,and the security exposure of persisting a non-expiring token credentialin a readable API object is acceptable to you. When using this Secret type, you need to ensure that thekubernetes.io/service-account.name annotation is set to an existingservice account name. If you are creating both the ServiceAccount andthe Secret objects, you should create the ServiceAccount object first. After the Secret is created, a Kubernetes controllerfills in some other fields such as the kubernetes.io/service-account.uid annotation, and thetoken key in the data field, which is populated with an authentication token. The following example configuration declares a service account token Secret: apiVersion: v1kind: Secretmetadata:  name: secret-sa-sample  annotations:    kubernetes.io/service-account.name: ""sa-name""type: kubernetes.io/service-account-tokendata:  # You can include additional key value pairs as you do with Opaque Secrets  extra: YmFyCg== After creating the Secret, wait for Kubernetes to populate the token key in the data field. See the ServiceAccountdocumentation for more information on how service accounts work.You can also check the automountServiceAccountToken field and theserviceAccountName field of thePodfor information on referencing service account credentials from within Pods.",538
8.3 - Secrets,Docker config Secrets,"Docker config Secrets You can use one of the following type values to create a Secret tostore the credentials for accessing a container image registry: kubernetes.io/dockercfgkubernetes.io/dockerconfigjson The kubernetes.io/dockercfg type is reserved to store a serialized~/.dockercfg which is the legacy format for configuring Docker command line.When using this Secret type, you have to ensure the Secret data fieldcontains a .dockercfg key whose value is content of a ~/.dockercfg fileencoded in the base64 format. The kubernetes.io/dockerconfigjson type is designed for storing a serializedJSON that follows the same format rules as the ~/.docker/config.json filewhich is a new format for ~/.dockercfg.When using this Secret type, the data field of the Secret object mustcontain a .dockerconfigjson key, in which the content for the~/.docker/config.json file is provided as a base64 encoded string. Below is an example for a kubernetes.io/dockercfg type of Secret: apiVersion: v1kind: Secretmetadata:  name: secret-dockercfgtype: kubernetes.io/dockercfgdata:  .dockercfg: |        ""<base64 encoded ~/.dockercfg file>"" Note: If you do not want to perform the base64 encoding, you can choose to use thestringData field instead. When you create these types of Secrets using a manifest, the APIserver checks whether the expected key exists in the data field, andit verifies if the value provided can be parsed as a valid JSON. The APIserver doesn't validate if the JSON actually is a Docker config file. When you do not have a Docker config file, or you want to use kubectlto create a Secret for accessing a container registry, you can do: kubectl create secret docker-registry secret-tiger-docker \  --docker-email=tiger@acme.example \  --docker-username=tiger \  --docker-password=pass1234 \  --docker-server=my-registry.example:5000 That command creates a Secret of type kubernetes.io/dockerconfigjson.If you dump the .data.dockerconfigjson field from that new Secret and thendecode it from base64: kubectl get secret secret-tiger-docker -o jsonpath='{.data.*}' | base64 -d then the output is equivalent to this JSON document (which is also a validDocker configuration file): {  ""auths"": {    ""my-registry.example:5000"": {      ""username"": ""tiger"",      ""password"": ""pass1234"",      ""email"": ""tiger@acme.example"",      ""auth"": ""dGlnZXI6cGFzczEyMzQ=""    }  }} Note: The auth value there is base64 encoded; it is obscured but not secret.Anyone who can read that Secret can learn the registry access bearer token.",673
8.3 - Secrets,Basic authentication Secret,"Basic authentication Secret The kubernetes.io/basic-auth type is provided for storing credentials neededfor basic authentication. When using this Secret type, the data field of theSecret must contain one of the following two keys: username: the user name for authenticationpassword: the password or token for authentication Both values for the above two keys are base64 encoded strings. You can, ofcourse, provide the clear text content using the stringData for Secretcreation. The following manifest is an example of a basic authentication Secret: apiVersion: v1kind: Secretmetadata:  name: secret-basic-authtype: kubernetes.io/basic-authstringData:  username: admin      # required field for kubernetes.io/basic-auth  password: t0p-Secret # required field for kubernetes.io/basic-auth The basic authentication Secret type is provided only for convenience.You can create an Opaque type for credentials used for basic authentication.However, using the defined and public Secret type (kubernetes.io/basic-auth) helps otherpeople to understand the purpose of your Secret, and sets a convention for what key namesto expect.The Kubernetes API verifies that the required keys are set for a Secret of this type.",270
8.3 - Secrets,SSH authentication secrets,"SSH authentication secrets The builtin type kubernetes.io/ssh-auth is provided for storing data used inSSH authentication. When using this Secret type, you will have to specify assh-privatekey key-value pair in the data (or stringData) fieldas the SSH credential to use. The following manifest is an example of a Secret used for SSH public/privatekey authentication: apiVersion: v1kind: Secretmetadata:  name: secret-ssh-authtype: kubernetes.io/ssh-authdata:  # the data is abbreviated in this example  ssh-privatekey: |          MIIEpQIBAAKCAQEAulqb/Y ... The SSH authentication Secret type is provided only for user's convenience.You could instead create an Opaque type Secret for credentials used for SSH authentication.However, using the defined and public Secret type (kubernetes.io/ssh-auth) helps otherpeople to understand the purpose of your Secret, and sets a convention for what key namesto expect.and the API server does verify if the required keys are provided in a Secret configuration. Caution: SSH private keys do not establish trusted communication between an SSH client andhost server on their own. A secondary means of establishing trust is needed tomitigate ""man in the middle"" attacks, such as a known_hosts file added to a ConfigMap.",295
8.3 - Secrets,TLS secrets,"TLS secrets Kubernetes provides a builtin Secret type kubernetes.io/tls for storinga certificate and its associated key that are typically used for TLS. One common use for TLS secrets is to configure encryption in transit foran Ingress, but you can also use itwith other resources or directly in your workload.When using this type of Secret, the tls.key and the tls.crt key must be providedin the data (or stringData) field of the Secret configuration, although the APIserver doesn't actually validate the values for each key. The following YAML contains an example config for a TLS Secret: apiVersion: v1kind: Secretmetadata:  name: secret-tlstype: kubernetes.io/tlsdata:  # the data is abbreviated in this example  tls.crt: |        MIIC2DCCAcCgAwIBAgIBATANBgkqh ...  tls.key: |        MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ... The TLS Secret type is provided for user's convenience. You can create an Opaquefor credentials used for TLS server and/or client. However, using the builtin Secrettype helps ensure the consistency of Secret format in your project; the API serverdoes verify if the required keys are provided in a Secret configuration. When creating a TLS Secret using kubectl, you can use the tls subcommandas shown in the following example: kubectl create secret tls my-tls-secret \  --cert=path/to/cert/file \  --key=path/to/key/file The public/private key pair must exist before hand. The public key certificatefor --cert must be DER format as perSection 5.1 of RFC 7468,and must match the given private key for --key (PKCS #8 in DER format;Section 11 of RFC 7468). Note:A kubernetes.io/tls Secret stores the Base64-encoded DER data for keys andcertificates. If you're familiar with PEM format for private keys and for certificates,the base64 data are the same as that format except that you omitthe initial and the last lines that are used in PEM.For example, for a certificate, you do not include --------BEGIN CERTIFICATE-----and -------END CERTIFICATE----.",523
8.3 - Secrets,Bootstrap token Secrets,"Bootstrap token Secrets A bootstrap token Secret can be created by explicitly specifying the Secrettype to bootstrap.kubernetes.io/token. This type of Secret is designed fortokens used during the node bootstrap process. It stores tokens used to signwell-known ConfigMaps. A bootstrap token Secret is usually created in the kube-system namespace andnamed in the form bootstrap-token-<token-id> where <token-id> is a 6 characterstring of the token ID. As a Kubernetes manifest, a bootstrap token Secret might look like thefollowing: apiVersion: v1kind: Secretmetadata:  name: bootstrap-token-5emitj  namespace: kube-systemtype: bootstrap.kubernetes.io/tokendata:  auth-extra-groups: c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=  expiration: MjAyMC0wOS0xM1QwNDozOToxMFo=  token-id: NWVtaXRq  token-secret: a3E0Z2lodnN6emduMXAwcg==  usage-bootstrap-authentication: dHJ1ZQ==  usage-bootstrap-signing: dHJ1ZQ== A bootstrap type Secret has the following keys specified under data: token-id: A random 6 character string as the token identifier. Required.token-secret: A random 16 character string as the actual token secret. Required.description: A human-readable string that describes what the token isused for. Optional.expiration: An absolute UTC time using RFC3339 specifying when the tokenshould be expired. Optional.usage-bootstrap-<usage>: A boolean flag indicating additional usage forthe bootstrap token.auth-extra-groups: A comma-separated list of group names that will beauthenticated as in addition to the system:bootstrappers group. The above YAML may look confusing because the values are all in base64 encodedstrings. In fact, you can create an identical Secret using the following YAML: apiVersion: v1kind: Secretmetadata:  # Note how the Secret is named  name: bootstrap-token-5emitj  # A bootstrap token Secret usually resides in the kube-system namespace  namespace: kube-systemtype: bootstrap.kubernetes.io/tokenstringData:  auth-extra-groups: ""system:bootstrappers:kubeadm:default-node-token""  expiration: ""2020-09-13T04:39:10Z""  # This token ID is used in the name  token-id: ""5emitj""  token-secret: ""kq4gihvszzgn1p0r""  # This token can be used for authentication  usage-bootstrap-authentication: ""true""  # and it can be used for signing  usage-bootstrap-signing: ""true""",690
8.3 - Secrets,Immutable Secrets,"Immutable Secrets FEATURE STATE: Kubernetes v1.21 [stable] Kubernetes lets you mark specific Secrets (and ConfigMaps) as immutable.Preventing changes to the data of an existing Secret has the following benefits: protects you from accidental (or unwanted) updates that could cause applications outages(for clusters that extensively use Secrets - at least tens of thousands of unique Secretto Pod mounts), switching to immutable Secrets improves the performance of your clusterby significantly reducing load on kube-apiserver. The kubelet does not need to maintaina [watch] on any Secrets that are marked as immutable.",131
8.3 - Secrets,Marking a Secret as immutable,"Marking a Secret as immutable You can create an immutable Secret by setting the immutable field to true. For example, apiVersion: v1kind: Secretmetadata:  ...data:  ...immutable: true You can also update any existing mutable Secret to make it immutable. Note: Once a Secret or ConfigMap is marked as immutable, it is not possible to revert this changenor to mutate the contents of the data field. You can only delete and recreate the Secret.Existing Pods maintain a mount point to the deleted Secret - it is recommended to recreatethese pods.",122
8.3 - Secrets,Information security for Secrets,"Information security for Secrets Although ConfigMap and Secret work similarly, Kubernetes applies some additionalprotection for Secret objects. Secrets often hold values that span a spectrum of importance, many of which cancause escalations within Kubernetes (e.g. service account tokens) and toexternal systems. Even if an individual app can reason about the power of theSecrets it expects to interact with, other apps within the same namespace canrender those assumptions invalid. A Secret is only sent to a node if a Pod on that node requires it.For mounting secrets into Pods, the kubelet stores a copy of the data into a tmpfsso that the confidential data is not written to durable storage.Once the Pod that depends on the Secret is deleted, the kubelet deletes its local copyof the confidential data from the Secret. There may be several containers in a Pod. By default, containers you defineonly have access to the default ServiceAccount and its related Secret.You must explicitly define environment variables or map a volume into acontainer in order to provide access to any other Secret. There may be Secrets for several Pods on the same node. However, only theSecrets that a Pod requests are potentially visible within its containers.Therefore, one Pod does not have access to the Secrets of another Pod. Warning: Any containers that run with privileged: true on a node can access allSecrets used on that node. For guidelines to manage and improve the security of your Secrets, refer toGood practices for Kubernetes Secrets.Learn how to manage Secrets using kubectlLearn how to manage Secrets using config fileLearn how to manage Secrets using kustomizeRead the API reference for Secret",347
8.4 - Resource Management for Pods and Containers,default,"When you specify a Pod, you can optionally specify howmuch of each resource a container needs.The most common resources to specify are CPU and memory (RAM); there are others. When you specify the resource request for containers in a Pod, thekube-scheduler uses thisinformation to decide which node to place the Pod on. When you specify a resource limitfor a container, the kubelet enforces those limits so that the running container is notallowed to use more of that resource than the limit you set. The kubelet also reservesat least the request amount of that system resource specifically for that containerto use.",128
8.4 - Resource Management for Pods and Containers,Requests and limits,"Requests and limits If the node where a Pod is running has enough of a resource available, it's possible (andallowed) for a container to use more resource than its request for that resource specifies.However, a container is not allowed to use more than its resource limit. For example, if you set a memory request of 256 MiB for a container, and that container is ina Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to usemore RAM. If you set a memory limit of 4GiB for that container, the kubelet (andcontainer runtime) enforce the limit.The runtime prevents the container from using more than the configured resource limit. For example:when a process in the container tries to consume more than the allowed amount of memory,the system kernel terminates the process that attempted the allocation, with an out of memory(OOM) error. Limits can be implemented either reactively (the system intervenes once it sees a violation)or by enforcement (the system prevents the container from ever exceeding the limit). Differentruntimes can have different ways to implement the same restrictions. Note: If you specify a limit for a resource, but do not specify any request, and no admission-timemechanism has applied a default request for that resource, then Kubernetes copies the limityou specified and uses it as the requested value for the resource.",294
8.4 - Resource Management for Pods and Containers,Resource types,"Resource types CPU and memory are each a resource type. A resource type has a base unit.CPU represents compute processing and is specified in units of Kubernetes CPUs.Memory is specified in units of bytes.For Linux workloads, you can specify huge page resources.Huge pages are a Linux-specific feature where the node kernel allocates blocks of memorythat are much larger than the default page size. For example, on a system where the default page size is 4KiB, you could specify a limit,hugepages-2Mi: 80Mi. If the container tries allocating over 40 2MiB huge pages (atotal of 80 MiB), that allocation fails. Note: You cannot overcommit hugepages-* resources.This is different from the memory and cpu resources. CPU and memory are collectively referred to as compute resources, or resources. Computeresources are measurable quantities that can be requested, allocated, andconsumed. They are distinct fromAPI resources. API resources, such as Pods andServices are objects that can be read and modifiedthrough the Kubernetes API server.",226
8.4 - Resource Management for Pods and Containers,Resource requests and limits of Pod and container,"Resource requests and limits of Pod and container For each container, you can specify resource limits and requests,including the following: spec.containers[].resources.limits.cpuspec.containers[].resources.limits.memoryspec.containers[].resources.limits.hugepages-<size>spec.containers[].resources.requests.cpuspec.containers[].resources.requests.memoryspec.containers[].resources.requests.hugepages-<size> Although you can only specify requests and limits for individual containers,it is also useful to think about the overall resource requests and limits fora Pod.For a particular resource, a Pod resource request/limit is the sum of theresource requests/limits of that type for each container in the Pod.",164
8.4 - Resource Management for Pods and Containers,CPU resource units,"CPU resource units Limits and requests for CPU resources are measured in cpu units.In Kubernetes, 1 CPU unit is equivalent to 1 physical CPU core,or 1 virtual core, depending on whether the node is a physical hostor a virtual machine running inside a physical machine. Fractional requests are allowed. When you define a container withspec.containers[].resources.requests.cpu set to 0.5, you are requesting halfas much CPU time compared to if you asked for 1.0 CPU.For CPU resource units, the quantity expression 0.1 is equivalent to theexpression 100m, which can be read as ""one hundred millicpu"". Some people say""one hundred millicores"", and this is understood to mean the same thing. CPU resource is always specified as an absolute amount of resource, never as a relative amount. For example,500m CPU represents the roughly same amount of computing power whether that containerruns on a single-core, dual-core, or 48-core machine. Note: Kubernetes doesn't allow you to specify CPU resources with a precision finer than1m. Because of this, it's useful to specify CPU units less than 1.0 or 1000m usingthe milliCPU form; for example, 5m rather than 0.005.",268
8.4 - Resource Management for Pods and Containers,Memory resource units,"Memory resource units Limits and requests for memory are measured in bytes. You can express memory asa plain integer or as a fixed-point number using one of thesequantity suffixes:E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,Mi, Ki. For example, the following represent roughly the same value: 128974848, 129e6, 129M,  128974848000m, 123Mi Pay attention to the case of the suffixes. If you request 400m of memory, this is a requestfor 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (400Mi)or 400 megabytes (400M).",162
8.4 - Resource Management for Pods and Containers,Container resources example,"Container resources example The following Pod has two containers. Both containers are defined with a request for0.25 CPUand 64MiB (226 bytes) of memory. Each container has a limit of 0.5CPU and 128MiB of memory. You can say the Pod has a request of 0.5 CPU and 128MiB of memory, and a limit of 1 CPU and 256MiB of memory. ---apiVersion: v1kind: Podmetadata:  name: frontendspec:  containers:  - name: app    image: images.my-company.example/app:v4    resources:      requests:        memory: ""64Mi""        cpu: ""250m""      limits:        memory: ""128Mi""        cpu: ""500m""  - name: log-aggregator    image: images.my-company.example/log-aggregator:v6    resources:      requests:        memory: ""64Mi""        cpu: ""250m""      limits:        memory: ""128Mi""        cpu: ""500m""",226
8.4 - Resource Management for Pods and Containers,How Pods with resource requests are scheduled,"How Pods with resource requests are scheduled When you create a Pod, the Kubernetes scheduler selects a node for the Pod torun on. Each node has a maximum capacity for each of the resource types: theamount of CPU and memory it can provide for Pods. The scheduler ensures that,for each resource type, the sum of the resource requests of the scheduledcontainers is less than the capacity of the node.Note that although actual memoryor CPU resource usage on nodes is very low, the scheduler still refuses to placea Pod on a node if the capacity check fails. This protects against a resourceshortage on a node when resource usage later increases, for example, during adaily peak in request rate.",149
8.4 - Resource Management for Pods and Containers,How Kubernetes applies resource requests and limits,"How Kubernetes applies resource requests and limits When the kubelet starts a container as part of a Pod, the kubelet passes that container'srequests and limits for memory and CPU to the container runtime. On Linux, the container runtime typically configureskernel cgroups that apply and enforce thelimits you defined. The CPU limit defines a hard ceiling on how much CPU time that the container can use.During each scheduling interval (time slice), the Linux kernel checks to see if thislimit is exceeded; if so, the kernel waits before allowing that cgroup to resume execution.The CPU request typically defines a weighting. If several different containers (cgroups)want to run on a contended system, workloads with larger CPU requests are allocated moreCPU time than workloads with small requests.The memory request is mainly used during (Kubernetes) Pod scheduling. On a node that usescgroups v2, the container runtime might use the memory request as a hint to setmemory.min and memory.low.The memory limit defines a memory limit for that cgroup. If the container tries toallocate more memory than this limit, the Linux kernel out-of-memory subsystem activatesand, typically, intervenes by stopping one of the processes in the container that triedto allocate memory. If that process is the container's PID 1, and the container is markedas restartable, Kubernetes restarts the container.The memory limit for the Pod or container can also apply to pages in memory backedvolumes, such as an emptyDir. The kubelet tracks tmpfs emptyDir volumes as containermemory use, rather than as local ephemeral storage. If a container exceeds its memory request and the node that it runs on becomes short ofmemory overall, it is likely that the Pod the container belongs to will beevicted. A container might or might not be allowed to exceed its CPU limit for extended periods of time.However, container runtimes don't terminate Pods or containers for excessive CPU usage. To determine whether a container cannot be scheduled or is being killed due to resource limits,see the Troubleshooting section.",439
8.4 - Resource Management for Pods and Containers,Monitoring compute & memory resource usage,"Monitoring compute & memory resource usage The kubelet reports the resource usage of a Pod as part of the Podstatus. If optional tools for monitoringare available in your cluster, then Pod resource usage can be retrieved eitherfrom the Metrics APIdirectly or from your monitoring tools.",58
8.4 - Resource Management for Pods and Containers,Local ephemeral storage,"Local ephemeral storage FEATURE STATE: Kubernetes v1.25 [stable] Nodes have local ephemeral storage, backed bylocally-attached writeable devices or, sometimes, by RAM.""Ephemeral"" means that there is no long-term guarantee about durability. Pods use ephemeral local storage for scratch space, caching, and for logs.The kubelet can provide scratch space to Pods using local ephemeral storage tomount emptyDirvolumes into containers. The kubelet also uses this kind of storage to holdnode-level container logs,container images, and the writable layers of running containers. Caution: If a node fails, the data in its ephemeral storage can be lost.Your applications cannot expect any performance SLAs (disk IOPS for example)from local ephemeral storage. As a beta feature, Kubernetes lets you track, reserve and limit the amountof ephemeral local storage a Pod can consume.",207
8.4 - Resource Management for Pods and Containers,Configurations for local ephemeral storage,"Configurations for local ephemeral storage Kubernetes supports two ways to configure local ephemeral storage on a node:Single filesystemTwo filesystemsIn this configuration, you place all different kinds of ephemeral local data(emptyDir volumes, writeable layers, container images, logs) into one filesystem.The most effective way to configure the kubelet means dedicating this filesystemto Kubernetes (kubelet) data.The kubelet also writesnode-level container logsand treats these similarly to ephemeral local storage.The kubelet writes logs to files inside its configured log directory (/var/logby default); and has a base directory for other locally stored data(/var/lib/kubelet by default).Typically, both /var/lib/kubelet and /var/log are on the system root filesystem,and the kubelet is designed with that layout in mind.Your node can have as many other filesystems, not used for Kubernetes,as you like.You have a filesystem on the node that you're using for ephemeral data thatcomes from running Pods: logs, and emptyDir volumes. You can use this filesystemfor other data (for example: system logs not related to Kubernetes); it can evenbe the root filesystem.The kubelet also writesnode-level container logsinto the first filesystem, and treats these similarly to ephemeral local storage.You also use a separate filesystem, backed by a different logical storage device.In this configuration, the directory where you tell the kubelet to placecontainer image layers and writeable layers is on this second filesystem.The first filesystem does not hold any image layers or writeable layers.Your node can have as many other filesystems, not used for Kubernetes,as you like. The kubelet can measure how much local storage it is using. It does this providedthat you have set up the node using one of the supported configurations for localephemeral storage. If you have a different configuration, then the kubelet does not apply resourcelimits for ephemeral local storage. Note: The kubelet tracks tmpfs emptyDir volumes as container memory use, ratherthan as local ephemeral storage. Note: The kubelet will only track the root filesystem for ephemeral storage. OS layouts that mount a separate disk to /var/lib/kubelet or /var/lib/containers will not report ephemeral storage correctly.",518
8.4 - Resource Management for Pods and Containers,Setting requests and limits for local ephemeral storage,"Setting requests and limits for local ephemeral storage You can specify ephemeral-storage for managing local ephemeral storage. Eachcontainer of a Pod can specify either or both of the following: spec.containers[].resources.limits.ephemeral-storagespec.containers[].resources.requests.ephemeral-storage Limits and requests for ephemeral-storage are measured in byte quantities.You can express storage as a plain integer or as a fixed-point number using one of these suffixes:E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,Mi, Ki. For example, the following quantities all represent roughly the same value: 128974848129e6129M123Mi Pay attention to the case of the suffixes. If you request 400m of ephemeral-storage, this is a requestfor 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (400Mi)or 400 megabytes (400M). In the following example, the Pod has two containers. Each container has a request of2GiB of local ephemeral storage. Each container has a limit of 4GiB of local ephemeralstorage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, anda limit of 8GiB of local ephemeral storage. 500Mi of that limit could beconsumed by the emptyDir volume. apiVersion: v1kind: Podmetadata:  name: frontendspec:  containers:  - name: app    image: images.my-company.example/app:v4    resources:      requests:        ephemeral-storage: ""2Gi""      limits:        ephemeral-storage: ""4Gi""    volumeMounts:    - name: ephemeral      mountPath: ""/tmp""  - name: log-aggregator    image: images.my-company.example/log-aggregator:v6    resources:      requests:        ephemeral-storage: ""2Gi""      limits:        ephemeral-storage: ""4Gi""    volumeMounts:    - name: ephemeral      mountPath: ""/tmp""  volumes:    - name: ephemeral      emptyDir:        sizeLimit: 500Mi",508
8.4 - Resource Management for Pods and Containers,How Pods with ephemeral-storage requests are scheduled,"How Pods with ephemeral-storage requests are scheduled When you create a Pod, the Kubernetes scheduler selects a node for the Pod torun on. Each node has a maximum amount of local ephemeral storage it can provide for Pods.For more information, seeNode Allocatable. The scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node.",88
8.4 - Resource Management for Pods and Containers,Ephemeral storage consumption management,"Ephemeral storage consumption management If the kubelet is managing local ephemeral storage as a resource, then thekubelet measures storage use in: emptyDir volumes, except tmpfs emptyDir volumesdirectories holding node-level logswriteable container layers If a Pod is using more ephemeral storage than you allow it to, the kubeletsets an eviction signal that triggers Pod eviction. For container-level isolation, if a container's writable layer and logusage exceeds its storage limit, the kubelet marks the Pod for eviction. For pod-level isolation the kubelet works out an overall Pod storage limit bysumming the limits for the containers in that Pod. In this case, if the sum ofthe local ephemeral storage usage from all containers and also the Pod's emptyDirvolumes exceeds the overall Pod storage limit, then the kubelet also marks the Podfor eviction. Caution:If the kubelet is not measuring local ephemeral storage, then a Podthat exceeds its local storage limit will not be evicted for breachinglocal storage resource limits.However, if the filesystem space for writeable container layers, node-level logs,or emptyDir volumes falls low, the nodetaints itself as short on local storageand this taint triggers eviction for any Pods that don't specifically tolerate the taint.See the supported configurationsfor ephemeral local storage. The kubelet supports different ways to measure Pod storage use: Periodic scanningFilesystem project quota The kubelet performs regular, scheduled checks that scan eachemptyDir volume, container log directory, and writeable container layer.The scan measures how much space is used.Note:In this mode, the kubelet does not track open file descriptorsfor deleted files.If you (or a container) create a file inside an emptyDir volume,something then opens that file, and you delete the file while it isstill open, then the inode for the deleted file stays until you closethat file but the kubelet does not categorize the space as in use.FEATURE STATE: Kubernetes v1.15 [alpha]Project quotas are an operating-system level feature for managingstorage use on filesystems. With Kubernetes, you can enable projectquotas for monitoring storage use. Make sure that the filesystembacking the emptyDir volumes, on the node, provides project quota support.For example, XFS and ext4fs offer project quotas.Note: Project quotas let you monitor storage use; they do not enforce limits.Kubernetes uses project IDs starting from 1048576. The IDs in use areregistered in /etc/projects and /etc/projid. If project IDs inthis range are used for other purposes on the system, those projectIDs must be registered in /etc/projects and /etc/projid so thatKubernetes does not use them.Quotas are faster and more accurate than directory scanning. When adirectory is assigned to a project, all files created under adirectory are created in that project, and the kernel merely has tokeep track of how many blocks are in use by files in that project.If a file is created and deleted, but has an open file descriptor,it continues to consume space. Quota tracking records that space accuratelywhereas directory scans overlook the storage used by deleted files.If you want to use project quotas, you should:Enable the LocalStorageCapacityIsolationFSQuotaMonitoring=truefeature gateusing the featureGates field in thekubelet configurationor the --feature-gates command line flag.Ensure that the root filesystem (or optional runtime filesystem)has project quotas enabled. All XFS filesystems support project quotas.For ext4 filesystems, you need to enable the project quota tracking featurewhile the filesystem is not mounted.# For ext4, with /dev/block-device not mountedsudo tune2fs -O project -Q prjquota /dev/block-deviceEnsure that the root filesystem (or optional runtime filesystem) ismounted with project quotas enabled. For both XFS and ext4fs, themount option is named prjquota.",873
8.4 - Resource Management for Pods and Containers,Extended resources,"Extended resources Extended resources are fully-qualified resource names outside thekubernetes.io domain. They allow cluster operators to advertise and users toconsume the non-Kubernetes-built-in resources. There are two steps required to use Extended Resources. First, the clusteroperator must advertise an Extended Resource. Second, users must request theExtended Resource in Pods.",79
8.4 - Resource Management for Pods and Containers,Node-level extended resources,"Node-level extended resources Node-level extended resources are tied to nodes. Device plugin managed resources See DevicePluginfor how to advertise device plugin managed resources on each node. Other resources To advertise a new node-level extended resource, the cluster operator cansubmit a PATCH HTTP request to the API server to specify the availablequantity in the status.capacity for a node in the cluster. After thisoperation, the node's status.capacity will include a new resource. Thestatus.allocatable field is updated automatically with the new resourceasynchronously by the kubelet. Because the scheduler uses the node's status.allocatable value whenevaluating Pod fitness, the scheduler only takes account of the new value afterthat asynchronous update. There may be a short delay between patching thenode capacity with a new resource and the time when the first Pod that requeststhe resource can be scheduled on that node. Example: Here is an example showing how to use curl to form an HTTP request thatadvertises five ""example.com/foo"" resources on node k8s-node-1 whose masteris k8s-master. curl --header ""Content-Type: application/json-patch+json"" \--request PATCH \--data '[{""op"": ""add"", ""path"": ""/status/capacity/example.com~1foo"", ""value"": ""5""}]' \http://k8s-master:8080/api/v1/nodes/k8s-node-1/status Note: In the preceding request, ~1 is the encoding for the character /in the patch path. The operation path value in JSON-Patch is interpreted as aJSON-Pointer. For more details, seeIETF RFC 6901, section 3.",365
8.4 - Resource Management for Pods and Containers,Cluster-level extended resources,"Cluster-level extended resources Cluster-level extended resources are not tied to nodes. They are usually managedby scheduler extenders, which handle the resource consumption and resource quota. You can specify the extended resources that are handled by scheduler extendersin scheduler configuration Example: The following configuration for a scheduler policy indicates that thecluster-level extended resource ""example.com/foo"" is handled by the schedulerextender. The scheduler sends a Pod to the scheduler extender only if the Pod requests""example.com/foo"".The ignoredByScheduler field specifies that the scheduler does not checkthe ""example.com/foo"" resource in its PodFitsResources predicate. {  ""kind"": ""Policy"",  ""apiVersion"": ""v1"",  ""extenders"": [    {      ""urlPrefix"":""<extender-endpoint>"",      ""bindVerb"": ""bind"",      ""managedResources"": [        {          ""name"": ""example.com/foo"",          ""ignoredByScheduler"": true        }      ]    }  ]}",231
8.4 - Resource Management for Pods and Containers,Consuming extended resources,"Consuming extended resources Users can consume extended resources in Pod specs like CPU and memory.The scheduler takes care of the resource accounting so that no more than theavailable amount is simultaneously allocated to Pods. The API server restricts quantities of extended resources to whole numbers.Examples of valid quantities are 3, 3000m and 3Ki. Examples ofinvalid quantities are 0.5 and 1500m. Note: Extended resources replace Opaque Integer Resources.Users can use any domain name prefix other than kubernetes.io which is reserved. To consume an extended resource in a Pod, include the resource name as a keyin the spec.containers[].resources.limits map in the container spec. Note: Extended resources cannot be overcommitted, so request and limitmust be equal if both are present in a container spec. A Pod is scheduled only if all of the resource requests are satisfied, includingCPU, memory and any extended resources. The Pod remains in the PENDING stateas long as the resource request cannot be satisfied. Example: The Pod below requests 2 CPUs and 1 ""example.com/foo"" (an extended resource). apiVersion: v1kind: Podmetadata:  name: my-podspec:  containers:  - name: my-container    image: myimage    resources:      requests:        cpu: 2        example.com/foo: 1      limits:        example.com/foo: 1",294
8.4 - Resource Management for Pods and Containers,PID limiting,PID limiting Process ID (PID) limits allow for the configuration of a kubeletto limit the number of PIDs that a given Pod can consume. SeePID Limiting for information.,41
8.4 - Resource Management for Pods and Containers,My Pods are pending with event message FailedScheduling,"My Pods are pending with event message FailedScheduling If the scheduler cannot find any node where a Pod can fit, the Pod remainsunscheduled until a place can be found. AnEvent is producedeach time the scheduler fails to find a place for the Pod. You can use kubectlto view the events for a Pod; for example: kubectl describe pod frontend | grep -A 9999999999 Events Events:  Type     Reason            Age   From               Message  ----     ------            ----  ----               -------  Warning  FailedScheduling  23s   default-scheduler  0/42 nodes available: insufficient cpu In the preceding example, the Pod named ""frontend"" fails to be scheduled due toinsufficient CPU resource on any node. Similar error messages can also suggestfailure due to insufficient memory (PodExceedsFreeMemory). In general, if a Podis pending with a message of this type, there are several things to try: Add more nodes to the cluster.Terminate unneeded Pods to make room for pending Pods.Check that the Pod is not larger than all the nodes. For example, if all thenodes have a capacity of cpu: 1, then a Pod with a request of cpu: 1.1 willnever be scheduled.Check for node taints. If most of your nodes are tainted, and the new Pod doesnot tolerate that taint, the scheduler only considers placements onto theremaining nodes that don't have that taint. You can check node capacities and amounts allocated with thekubectl describe nodes command. For example: kubectl describe nodes e2e-test-node-pool-4lw4 Name:            e2e-test-node-pool-4lw4[ ... lines removed for clarity ...]Capacity: cpu:                               2 memory:                            7679792Ki pods:                              110Allocatable: cpu:                               1800m memory:                            7474992Ki pods:                              110[ ... lines removed for clarity ...]Non-terminated Pods:        (5 in total)  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  ---------    ----                                  ------------  ----------  ---------------  -------------  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)Allocated resources:  (Total limits may be over 100 percent, i.e., overcommitted.)  CPU Requests    CPU Limits    Memory Requests    Memory Limits  ------------    ----------    ---------------    -------------  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%) In the preceding output, you can see that if a Pod requests more than 1.120 CPUsor more than 6.23Gi of memory, that Pod will not fit on the node. By looking at the “Pods” section, you can see which Pods are taking up space onthe node. The amount of resources available to Pods is less than the node capacity becausesystem daemons use a portion of the available resources. Within the Kubernetes API,each Node has a .status.allocatable field(see NodeStatusfor details). The .status.allocatable field describes the amount of resources that are availableto Pods on that node (for example: 15 virtual CPUs and 7538 MiB of memory).For more information on node allocatable resources in Kubernetes, seeReserve Compute Resources for System Daemons. You can configure resource quotasto limit the total amount of resources that a namespace can consume.Kubernetes enforces quotas for objects in particular namespace when there is aResourceQuota in that namespace.For example, if you assign specific namespaces to different teams, youcan add ResourceQuotas into those namespaces. Setting resource quotas helps toprevent one team from using so much of any resource that this over-use affects other teams. You should also consider what access you grant to that namespace:full write access to a namespace allows someone with that access to remove anyresource, including a configured ResourceQuota.",1088
8.4 - Resource Management for Pods and Containers,My container is terminated,"My container is terminated Your container might get terminated because it is resource-starved. To checkwhether a container is being killed because it is hitting a resource limit, callkubectl describe pod on the Pod of interest: kubectl describe pod simmemleak-hra99 The output is similar to: Name:                           simmemleak-hra99Namespace:                      defaultImage(s):                       saadali/simmemleakNode:                           kubernetes-node-tf0f/10.240.216.66Labels:                         name=simmemleakStatus:                         RunningReason:Message:IP:                             10.244.2.75Containers:  simmemleak:    Image:  saadali/simmemleak:latest    Limits:      cpu:          100m      memory:       50Mi    State:          Running      Started:      Tue, 07 Jul 2019 12:54:41 -0700    Last State:     Terminated      Reason:       OOMKilled      Exit Code:    137      Started:      Fri, 07 Jul 2019 12:54:30 -0700      Finished:     Fri, 07 Jul 2019 12:54:33 -0700    Ready:          False    Restart Count:  5Conditions:  Type      Status  Ready     FalseEvents:  Type    Reason     Age   From               Message  ----    ------     ----  ----               -------  Normal  Scheduled  42s   default-scheduler  Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f  Normal  Pulled     41s   kubelet            Container image ""saadali/simmemleak:latest"" already present on machine  Normal  Created    41s   kubelet            Created container simmemleak  Normal  Started    40s   kubelet            Started container simmemleak  Normal  Killing    32s   kubelet            Killing container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill Pod In the preceding example, the Restart Count: 5 indicates that the simmemleakcontainer in the Pod was terminated and restarted five times (so far).The OOMKilled reason shows that the container tried to use more memory than its limit. Your next step might be to check the application code for a memory leak. If youfind that the application is behaving how you expect, consider setting a highermemory limit (and possibly request) for that container. Get hands-on experience assigning Memory resources to containers and Pods.Get hands-on experience assigning CPU resources to containers and Pods.Read how the API reference defines a containerand its resource requirementsRead about project quotas in XFSRead more about the kube-scheduler configuration reference (v1beta3)Read more about Quality of Service classes for Pods",625
8.5 - Organizing Cluster Access Using kubeconfig Files,default,"Use kubeconfig files to organize information about clusters, users, namespaces, andauthentication mechanisms. The kubectl command-line tool uses kubeconfig files tofind the information it needs to choose a cluster and communicate with the API serverof a cluster. Note: A file that is used to configure access to clusters is calleda kubeconfig file. This is a generic way of referring to configuration files.It does not mean that there is a file named kubeconfig. Warning: Only use kubeconfig files from trusted sources. Using a specially-crafted kubeconfig file could result in malicious code execution or file exposure.If you must use an untrusted kubeconfig file, inspect it carefully first, much as you would a shell script. By default, kubectl looks for a file named config in the $HOME/.kube directory.You can specify other kubeconfig files by setting the KUBECONFIG environmentvariable or by setting the--kubeconfig flag. For step-by-step instructions on creating and specifying kubeconfig files, seeConfigure Access to Multiple Clusters.",252
8.5 - Organizing Cluster Access Using kubeconfig Files,"Supporting multiple clusters, users, and authentication mechanisms","Supporting multiple clusters, users, and authentication mechanisms Suppose you have several clusters, and your users and components authenticatein a variety of ways. For example: A running kubelet might authenticate using certificates.A user might authenticate using tokens.Administrators might have sets of certificates that they provide to individual users. With kubeconfig files, you can organize your clusters, users, and namespaces.You can also define contexts to quickly and easily switch betweenclusters and namespaces.",102
8.5 - Organizing Cluster Access Using kubeconfig Files,Context,"Context A context element in a kubeconfig file is used to group access parametersunder a convenient name. Each context has three parameters: cluster, namespace, and user.By default, the kubectl command-line tool uses parameters fromthe current context to communicate with the cluster. To choose the current context: kubectl config use-context",74
8.5 - Organizing Cluster Access Using kubeconfig Files,The KUBECONFIG environment variable,"The KUBECONFIG environment variable The KUBECONFIG environment variable holds a list of kubeconfig files.For Linux and Mac, the list is colon-delimited. For Windows, the listis semicolon-delimited. The KUBECONFIG environment variable is notrequired. If the KUBECONFIG environment variable doesn't exist,kubectl uses the default kubeconfig file, $HOME/.kube/config. If the KUBECONFIG environment variable does exist, kubectl usesan effective configuration that is the result of merging the fileslisted in the KUBECONFIG environment variable.",141
8.5 - Organizing Cluster Access Using kubeconfig Files,Merging kubeconfig files,"Merging kubeconfig files To see your configuration, enter this command: kubectl config view As described previously, the output might be from a single kubeconfig file,or it might be the result of merging several kubeconfig files. Here are the rules that kubectl uses when it merges kubeconfig files: If the --kubeconfig flag is set, use only the specified file. Do not merge.Only one instance of this flag is allowed.Otherwise, if the KUBECONFIG environment variable is set, use it as alist of files that should be merged.Merge the files listed in the KUBECONFIG environment variableaccording to these rules:Ignore empty filenames.Produce errors for files with content that cannot be deserialized.The first file to set a particular value or map key wins.Never change the value or map key.Example: Preserve the context of the first file to set current-context.Example: If two files specify a red-user, use only values from the first file's red-user.Even if the second file has non-conflicting entries under red-user, discard them.For an example of setting the KUBECONFIG environment variable, seeSetting the KUBECONFIG environment variable.Otherwise, use the default kubeconfig file, $HOME/.kube/config, with no merging.Determine the context to use based on the first hit in this chain:Use the --context command-line flag if it exists.Use the current-context from the merged kubeconfig files.An empty context is allowed at this point.Determine the cluster and user. At this point, there might or might not be a context.Determine the cluster and user based on the first hit in this chain,which is run twice: once for user and once for cluster:Use a command-line flag if it exists: --user or --cluster.If the context is non-empty, take the user or cluster from the context.The user and cluster can be empty at this point.Determine the actual cluster information to use. At this point, there might ormight not be cluster information.Build each piece of the cluster information based on this chain; the first hit wins:Use command line flags if they exist: --server, --certificate-authority, --insecure-skip-tls-verify.If any cluster information attributes exist from the merged kubeconfig files, use them.If there is no server location, fail.Determine the actual user information to use. Build user information using the samerules as cluster information, except allow only one authenticationtechnique per user:Use command line flags if they exist: --client-certificate, --client-key, --username, --password, --token.Use the user fields from the merged kubeconfig files.If there are two conflicting techniques, fail.For any information still missing, use default values and potentiallyprompt for authentication information.",649
8.5 - Organizing Cluster Access Using kubeconfig Files,File references,"File references File and path references in a kubeconfig file are relative to the location of the kubeconfig file.File references on the command line are relative to the current working directory.In $HOME/.kube/config, relative paths are stored relatively, and absolute pathsare stored absolutely.",64
8.5 - Organizing Cluster Access Using kubeconfig Files,Proxy,"Proxy You can configure kubectl to use a proxy per cluster using proxy-url in your kubeconfig file, like this: apiVersion: v1kind: Configclusters:- cluster:    proxy-url: http://proxy.example.org:3128    server: https://k8s.example.org/k8s/clusters/c-xxyyzz  name: developmentusers:- name: developercontexts:- context:  name: development Configure Access to Multiple Clusterskubectl config",113
8.6 - Resource Management for Windows nodes,default,"This page outlines the differences in how resources are managed between Linux and Windows. On Linux nodes, cgroups are usedas a pod boundary for resource control. Containers are created within that boundaryfor network, process and file system isolation. The Linux cgroup APIs can be used togather CPU, I/O, and memory use statistics. In contrast, Windows uses a job object per container with a system namespace filterto contain all processes in a container and provide logical isolation from thehost.(Job objects are a Windows process isolation mechanism and are different fromwhat Kubernetes refers to as a Job). There is no way to run a Windows container without the namespace filtering inplace. This means that system privileges cannot be asserted in the context of thehost, and thus privileged containers are not available on Windows.Containers cannot assume an identity from the host because the Security Account Manager(SAM) is separate.",183
8.6 - Resource Management for Windows nodes,Memory management,"Memory management Windows does not have an out-of-memory process killer as Linux does. Windows alwaystreats all user-mode memory allocations as virtual, and pagefiles are mandatory. Windows nodes do not overcommit memory for processes. Thenet effect is that Windows won't reach out of memory conditions the same way Linuxdoes, and processes page to disk instead of being subject to out of memory (OOM)termination. If memory is over-provisioned and all physical memory is exhausted,then paging can slow down performance.",110
8.6 - Resource Management for Windows nodes,CPU management,"CPU management Windows can limit the amount of CPU time allocated for different processes but cannotguarantee a minimum amount of CPU time. On Windows, the kubelet supports a command-line flag to set thescheduling priority of thekubelet process: --windows-priorityclass. This flag allows the kubelet process to getmore CPU time slices when compared to other processes running on the Windows host.More information on the allowable values and their meaning is available atWindows Priority Classes.To ensure that running Pods do not starve the kubelet of CPU cycles, set this flag to ABOVE_NORMAL_PRIORITY_CLASS or above.",136
8.6 - Resource Management for Windows nodes,Resource reservation,"Resource reservation To account for memory and CPU used by the operating system, the container runtime, and byKubernetes host processes such as the kubelet, you can (and should) reservememory and CPU resources with the --kube-reserved and/or --system-reserved kubelet flags.On Windows these values are only used to calculate the node'sallocatable resources. Caution:As you deploy workloads, set resource memory and CPU limits on containers.This also subtracts from NodeAllocatable and helps the cluster-wide scheduler in determining which pods to place on which nodes.Scheduling pods without limits may over-provision the Windows nodes and in extremecases can cause the nodes to become unhealthy. On Windows, a good practice is to reserve at least 2GiB of memory. To determine how much CPU to reserve,identify the maximum pod density for each node and monitor the CPU usage ofthe system services running there, then choose a value that meets your workload needs.",209
9.1 - Overview of Cloud Native Security,default,"A model for thinking about Kubernetes security in the context of Cloud Native security. This overview defines a model for thinking about Kubernetes security in the context of Cloud Native security. Warning: This container security model provides suggestions, not proven information security policies.",54
9.1 - Overview of Cloud Native Security,The 4C's of Cloud Native security,"The 4C's of Cloud Native security You can think about security in layers. The 4C's of Cloud Native security are Cloud,Clusters, Containers, and Code. Note: This layered approach augments the defense in depthcomputing approach to security, which is widely regarded as a best practice for securingsoftware systems. The 4C's of Cloud Native Security Each layer of the Cloud Native security model builds upon the next outermost layer.The Code layer benefits from strong base (Cloud, Cluster, Container) security layers.You cannot safeguard against poor security standards in the base layers by addressingsecurity at the Code level.",126
9.1 - Overview of Cloud Native Security,Cloud,"Cloud In many ways, the Cloud (or co-located servers, or the corporate datacenter) is thetrusted computing baseof a Kubernetes cluster. If the Cloud layer is vulnerable (orconfigured in a vulnerable way) then there is no guarantee that the components builton top of this base are secure. Each cloud provider makes security recommendationsfor running workloads securely in their environment.",83
9.1 - Overview of Cloud Native Security,Cloud provider security,"Cloud provider security If you are running a Kubernetes cluster on your own hardware or a different cloud provider,consult your documentation for security best practices.Here are links to some of the popular cloud providers' security documentation: Cloud provider securityIaaS ProviderLinkAlibaba Cloudhttps://www.alibabacloud.com/trust-centerAmazon Web Serviceshttps://aws.amazon.com/securityGoogle Cloud Platformhttps://cloud.google.com/securityHuawei Cloudhttps://www.huaweicloud.com/securecenter/overallsafetyIBM Cloudhttps://www.ibm.com/cloud/securityMicrosoft Azurehttps://docs.microsoft.com/en-us/azure/security/azure-securityOracle Cloud Infrastructurehttps://www.oracle.com/securityVMware vSpherehttps://www.vmware.com/security/hardening-guides",189
9.1 - Overview of Cloud Native Security,Infrastructure security,"Infrastructure security Suggestions for securing your infrastructure in a Kubernetes cluster: Infrastructure securityArea of Concern for Kubernetes InfrastructureRecommendationNetwork access to API Server (Control plane)All access to the Kubernetes control plane is not allowed publicly on the internet and is controlled by network access control lists restricted to the set of IP addresses needed to administer the cluster.Network access to Nodes (nodes)Nodes should be configured to only accept connections (via network access control lists) from the control plane on the specified ports, and accept connections for services in Kubernetes of type NodePort and LoadBalancer. If possible, these nodes should not be exposed on the public internet entirely.Kubernetes access to Cloud Provider APIEach cloud provider needs to grant a different set of permissions to the Kubernetes control plane and nodes. It is best to provide the cluster with cloud provider access that follows the principle of least privilege for the resources it needs to administer. The Kops documentation provides information about IAM policies and roles.Access to etcdAccess to etcd (the datastore of Kubernetes) should be limited to the control plane only. Depending on your configuration, you should attempt to use etcd over TLS. More information can be found in the etcd documentation.etcd EncryptionWherever possible it's a good practice to encrypt all storage at rest, and since etcd holds the state of the entire cluster (including Secrets) its disk should especially be encrypted at rest.",312
9.1 - Overview of Cloud Native Security,Components in the cluster (your application),"Components in the cluster (your application) Depending on the attack surface of your application, you may want to focus on specificaspects of security. For example: If you are running a service (Service A) that is criticalin a chain of other resources and a separate workload (Service B) which isvulnerable to a resource exhaustion attack, then the risk of compromising Service Ais high if you do not limit the resources of Service B. The following table listsareas of security concerns and recommendations for securing workloads running in Kubernetes: Area of Concern for Workload SecurityRecommendationRBAC Authorization (Access to the Kubernetes API)https://kubernetes.io/docs/reference/access-authn-authz/rbac/Authenticationhttps://kubernetes.io/docs/concepts/security/controlling-access/Application secrets management (and encrypting them in etcd at rest)https://kubernetes.io/docs/concepts/configuration/secret/https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/Ensuring that pods meet defined Pod Security Standardshttps://kubernetes.io/docs/concepts/security/pod-security-standards/#policy-instantiationQuality of Service (and Cluster resource management)https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/Network Policieshttps://kubernetes.io/docs/concepts/services-networking/network-policies/TLS for Kubernetes Ingresshttps://kubernetes.io/docs/concepts/services-networking/ingress/#tls",373
9.1 - Overview of Cloud Native Security,Container,"Container Container security is outside the scope of this guide. Here are general recommendations andlinks to explore this topic: Area of Concern for ContainersRecommendationContainer Vulnerability Scanning and OS Dependency SecurityAs part of an image build step, you should scan your containers for known vulnerabilities.Image Signing and EnforcementSign container images to maintain a system of trust for the content of your containers.Disallow privileged usersWhen constructing containers, consult your documentation for how to create users inside of the containers that have the least level of operating system privilege necessary in order to carry out the goal of the container.Use container runtime with stronger isolationSelect container runtime classes that provide stronger isolation.",134
9.1 - Overview of Cloud Native Security,Code,"Code Application code is one of the primary attack surfaces over which you have the most control.While securing application code is outside of the Kubernetes security topic, hereare recommendations to protect application code:",41
9.1 - Overview of Cloud Native Security,Code security,"Code security Code securityArea of Concern for CodeRecommendationAccess over TLS onlyIf your code needs to communicate by TCP, perform a TLS handshake with the client ahead of time. With the exception of a few cases, encrypt everything in transit. Going one step further, it's a good idea to encrypt network traffic between services. This can be done through a process known as mutual TLS authentication or mTLS which performs a two sided verification of communication between two certificate holding services.Limiting port ranges of communicationThis recommendation may be a bit self-explanatory, but wherever possible you should only expose the ports on your service that are absolutely essential for communication or metric gathering.3rd Party Dependency SecurityIt is a good practice to regularly scan your application's third party libraries for known security vulnerabilities. Each programming language has a tool for performing this check automatically.Static Code AnalysisMost languages provide a way for a snippet of code to be analyzed for any potentially unsafe coding practices. Whenever possible you should perform checks using automated tooling that can scan codebases for common security errors. Some of the tools can be found at: https://owasp.org/www-community/Source_Code_Analysis_ToolsDynamic probing attacksThere are a few automated tools that you can run against your service to try some of the well known service attacks. These include SQL injection, CSRF, and XSS. One of the most popular dynamic analysis tools is the OWASP Zed Attack proxy tool. Learn about related Kubernetes security topics: Pod security standardsNetwork policies for PodsControlling Access to the Kubernetes APISecuring your clusterData encryption in transit for the control planeData encryption at restSecrets in KubernetesRuntime class",353
9.2 - Pod Security Standards,default,"A detailed look at the different policy levels defined in the Pod Security Standards. The Pod Security Standards define three different policies to broadly cover the securityspectrum. These policies are cumulative and range from highly-permissive to highly-restrictive.This guide outlines the requirements of each policy. ProfileDescriptionPrivilegedUnrestricted policy, providing the widest possible level of permissions. This policy allows for known privilege escalations.BaselineMinimally restrictive policy which prevents known privilege escalations. Allows the default (minimally specified) Pod configuration.RestrictedHeavily restricted policy, following current Pod hardening best practices.",125
9.2 - Pod Security Standards,Privileged,"Privileged The Privileged policy is purposely-open, and entirely unrestricted. This type of policy istypically aimed at system- and infrastructure-level workloads managed by privileged, trusted users. The Privileged policy is defined by an absence of restrictions. Allow-by-defaultmechanisms (such as gatekeeper) may be Privileged by default. In contrast, for a deny-by-default mechanism (such as PodSecurity Policy) the Privileged policy should disable all restrictions.",98
9.2 - Pod Security Standards,Baseline,"Baseline The Baseline policy is aimed at ease of adoption for common containerized workloads whilepreventing known privilege escalations. This policy is targeted at application operators anddevelopers of non-critical applications. The following listed controls should beenforced/disallowed: Note: In this table, wildcards (*) indicate all elements in a list. For example,spec.containers[*].securityContext refers to the Security Context object for all definedcontainers. If any of the listed containers fails to meet the requirements, the entire pod willfail validation. Baseline policy specificationControlPolicyHostProcessWindows pods offer the ability to run HostProcess containers which enables privileged access to the Windows node. Privileged access to the host is disallowed in the baseline policy.FEATURE STATE: Kubernetes v1.23 [beta]Restricted Fieldsspec.securityContext.windowsOptions.hostProcessspec.containers[*].securityContext.windowsOptions.hostProcessspec.initContainers[*].securityContext.windowsOptions.hostProcessspec.ephemeralContainers[*].securityContext.windowsOptions.hostProcessAllowed ValuesUndefined/nilfalseHost NamespacesSharing the host namespaces must be disallowed.Restricted Fieldsspec.hostNetworkspec.hostPIDspec.hostIPCAllowed ValuesUndefined/nilfalsePrivileged ContainersPrivileged Pods disable most security mechanisms and must be disallowed.Restricted Fieldsspec.containers[*].securityContext.privilegedspec.initContainers[*].securityContext.privilegedspec.ephemeralContainers[*].securityContext.privilegedAllowed ValuesUndefined/nilfalseCapabilitiesAdding additional capabilities beyond those listed below must be disallowed.Restricted Fieldsspec.containers[*].securityContext.capabilities.addspec.initContainers[*].securityContext.capabilities.addspec.ephemeralContainers[*].securityContext.capabilities.addAllowed ValuesUndefined/nilAUDIT_WRITECHOWNDAC_OVERRIDEFOWNERFSETIDKILLMKNODNET_BIND_SERVICESETFCAPSETGIDSETPCAPSETUIDSYS_CHROOTHostPath VolumesHostPath volumes must be forbidden.Restricted Fieldsspec.volumes[*].hostPathAllowed ValuesUndefined/nilHost PortsHostPorts should be disallowed entirely (recommended) or restricted to a known listRestricted Fieldsspec.containers[*].ports[*].hostPortspec.initContainers[*].ports[*].hostPortspec.ephemeralContainers[*].ports[*].hostPortAllowed ValuesUndefined/nilKnown list (not supported by the built-in Pod Security Admission controller)0AppArmorOn supported hosts, the runtime/default AppArmor profile is applied by default. The baseline policy should prevent overriding or disabling the default AppArmor profile, or restrict overrides to an allowed set of profiles.Restricted Fieldsmetadata.annotations[""container.apparmor.security.beta.kubernetes.io/*""]Allowed ValuesUndefined/nilruntime/defaultlocalhost/*SELinuxSetting the SELinux type is restricted, and setting a custom SELinux user or role option is forbidden.Restricted Fieldsspec.securityContext.seLinuxOptions.typespec.containers[*].securityContext.seLinuxOptions.typespec.initContainers[*].securityContext.seLinuxOptions.typespec.ephemeralContainers[*].securityContext.seLinuxOptions.typeAllowed ValuesUndefined/""""container_tcontainer_init_tcontainer_kvm_tRestricted Fieldsspec.securityContext.seLinuxOptions.userspec.containers[*].securityContext.seLinuxOptions.userspec.initContainers[*].securityContext.seLinuxOptions.userspec.ephemeralContainers[*].securityContext.seLinuxOptions.userspec.securityContext.seLinuxOptions.rolespec.containers[*].securityContext.seLinuxOptions.rolespec.initContainers[*].securityContext.seLinuxOptions.rolespec.ephemeralContainers[*].securityContext.seLinuxOptions.roleAllowed ValuesUndefined/""""/proc Mount TypeThe default /proc masks are set up to reduce attack surface, and should be required.Restricted Fieldsspec.containers[*].securityContext.procMountspec.initContainers[*].securityContext.procMountspec.ephemeralContainers[*].securityContext.procMountAllowed ValuesUndefined/nilDefaultSeccompSeccomp profile must not be explicitly set to Unconfined.Restricted Fieldsspec.securityContext.seccompProfile.typespec.containers[*].securityContext.seccompProfile.typespec.initContainers[*].securityContext.seccompProfile.typespec.ephemeralContainers[*].securityContext.seccompProfile.typeAllowed ValuesUndefined/nilRuntimeDefaultLocalhostSysctlsSysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed ""safe"" subset. A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node.Restricted Fieldsspec.securityContext.sysctls[*].nameAllowed ValuesUndefined/nilkernel.shm_rmid_forcednet.ipv4.ip_local_port_rangenet.ipv4.ip_unprivileged_port_startnet.ipv4.tcp_syncookiesnet.ipv4.ping_group_range",1238
9.2 - Pod Security Standards,Restricted,"Restricted The Restricted policy is aimed at enforcing current Pod hardening best practices, at theexpense of some compatibility. It is targeted at operators and developers of security-criticalapplications, as well as lower-trust users. The following listed controls should beenforced/disallowed: Note: In this table, wildcards (*) indicate all elements in a list. For example,spec.containers[*].securityContext refers to the Security Context object for all definedcontainers. If any of the listed containers fails to meet the requirements, the entire pod willfail validation. Restricted policy specificationControlPolicyEverything from the baseline profile.Volume TypesThe restricted policy only permits the following volume types.Restricted Fieldsspec.volumes[*]Allowed ValuesEvery item in the spec.volumes[*] list must set one of the following fields to a non-null value:spec.volumes[*].configMapspec.volumes[*].csispec.volumes[*].downwardAPIspec.volumes[*].emptyDirspec.volumes[*].ephemeralspec.volumes[*].persistentVolumeClaimspec.volumes[*].projectedspec.volumes[*].secretPrivilege Escalation (v1.8+)Privilege escalation (such as via set-user-ID or set-group-ID file mode) should not be allowed. This is Linux only policy in v1.25+ (spec.os.name != windows)Restricted Fieldsspec.containers[*].securityContext.allowPrivilegeEscalationspec.initContainers[*].securityContext.allowPrivilegeEscalationspec.ephemeralContainers[*].securityContext.allowPrivilegeEscalationAllowed ValuesfalseRunning as Non-rootContainers must be required to run as non-root users.Restricted Fieldsspec.securityContext.runAsNonRootspec.containers[*].securityContext.runAsNonRootspec.initContainers[*].securityContext.runAsNonRootspec.ephemeralContainers[*].securityContext.runAsNonRootAllowed ValuestrueThe container fields may be undefined/nil if the pod-levelspec.securityContext.runAsNonRoot is set to true.Running as Non-root user (v1.23+)Containers must not set runAsUser to 0Restricted Fieldsspec.securityContext.runAsUserspec.containers[*].securityContext.runAsUserspec.initContainers[*].securityContext.runAsUserspec.ephemeralContainers[*].securityContext.runAsUserAllowed Valuesany non-zero valueundefined/nullSeccomp (v1.19+)Seccomp profile must be explicitly set to one of the allowed values. Both the Unconfined profile and the absence of a profile are prohibited. This is Linux only policy in v1.25+ (spec.os.name != windows)Restricted Fieldsspec.securityContext.seccompProfile.typespec.containers[*].securityContext.seccompProfile.typespec.initContainers[*].securityContext.seccompProfile.typespec.ephemeralContainers[*].securityContext.seccompProfile.typeAllowed ValuesRuntimeDefaultLocalhostThe container fields may be undefined/nil if the pod-levelspec.securityContext.seccompProfile.type field is set appropriately.Conversely, the pod-level field may be undefined/nil if _all_ container-level fields are set.Capabilities (v1.22+)Containers must drop ALL capabilities, and are only permitted to add backthe NET_BIND_SERVICE capability. This is Linux only policy in v1.25+ (.spec.os.name != ""windows"")Restricted Fieldsspec.containers[*].securityContext.capabilities.dropspec.initContainers[*].securityContext.capabilities.dropspec.ephemeralContainers[*].securityContext.capabilities.dropAllowed ValuesAny list of capabilities that includes ALLRestricted Fieldsspec.containers[*].securityContext.capabilities.addspec.initContainers[*].securityContext.capabilities.addspec.ephemeralContainers[*].securityContext.capabilities.addAllowed ValuesUndefined/nilNET_BIND_SERVICE",938
9.2 - Pod Security Standards,Policy Instantiation,"Policy Instantiation Decoupling policy definition from policy instantiation allows for a common understanding andconsistent language of policies across clusters, independent of the underlying enforcementmechanism. As mechanisms mature, they will be defined below on a per-policy basis. The methods of enforcementof individual policies are not defined here. Pod Security Admission Controller Privileged namespaceBaseline namespaceRestricted namespace",76
9.2 - Pod Security Standards,Alternatives,"Alternatives Note:This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information. Other alternatives for enforcing policies are being developed in the Kubernetes ecosystem, such as: KubewardenKyvernoOPA Gatekeeper",89
9.2 - Pod Security Standards,Pod OS field,"Pod OS field Kubernetes lets you use nodes that run either Linux or Windows. You can mix both kinds ofnode in one cluster.Windows in Kubernetes has some limitations and differentiators from Linux-basedworkloads. Specifically, many of the Pod securityContext fieldshave no effect on Windows. Note: Kubelets prior to v1.24 don't enforce the pod OS field, and if a cluster has nodes on versions earlier than v1.24 the restricted policies should be pinned to a version prior to v1.25.",114
9.2 - Pod Security Standards,Restricted Pod Security Standard changes,"Restricted Pod Security Standard changes Another important change, made in Kubernetes v1.25 is that the restricted Pod securityhas been updated to use the pod.spec.os.name field. Based on the OS name, certain policies that are specificto a particular OS can be relaxed for the other OS.",66
9.2 - Pod Security Standards,Why isn't there a profile between privileged and baseline?,"Why isn't there a profile between privileged and baseline? The three profiles defined here have a clear linear progression from most secure (restricted) to leastsecure (privileged), and cover a broad set of workloads. Privileges required above the baselinepolicy are typically very application specific, so we do not offer a standard profile in thisniche. This is not to say that the privileged profile should always be used in this case, but thatpolicies in this space need to be defined on a case-by-case basis. SIG Auth may reconsider this position in the future, should a clear need for other profiles arise.",127
9.2 - Pod Security Standards,What's the difference between a security profile and a security context?,"What's the difference between a security profile and a security context? Security Contexts configure Pods andContainers at runtime. Security contexts are defined as part of the Pod and container specificationsin the Pod manifest, and represent parameters to the container runtime. Security profiles are control plane mechanisms to enforce specific settings in the Security Context,as well as other related parameters outside the Security Context. As of July 2021,Pod Security Policies are deprecated in favor of thebuilt-in Pod Security Admission Controller.",98
9.2 - Pod Security Standards,What about sandboxed Pods?,"What about sandboxed Pods? There is not currently an API standard that controls whether a Pod is considered sandboxed ornot. Sandbox Pods may be identified by the use of a sandboxed runtime (such as gVisor or KataContainers), but there is no standard definition of what a sandboxed runtime is. The protections necessary for sandboxed workloads can differ from others. For example, the need torestrict privileged permissions is lessened when the workload is isolated from the underlyingkernel. This allows for workloads requiring heightened permissions to still be isolated. Additionally, the protection of sandboxed workloads is highly dependent on the method ofsandboxing. As such, no single recommended profile is recommended for all sandboxed workloads.",151
9.3 - Service Accounts,default,"Learn about ServiceAccount objects in Kubernetes. This page introduces the ServiceAccount object in Kubernetes, providinginformation about how service accounts work, use cases, limitations,alternatives, and links to resources for additional guidance.",48
9.3 - Service Accounts,What are service accounts?,"What are service accounts? A service account is a type of non-human account that, in Kubernetes, providesa distinct identity in a Kubernetes cluster. Application Pods, systemcomponents, and entities inside and outside the cluster can use a specificServiceAccount's credentials to identify as that ServiceAccount. This identityis useful in various situations, including authenticating to the API server orimplementing identity-based security policies. Service accounts exist as ServiceAccount objects in the API server. Serviceaccounts have the following properties: Namespaced: Each service account is bound to a Kubernetesnamespace. Every namespacegets a default ServiceAccount upon creation.Lightweight: Service accounts exist in the cluster and aredefined in the Kubernetes API. You can quickly create service accounts toenable specific tasks.Portable: A configuration bundle for a complex containerized workloadmight include service account definitions for the system's components. Thelightweight nature of service accounts and the namespaced identities makethe configurations portable. Service accounts are different from user accounts, which are authenticatedhuman users in the cluster. By default, user accounts don't exist in the KubernetesAPI server; instead, the API server treats user identities as opaquedata. You can authenticate as a user account using multiple methods. SomeKubernetes distributions might add custom extension APIs to represent useraccounts in the API server. Comparison between service accounts and usersDescriptionServiceAccountUser or groupLocationKubernetes API (ServiceAccount object)ExternalAccess controlKubernetes RBAC or other authorization mechanismsKubernetes RBAC or other identity and access management mechanismsIntended useWorkloads, automationPeople",348
9.3 - Service Accounts,Default service accounts,"Default service accounts When you create a cluster, Kubernetes automatically creates a ServiceAccountobject named default for every namespace in your cluster. The defaultservice accounts in each namespace get no permissions by default other than thedefault API discovery permissionsthat Kubernetes grants to all authenticated principals if role-based access control (RBAC) is enabled.If you delete the default ServiceAccount object in a namespace, thecontrol planereplaces it with a new one. If you deploy a Pod in a namespace, and you don'tmanually assign a ServiceAccount to the Pod, Kubernetesassigns the default ServiceAccount for that namespace to the Pod.",135
9.3 - Service Accounts,Use cases for Kubernetes service accounts,"Use cases for Kubernetes service accounts As a general guideline, you can use service accounts to provide identities inthe following scenarios: Your Pods need to communicate with the Kubernetes API server, for example insituations such as the following:Providing read-only access to sensitive information stored in Secrets.Granting cross-namespace access, such as allowing aPod in namespace example to read, list, and watch for Lease objects inthe kube-node-lease namespace.Your Pods need to communicate with an external service. For example, aworkload Pod requires an identity for a commercially available cloud API,and the commercial provider allows configuring a suitable trust relationship.Authenticating to a private image registry using an imagePullSecret.An external service needs to communicate with the Kubernetes API server. Forexample, authenticating to the cluster as part of a CI/CD pipeline.You use third-party security software in your cluster that relies on theServiceAccount identity of different Pods to group those Pods into differentcontexts.",219
9.3 - Service Accounts,How to use service accounts,"How to use service accounts To use a Kubernetes service account, you do the following: Create a ServiceAccount object using a Kubernetesclient like kubectl or a manifest that defines the object.Grant permissions to the ServiceAccount object using an authorizationmechanism such asRBAC.Assign the ServiceAccount object to Pods during Pod creation.If you're using the identity from an external service,retrieve the ServiceAccount token and use it from thatservice instead. For instructions, refer toConfigure Service Accounts for Pods.",114
9.3 - Service Accounts,Grant permissions to a ServiceAccount,"Grant permissions to a ServiceAccount You can use the built-in Kubernetesrole-based access control (RBAC)mechanism to grant the minimum permissions required by each service account.You create a role, which grants access, and then bind the role to yourServiceAccount. RBAC lets you define a minimum set of permissions so that theservice account permissions follow the principle of least privilege. Pods thatuse that service account don't get more permissions than are required tofunction correctly. For instructions, refer toServiceAccount permissions.",110
9.3 - Service Accounts,Cross-namespace access using a ServiceAccount,"Cross-namespace access using a ServiceAccount You can use RBAC to allow service accounts in one namespace to perform actionson resources in a different namespace in the cluster. For example, consider ascenario where you have a service account and Pod in the dev namespace andyou want your Pod to see Jobs running in the maintenance namespace. You couldcreate a Role object that grants permissions to list Job objects. Then,you'd create a RoleBinding object in the maintenance namespace to bind theRole to the ServiceAccount object. Now, Pods in the dev namespace can listJob objects in the maintenance namespace using that service account.",125
9.3 - Service Accounts,Assign a ServiceAccount to a Pod,"Assign a ServiceAccount to a Pod To assign a ServiceAccount to a Pod, you set the spec.serviceAccountNamefield in the Pod specification. Kubernetes then automatically provides thecredentials for that ServiceAccount to the Pod. In v1.22 and later, Kubernetesgets a short-lived, automatically rotating token using the TokenRequestAPI and mounts the token as aprojected volume. By default, Kubernetes provides the Podwith the credentials for an assigned ServiceAccount, whether that is thedefault ServiceAccount or a custom ServiceAccount that you specify. To prevent Kubernetes from automatically injectingcredentials for a specified ServiceAccount or the default ServiceAccount, set theautomountServiceAccountToken field in your Pod specification to false. In versions earlier than 1.22, Kubernetes provides a long-lived, static tokento the Pod as a Secret.",188
9.3 - Service Accounts,Manually retrieve ServiceAccount credentials,"Manually retrieve ServiceAccount credentials If you need the credentials for a ServiceAccount to mount in a non-standardlocation, or for an audience that isn't the API server, use one of thefollowing methods: TokenRequest API(recommended): Request a short-lived service account token from withinyour own application code. The token expires automatically and can rotateupon expiration.If you have a legacy application that is not aware of Kubernetes, youcould use a sidecar container within the same pod to fetch these tokensand make them available to the application workload.Token Volume Projection(also recommended): In Kubernetes v1.20 and later, use the Pod specification totell the kubelet to add the service account token to the Pod as aprojected volume. Projected tokens expire automatically, and the kubeletrotates the token before it expires.Service Account Token Secrets(not recommended): You can mount service account tokens as KubernetesSecrets in Pods. These tokens don't expire and don't rotate. This methodis not recommended, especially at scale, because of the risks associatedwith static, long-lived credentials. In Kubernetes v1.24 and later, theLegacyServiceAccountTokenNoAutoGeneration feature gateprevents Kubernetes from automatically creating these tokens forServiceAccounts. LegacyServiceAccountTokenNoAutoGeneration is enabledby default; in other words, Kubernetes does not create these tokens.",304
9.3 - Service Accounts,Authenticating service account credentials,"Authenticating service account credentials ServiceAccounts use signedJSON Web Tokens (JWTs)to authenticate to the Kubernetes API server, and to any other system where atrust relationship exists. Depending on how the token was issued(either time-limited using a TokenRequest or using a legacy mechanism witha Secret), a ServiceAccount token might also have an expiry time, an audience,and a time after which the token starts being valid. When a client that isacting as a ServiceAccount tries to communicate with the Kubernetes API server,the client includes an Authorization: Bearer <token> header with the HTTPrequest. The API server checks the validity of that bearer token as follows: Check the token signature.Check whether the token has expired.Check whether object references in the token claims are currently valid.Check whether the token is currently valid.Check the audience claims. The TokenRequest API produces bound tokens for a ServiceAccount. Thisbinding is linked to the lifetime of the client, such as a Pod, that is actingas that ServiceAccount. For tokens issued using the TokenRequest API, the API server also checks thatthe specific object reference that is using the ServiceAccount still exists,matching by the unique ID of thatobject. For legacy tokens that are mounted as Secrets in Pods, the API serverchecks the token against the Secret. For more information about the authentication process, refer toAuthentication.",292
9.3 - Service Accounts,Authenticating service account credentials in your own code,"Authenticating service account credentials in your own code If you have services of your own that need to validate Kubernetes serviceaccount credentials, you can use the following methods: TokenReview API(recommended)OIDC discovery The Kubernetes project recommends that you use the TokenReview API, becausethis method invalidates tokens that are bound to API objects such as Secrets,ServiceAccounts, and Pods when those objects are deleted. For example, if youdelete the Pod that contains a projected ServiceAccount token, the clusterinvalidates that token immediately and a TokenReview immediately fails.If you use OIDC validation instead, your clients continue to treat the tokenas valid until the token reaches its expiration timestamp. Your application should always define the audience that it accepts, and shouldcheck that the token's audiences match the audiences that the applicationexpects. This helps to minimize the scope of the token so that it can only beused in your application and nowhere else.",200
9.3 - Service Accounts,Alternatives,"Alternatives Issue your own tokens using another mechanism, and then useWebhook Token Authenticationto validate bearer tokens using your own validation service.Provide your own identities to Pods.Use the SPIFFE CSI driver plugin to provide SPIFFE SVIDs as X.509 certificate pairs to Pods.🛇 This item links to a third party project or product that is not part of Kubernetes itself. More informationUse a service mesh such as Istio to provide certificates to Pods.Authenticate from outside the cluster to the API server without using service account tokens:Configure the API server to accept OpenID Connect (OIDC) tokens from your identity provider.Use service accounts or user accounts created using an external Identityand Access Management (IAM) service, such as from a cloud provider, toauthenticate to your cluster.Use the CertificateSigningRequest API with client certificates.Configure the kubelet to retrieve credentials from an image registry.Use a Device Plugin to access a virtual Trusted Platform Module (TPM), whichthen allows authentication using a private key. Learn how to manage your ServiceAccounts as a cluster administrator.Learn how to assign a ServiceAccount to a Pod.Read the ServiceAccount API reference.",256
9.4 - Pod Security Admission,default,"An overview of the Pod Security Admission Controller, which can enforce the Pod Security Standards. FEATURE STATE: Kubernetes v1.25 [stable] The Kubernetes Pod Security Standards definedifferent isolation levels for Pods. These standards let you define how you want to restrict thebehavior of pods in a clear, consistent fashion. Kubernetes offers a built-in Pod Security admission controller to enforce the Pod Security Standards. Pod security restrictionsare applied at the namespace level when pods arecreated.",103
9.4 - Pod Security Admission,Built-in Pod Security admission enforcement,"Built-in Pod Security admission enforcement This page is part of the documentation for Kubernetes v1.26.If you are running a different version of Kubernetes, consult the documentation for that release.",44
9.4 - Pod Security Admission,Pod Security levels,"Pod Security levels Pod Security admission places requirements on a Pod's SecurityContext and other related fields accordingto the three levels defined by the Pod SecurityStandards: privileged, baseline, andrestricted. Refer to the Pod Security Standardspage for an in-depth look at those requirements.",55
9.4 - Pod Security Admission,Pod Security Admission labels for namespaces,"Pod Security Admission labels for namespaces Once the feature is enabled or the webhook is installed, you can configure namespaces to define the admissioncontrol mode you want to use for pod security in each namespace. Kubernetes defines a set oflabels that you can set to define which of thepredefined Pod Security Standard levels you want to use for a namespace. The label you selectdefines what action the control planetakes if a potential violation is detected: Pod Security Admission modesModeDescriptionenforcePolicy violations will cause the pod to be rejected.auditPolicy violations will trigger the addition of an audit annotation to the event recorded in the audit log, but are otherwise allowed.warnPolicy violations will trigger a user-facing warning, but are otherwise allowed. A namespace can configure any or all modes, or even set a different level for different modes. For each mode, there are two labels that determine the policy used: # The per-mode level label indicates which policy level to apply for the mode.## MODE must be one of `enforce`, `audit`, or `warn`.# LEVEL must be one of `privileged`, `baseline`, or `restricted`.pod-security.kubernetes.io/<MODE>: <LEVEL># Optional: per-mode version label that can be used to pin the policy to the# version that shipped with a given Kubernetes minor version (for example v1.26).## MODE must be one of `enforce`, `audit`, or `warn`.# VERSION must be a valid Kubernetes minor version, or `latest`.pod-security.kubernetes.io/<MODE>-version: <VERSION> Check out Enforce Pod Security Standards with Namespace Labels to see example usage.",371
9.4 - Pod Security Admission,Workload resources and Pod templates,"Workload resources and Pod templates Pods are often created indirectly, by creating a workloadobject such as a Deployment or Job. The workload object defines aPod template and a controller for theworkload resource creates Pods based on that template. To help catch violations early, both theaudit and warning modes are applied to the workload resources. However, enforce mode is notapplied to workload resources, only to the resulting pod objects.",88
9.4 - Pod Security Admission,Exemptions,"Exemptions You can define exemptions from pod security enforcement in order to allow the creation of pods thatwould have otherwise been prohibited due to the policy associated with a given namespace.Exemptions can be statically configured in theAdmission Controller configuration. Exemptions must be explicitly enumerated. Requests meeting exemption criteria are ignored by theAdmission Controller (all enforce, audit and warn behaviors are skipped). Exemption dimensions include: Usernames: requests from users with an exempt authenticated (or impersonated) username areignored.RuntimeClassNames: pods and workload resources specifying an exempt runtime class name areignored.Namespaces: pods and workload resources in an exempt namespace are ignored. Caution: Most pods are created by a controller in response to a workloadresource, meaning that exempting an end user will onlyexempt them from enforcement when creating pods directly, but not when creating a workload resource.Controller service accounts (such as system:serviceaccount:kube-system:replicaset-controller)should generally not be exempted, as doing so would implicitly exempt any user that can create thecorresponding workload resource. Updates to the following pod fields are exempt from policy checks, meaning that if a pod updaterequest only changes these fields, it will not be denied even if the pod is in violation of thecurrent policy level: Any metadata updates except changes to the seccomp or AppArmor annotations:seccomp.security.alpha.kubernetes.io/pod (deprecated)container.seccomp.security.alpha.kubernetes.io/* (deprecated)container.apparmor.security.beta.kubernetes.io/*Valid updates to .spec.activeDeadlineSecondsValid updates to .spec.tolerations Pod Security StandardsEnforcing Pod Security StandardsEnforce Pod Security Standards by Configuring the Built-in Admission ControllerEnforce Pod Security Standards with Namespace Labels If you are running an older version of Kubernetes and want to upgradeto a version of Kubernetes that does not include PodSecurityPolicies,read migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller.",445
9.5 - Pod Security Policies,default,"Removed featurePodSecurityPolicy was deprecatedin Kubernetes v1.21, and removed from Kubernetes in v1.25. Instead of using PodSecurityPolicy, you can enforce similar restrictions on Pods usingeither or both: Pod Security Admissiona 3rd party admission plugin, that you deploy and configure yourself For a migration guide, see Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller.For more information on the removal of this API,see PodSecurityPolicy Deprecation: Past, Present, and Future. If you are not running Kubernetes v1.26, check the documentation foryour version of Kubernetes.",139
9.6 - Security For Windows Nodes,Protection for Secret data on nodes,"Protection for Secret data on nodes On Windows, data from Secrets are written out in clear text onto the node's localstorage (as compared to using tmpfs / in-memory filesystems on Linux). As a clusteroperator, you should take both of the following additional measures: Use file ACLs to secure the Secrets' file location.Apply volume-level encryption usingBitLocker.",78
9.6 - Security For Windows Nodes,Container users,"Container users RunAsUsernamecan be specified for Windows Pods or containers to execute the containerprocesses as specific user. This is roughly equivalent toRunAsUser. Windows containers offer two default user accounts, ContainerUser and ContainerAdministrator.The differences between these two user accounts are covered inWhen to use ContainerAdmin and ContainerUser user accountswithin Microsoft's Secure Windows containers documentation. Local users can be added to container images during the container build process. Note:Nano Server based images run asContainerUser by defaultServer Core based images run asContainerAdministrator by default Windows containers can also run as Active Directory identities by utilizingGroup Managed Service Accounts",133
9.6 - Security For Windows Nodes,Pod-level security isolation,"Pod-level security isolation Linux-specific pod security context mechanisms (such as SELinux, AppArmor, Seccomp, or customPOSIX capabilities) are not supported on Windows nodes. Privileged containers are not supportedon Windows.Instead HostProcess containerscan be used on Windows to perform many of the tasks performed by privileged containers on Linux.",71
9.7 - Controlling Access to the Kubernetes API,default,"This page provides an overview of controlling access to the Kubernetes API. Users access the Kubernetes API using kubectl,client libraries, or by making REST requests. Both human users andKubernetes service accounts can beauthorized for API access.When a request reaches the API, it goes through several stages, illustrated in thefollowing diagram: ",77
9.7 - Controlling Access to the Kubernetes API,Transport security,"Transport security By default, the Kubernetes API server listens on port 6443 on the first non-localhost network interface, protected by TLS. In a typical production Kubernetes cluster, the API serves on port 443. The port can be changed with the --secure-port, and the listening IP address with the --bind-address flag. The API server presents a certificate. This certificate may be signed usinga private certificate authority (CA), or based on a public key infrastructure linkedto a generally recognized CA. The certificate and corresponding private key can be set by using the --tls-cert-file and --tls-private-key-file flags. If your cluster uses a private certificate authority, you need a copy of that CAcertificate configured into your ~/.kube/config on the client, so that you cantrust the connection and be confident it was not intercepted. Your client can present a TLS client certificate at this stage.",195
9.7 - Controlling Access to the Kubernetes API,Authentication,"Authentication Once TLS is established, the HTTP request moves to the Authentication step.This is shown as step 1 in the diagram.The cluster creation script or cluster admin configures the API server to runone or more Authenticator modules.Authenticators are described in more detail inAuthentication. The input to the authentication step is the entire HTTP request; however, it typicallyexamines the headers and/or client certificate. Authentication modules include client certificates, password, and plain tokens,bootstrap tokens, and JSON Web Tokens (used for service accounts). Multiple authentication modules can be specified, in which case each one is tried in sequence,until one of them succeeds. If the request cannot be authenticated, it is rejected with HTTP status code 401.Otherwise, the user is authenticated as a specific username, and the user nameis available to subsequent steps to use in their decisions. Some authenticatorsalso provide the group memberships of the user, while other authenticatorsdo not. While Kubernetes uses usernames for access control decisions and in request logging,it does not have a User object nor does it store usernames or other information aboutusers in its API.",236
9.7 - Controlling Access to the Kubernetes API,Authorization,"Authorization After the request is authenticated as coming from a specific user, the request must be authorized. This is shown as step 2 in the diagram. A request must include the username of the requester, the requested action, and the object affected by the action. The request is authorized if an existing policy declares that the user has permissions to complete the requested action. For example, if Bob has the policy below, then he can read pods only in the namespace projectCaribou: {    ""apiVersion"": ""abac.authorization.kubernetes.io/v1beta1"",    ""kind"": ""Policy"",    ""spec"": {        ""user"": ""bob"",        ""namespace"": ""projectCaribou"",        ""resource"": ""pods"",        ""readonly"": true    }} If Bob makes the following request, the request is authorized because he is allowed to read objects in the projectCaribou namespace: {  ""apiVersion"": ""authorization.k8s.io/v1beta1"",  ""kind"": ""SubjectAccessReview"",  ""spec"": {    ""resourceAttributes"": {      ""namespace"": ""projectCaribou"",      ""verb"": ""get"",      ""group"": ""unicorn.example.org"",      ""resource"": ""pods""    }  }} If Bob makes a request to write (create or update) to the objects in the projectCaribou namespace, his authorization is denied. If Bob makes a request to read (get) objects in a different namespace such as projectFish, then his authorization is denied. Kubernetes authorization requires that you use common REST attributes to interact with existing organization-wide or cloud-provider-wide access control systems. It is important to use REST formatting because these control systems might interact with other APIs besides the Kubernetes API. Kubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode, and Webhook mode. When an administrator creates a cluster, they configure the authorization modules that should be used in the API server. If more than one authorization modules are configured, Kubernetes checks each module, and if any module authorizes the request, then the request can proceed. If all of the modules deny the request, then the request is denied (HTTP status code 403). To learn more about Kubernetes authorization, including details about creating policies using the supported authorization modules, see Authorization.",509
9.7 - Controlling Access to the Kubernetes API,Admission control,"Admission control Admission Control modules are software modules that can modify or reject requests.In addition to the attributes available to Authorization modules, AdmissionControl modules can access the contents of the object that is being created or modified. Admission controllers act on requests that create, modify, delete, or connect to (proxy) an object.Admission controllers do not act on requests that merely read objects.When multiple admission controllers are configured, they are called in order. This is shown as step 3 in the diagram. Unlike Authentication and Authorization modules, if any admission controller modulerejects, then the request is immediately rejected. In addition to rejecting objects, admission controllers can also set complex defaults forfields. The available Admission Control modules are described in Admission Controllers. Once a request passes all admission controllers, it is validated using the validation routinesfor the corresponding API object, and then written to the object store (shown as step 4).",184
9.7 - Controlling Access to the Kubernetes API,Auditing,"Auditing Kubernetes auditing provides a security-relevant, chronological set of records documenting the sequence of actions in a cluster.The cluster audits the activities generated by users, by applications that use the Kubernetes API, and by the control plane itself. For more information, see Auditing. Read more documentation on authentication, authorization and API access control: AuthenticatingAuthenticating with Bootstrap TokensAdmission ControllersDynamic Admission ControlAuthorizationRole Based Access ControlAttribute Based Access ControlNode AuthorizationWebhook AuthorizationCertificate Signing Requestsincluding CSR approvaland certificate signingService accountsDeveloper guideAdministration You can learn about: how Pods can useSecretsto obtain API credentials.",140
9.8 - Role Based Access Control Good Practices,default,"Principles and practices for good RBAC design for cluster operators. Kubernetes RBAC is a key security controlto ensure that cluster users and workloads have only the access to resources required toexecute their roles. It is important to ensure that, when designing permissions for clusterusers, the cluster administrator understands the areas where privilege escalation could occur,to reduce the risk of excessive access leading to security incidents. The good practices laid out here should be read in conjunction with the generalRBAC documentation.",100
9.8 - Role Based Access Control Good Practices,Least privilege,"Least privilege Ideally, minimal RBAC rights should be assigned to users and service accounts. Only permissionsexplicitly required for their operation should be used. While each cluster will be different,some general rules that can be applied are : Assign permissions at the namespace level where possible. Use RoleBindings as opposed toClusterRoleBindings to give users rights only within a specific namespace.Avoid providing wildcard permissions when possible, especially to all resources.As Kubernetes is an extensible system, providing wildcard access gives rightsnot just to all object types that currently exist in the cluster, but also to all object typeswhich are created in the future.Administrators should not use cluster-admin accounts except where specifically needed.Providing a low privileged account withimpersonation rightscan avoid accidental modification of cluster resources.Avoid adding users to the system:masters group. Any user who is a member of this groupbypasses all RBAC rights checks and will always have unrestricted superuser access, which cannot berevoked by removing RoleBindings or ClusterRoleBindings. As an aside, if a cluster isusing an authorization webhook, membership of this group also bypasses that webhook (requestsfrom users who are members of that group are never sent to the webhook)",264
9.8 - Role Based Access Control Good Practices,Minimize distribution of privileged tokens,"Minimize distribution of privileged tokens Ideally, pods shouldn't be assigned service accounts that have been granted powerful permissions(for example, any of the rights listed under privilege escalation risks).In cases where a workload requires powerful permissions, consider the following practices: Limit the number of nodes running powerful pods. Ensure that any DaemonSets you runare necessary and are run with least privilege to limit the blast radius of container escapes.Avoid running powerful pods alongside untrusted or publicly-exposed ones. Consider usingTaints and Toleration,NodeAffinity, orPodAntiAffinityto ensure pods don't run alongside untrusted or less-trusted Pods. Pay especial attention tosituations where less-trustworthy Pods are not meeting the Restricted Pod Security Standard.",160
9.8 - Role Based Access Control Good Practices,Hardening,"Hardening Kubernetes defaults to providing access which may not be required in every cluster. Reviewingthe RBAC rights provided by default can provide opportunities for security hardening.In general, changes should not be made to rights provided to system: accounts some optionsto harden cluster rights exist: Review bindings for the system:unauthenticated group and remove them where possible, as this givesaccess to anyone who can contact the API server at a network level.Avoid the default auto-mounting of service account tokens by settingautomountServiceAccountToken: false. For more details, seeusing default service account token.Setting this value for a Pod will overwrite the service account setting, workloadswhich require service account tokens can still mount them.",152
9.8 - Role Based Access Control Good Practices,Periodic review,"Periodic review It is vital to periodically review the Kubernetes RBAC settings for redundant entries andpossible privilege escalations.If an attacker is able to create a user account with the same name as a deleted user,they can automatically inherit all the rights of the deleted user, especially therights assigned to that user.",67
9.8 - Role Based Access Control Good Practices,Kubernetes RBAC - privilege escalation risks,"Kubernetes RBAC - privilege escalation risks Within Kubernetes RBAC there are a number of privileges which, if granted, can allow a user or a service accountto escalate their privileges in the cluster or affect systems outside the cluster. This section is intended to provide visibility of the areas where cluster operatorsshould take care, to ensure that they do not inadvertently allow for more access to clusters than intended.",83
9.8 - Role Based Access Control Good Practices,Listing secrets,"Listing secrets It is generally clear that allowing get access on Secrets will allow a user to read their contents.It is also important to note that list and watch access also effectively allow for users to reveal the Secret contents.For example, when a List response is returned (for example, via kubectl get secrets -A -o yaml), the responseincludes the contents of all Secrets.",80
9.8 - Role Based Access Control Good Practices,Workload creation,"Workload creation Permission to create workloads (either Pods, orworkload resources that manage Pods) in a namespaceimplicitly grants access to many other resources in that namespace, such as Secrets, ConfigMaps, andPersistentVolumes that can be mounted in Pods. Additionally, since Pods can run as anyServiceAccount, granting permissionto create workloads also implicitly grants the API access levels of any service account in thatnamespace. Users who can run privileged Pods can use that access to gain node access and potentially tofurther elevate their privileges. Where you do not fully trust a user or other principalwith the ability to create suitably secure and isolated Pods, you should enforce either theBaseline or Restricted Pod Security Standard.You can use Pod Security admissionor other (third party) mechanisms to implement that enforcement. For these reasons, namespaces should be used to separate resources requiring different levels oftrust or tenancy. It is still considered best practice to follow least privilegeprinciples and assign the minimum set of permissions, but boundaries within a namespace should beconsidered weak.",225
9.8 - Role Based Access Control Good Practices,Persistent volume creation,"Persistent volume creation If someone - or some application - is allowed to create arbitrary PersistentVolumes, that accessincludes the creation of hostPath volumes, which then means that a Pod would get accessto the underlying host filesystem(s) on the associated node. Granting that ability is a security risk. There are many ways a container with unrestricted access to the host filesystem can escalate privileges, includingreading data from other containers, and abusing the credentials of system services, such as Kubelet. You should only allow access to create PersistentVolume objects for: users (cluster operators) that need this access for their work, and who you trust,the Kubernetes control plane components which creates PersistentVolumes based on PersistentVolumeClaimsthat are configured for automatic provisioning.This is usually setup by the Kubernetes provider or by the operator when installing a CSI driver. Where access to persistent storage is required trusted administrators should createPersistentVolumes, and constrained users should use PersistentVolumeClaims to access that storage.",215
9.8 - Role Based Access Control Good Practices,Access to proxy subresource of Nodes,"Access to proxy subresource of Nodes Users with access to the proxy sub-resource of node objects have rights to the Kubelet API,which allows for command execution on every pod on the node(s) to which they have rights.This access bypasses audit logging and admission control, so care should be taken beforegranting rights to this resource.",72
9.8 - Role Based Access Control Good Practices,Escalate verb,"Escalate verb Generally, the RBAC system prevents users from creating clusterroles with more rights than the user possesses.The exception to this is the escalate verb. As noted in the RBAC documentation,users with this right can effectively escalate their privileges.",52
9.8 - Role Based Access Control Good Practices,Bind verb,"Bind verb Similar to the escalate verb, granting users this right allows for the bypass of Kubernetesin-built protections against privilege escalation, allowing users to create bindings toroles with rights they do not already have.",44
9.8 - Role Based Access Control Good Practices,Impersonate verb,"Impersonate verb This verb allows users to impersonate and gain the rights of other users in the cluster.Care should be taken when granting it, to ensure that excessive permissions cannot be gainedvia one of the impersonated accounts.",46
9.8 - Role Based Access Control Good Practices,CSRs and certificate issuing,CSRs and certificate issuing The CSR API allows for users with create rights to CSRs and update rights on certificatesigningrequests/approvalwhere the signer is kubernetes.io/kube-apiserver-client to create new client certificateswhich allow users to authenticate to the cluster. Those client certificates can have arbitrarynames including duplicates of Kubernetes system components. This will effectively allow for privilege escalation.,90
9.8 - Role Based Access Control Good Practices,Control admission webhooks,"Control admission webhooks Users with control over validatingwebhookconfigurations or mutatingwebhookconfigurationscan control webhooks that can read any object admitted to the cluster, and in the case ofmutating webhooks, also mutate admitted objects.",54
9.8 - Role Based Access Control Good Practices,Object creation denial-of-service,"Object creation denial-of-service Users who have rights to create objects in a cluster may be able to create sufficient largeobjects to create a denial of service condition either based on the size or number of objects, as discussed inetcd used by Kubernetes is vulnerable to OOM attack. This may bespecifically relevant in multi-tenant clusters if semi-trusted or untrusted usersare allowed limited access to a system. One option for mitigation of this issue would be to useresource quotasto limit the quantity of objects which can be created. To learn more about RBAC, see the RBAC documentation.",129
9.9 - Good practices for Kubernetes Secrets,default,"Principles and practices for good Secret management for cluster administrators and application developers. In Kubernetes, a Secret is an object that stores sensitive information, such as passwords, OAuth tokens, and SSH keys. Secrets give you more control over how sensitive information is used and reducesthe risk of accidental exposure. Secret values are encoded as base64 strings andare stored unencrypted by default, but can be configured to beencrypted at rest. A Pod can reference the Secret ina variety of ways, such as in a volume mount or as an environment variable.Secrets are designed for confidential data andConfigMaps aredesigned for non-confidential data. The following good practices are intended for both cluster administrators andapplication developers. Use these guidelines to improve the security of yoursensitive information in Secret objects, as well as to more effectively manageyour Secrets.",172
9.9 - Good practices for Kubernetes Secrets,Configure encryption at rest,"Configure encryption at rest By default, Secret objects are stored unencrypted in etcd. You should configure encryption of your Secretdata in etcd. For instructions, refer toEncrypt Secret Data at Rest.",42
9.9 - Good practices for Kubernetes Secrets,Configure least-privilege access to Secrets,"Configure least-privilege access to Secrets When planning your access control mechanism, such as KubernetesRole-based Access Control (RBAC),consider the following guidelines for access to Secret objects. You shouldalso follow the other guidelines inRBAC good practices. Components: Restrict watch or list access to only the mostprivileged, system-level components. Only grant get access for Secrets ifthe component's normal behavior requires it.Humans: Restrict get, watch, or list access to Secrets. Only allowcluster administrators to access etcd. This includes read-only access. Formore complex access control, such as restricting access to Secrets withspecific annotations, consider using third-party authorization mechanisms. Caution: Granting list access to Secrets implicitly lets the subject fetch thecontents of the Secrets. A user who can create a Pod that uses a Secret can also see the value of thatSecret. Even if cluster policies do not allow a user to read the Secretdirectly, the same user could have access to run a Pod that then exposes theSecret. You can detect or limit the impact caused by Secret data being exposed,either intentionally or unintentionally, by a user with this access. Somerecommendations include: Use short-lived SecretsImplement audit rules that alert on specific events, such as concurrentreading of multiple Secrets by a single user",274
9.9 - Good practices for Kubernetes Secrets,Improve etcd management policies,"Improve etcd management policies Consider wiping or shredding the durable storage used by etcd once it isno longer in use. If there are multiple etcd instances, configure encrypted SSL/TLScommunication between the instances to protect the Secret data in transit.",51
9.9 - Good practices for Kubernetes Secrets,Configure access to external Secrets,"Configure access to external Secrets Note:This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information. You can use third-party Secrets store providers to keep your confidential dataoutside your cluster and then configure Pods to access that information.The Kubernetes Secrets Store CSI Driveris a DaemonSet that lets the kubelet retrieve Secrets from external stores, andmount the Secrets as a volume into specific Pods that you authorize to accessthe data. For a list of supported providers, refer toProviders for the Secret Store CSI Driver.",157
9.9 - Good practices for Kubernetes Secrets,Restrict Secret access to specific containers,"Restrict Secret access to specific containers If you are defining multiple containers in a Pod, and only one of thosecontainers needs access to a Secret, define the volume mount or environmentvariable configuration so that the other containers do not have access to thatSecret.",51
9.9 - Good practices for Kubernetes Secrets,Protect Secret data after reading,"Protect Secret data after reading Applications still need to protect the value of confidential information afterreading it from an environment variable or volume. For example, yourapplication must avoid logging the secret data in the clear or transmitting itto an untrusted party.",51
9.9 - Good practices for Kubernetes Secrets,Avoid sharing Secret manifests,"Avoid sharing Secret manifests If you configure a Secret through amanifest, with the secretdata encoded as base64, sharing this file or checking it in to a sourcerepository means the secret is available to everyone who can read the manifest. Caution: Base64 encoding is not an encryption method, it provides no additionalconfidentiality over plain text.",72
9.10 - Multi-tenancy,default,"This page provides an overview of available configuration options and best practices for clustermulti-tenancy. Sharing clusters saves costs and simplifies administration. However, sharing clusters alsopresents challenges such as security, fairness, and managing noisy neighbors. Clusters can be shared in many ways. In some cases, different applications may run in the samecluster. In other cases, multiple instances of the same application may run in the same cluster,one for each end user. All these types of sharing are frequently described using the umbrella termmulti-tenancy. While Kubernetes does not have first-class concepts of end users or tenants, it provides severalfeatures to help manage different tenancy requirements. These are discussed below.",148
9.10 - Multi-tenancy,Use cases,"Use cases The first step to determining how to share your cluster is understanding your use case, so you canevaluate the patterns and tools available. In general, multi-tenancy in Kubernetes clusters fallsinto two broad categories, though many variations and hybrids are also possible.",56
9.10 - Multi-tenancy,Multiple teams,"Multiple teams A common form of multi-tenancy is to share a cluster between multiple teams within anorganization, each of whom may operate one or more workloads. These workloads frequently need tocommunicate with each other, and with other workloads located on the same or different clusters. In this scenario, members of the teams often have direct access to Kubernetes resources via toolssuch as kubectl, or indirect access through GitOps controllers or other types of releaseautomation tools. There is often some level of trust between members of different teams, butKubernetes policies such as RBAC, quotas, and network policies are essential to safely and fairlyshare clusters.",142
9.10 - Multi-tenancy,Multiple customers,"Multiple customers The other major form of multi-tenancy frequently involves a Software-as-a-Service (SaaS) vendorrunning multiple instances of a workload for customers. This business model is so stronglyassociated with this deployment style that many people call it ""SaaS tenancy."" However, a betterterm might be ""multi-customer tenancy,"" since SaaS vendors may also use other deployment models,and this deployment model can also be used outside of SaaS. In this scenario, the customers do not have access to the cluster; Kubernetes is invisible fromtheir perspective and is only used by the vendor to manage the workloads. Cost optimization isfrequently a critical concern, and Kubernetes policies are used to ensure that the workloads arestrongly isolated from each other.",165
9.10 - Multi-tenancy,Tenants,"Tenants When discussing multi-tenancy in Kubernetes, there is no single definition for a ""tenant"".Rather, the definition of a tenant will vary depending on whether multi-team or multi-customertenancy is being discussed. In multi-team usage, a tenant is typically a team, where each team typically deploys a smallnumber of workloads that scales with the complexity of the service. However, the definition of""team"" may itself be fuzzy, as teams may be organized into higher-level divisions or subdividedinto smaller teams. By contrast, if each team deploys dedicated workloads for each new client, they are using amulti-customer model of tenancy. In this case, a ""tenant"" is simply a group of users who share asingle workload. This may be as large as an entire company, or as small as a single team at thatcompany. In many cases, the same organization may use both definitions of ""tenants"" in different contexts.For example, a platform team may offer shared services such as security tools and databases tomultiple internal “customers” and a SaaS vendor may also have multiple teams sharing a developmentcluster. Finally, hybrid architectures are also possible, such as a SaaS provider using acombination of per-customer workloads for sensitive data, combined with multi-tenant sharedservices. A cluster showing coexisting tenancy models",295
9.10 - Multi-tenancy,Isolation,"Isolation There are several ways to design and build multi-tenant solutions with Kubernetes. Each of thesemethods comes with its own set of tradeoffs that impact the isolation level, implementationeffort, operational complexity, and cost of service. A Kubernetes cluster consists of a control plane which runs Kubernetes software, and a data planeconsisting of worker nodes where tenant workloads are executed as pods. Tenant isolation can beapplied in both the control plane and the data plane based on organizational requirements. The level of isolation offered is sometimes described using terms like “hard” multi-tenancy, whichimplies strong isolation, and “soft” multi-tenancy, which implies weaker isolation. In particular,""hard"" multi-tenancy is often used to describe cases where the tenants do not trust each other,often from security and resource sharing perspectives (e.g. guarding against attacks such as dataexfiltration or DoS). Since data planes typically have much larger attack surfaces, ""hard""multi-tenancy often requires extra attention to isolating the data-plane, though control planeisolation also remains critical. However, the terms ""hard"" and ""soft"" can often be confusing, as there is no single definition thatwill apply to all users. Rather, ""hardness"" or ""softness"" is better understood as a broadspectrum, with many different techniques that can be used to maintain different types of isolationin your clusters, based on your requirements. In more extreme cases, it may be easier or necessary to forgo any cluster-level sharing at all andassign each tenant their dedicated cluster, possibly even running on dedicated hardware if VMs arenot considered an adequate security boundary. This may be easier with managed Kubernetes clusters,where the overhead of creating and operating clusters is at least somewhat taken on by a cloudprovider. The benefit of stronger tenant isolation must be evaluated against the cost andcomplexity of managing multiple clusters. The Multi-cluster SIGis responsible for addressing these types of use cases. The remainder of this page focuses on isolation techniques used for shared Kubernetes clusters.However, even if you are considering dedicated clusters, it may be valuable to review theserecommendations, as it will give you the flexibility to shift to shared clusters in the future ifyour needs or capabilities change.",489
9.10 - Multi-tenancy,Namespaces,"Namespaces In Kubernetes, a Namespace provides amechanism for isolating groups of API resources within a single cluster. This isolation has twokey dimensions: Object names within a namespace can overlap with names in other namespaces, similar to files infolders. This allows tenants to name their resources without having to consider what othertenants are doing.Many Kubernetes security policies are scoped to namespaces. For example, RBAC Roles and NetworkPolicies are namespace-scoped resources. Using RBAC, Users and Service Accounts can berestricted to a namespace. In a multi-tenant environment, a Namespace helps segment a tenant's workload into a logical anddistinct management unit. In fact, a common practice is to isolate every workload in its ownnamespace, even if multiple workloads are operated by the same tenant. This ensures that eachworkload has its own identity and can be configured with an appropriate security policy. The namespace isolation model requires configuration of several other Kubernetes resources,networking plugins, and adherence to security best practices to properly isolate tenant workloads.These considerations are discussed below.",234
9.10 - Multi-tenancy,Access controls,"Access controls The most important type of isolation for the control plane is authorization. If teams or theirworkloads can access or modify each others' API resources, they can change or disable all othertypes of policies thereby negating any protection those policies may offer. As a result, it iscritical to ensure that each tenant has the appropriate access to only the namespaces they need,and no more. This is known as the ""Principle of Least Privilege."" Role-based access control (RBAC) is commonly used to enforce authorization in the Kubernetescontrol plane, for both users and workloads (service accounts).Roles andRoleBindings areKubernetes objects that are used at a namespace level to enforce access control in yourapplication; similar objects exist for authorizing access to cluster-level objects, though theseare less useful for multi-tenant clusters. In a multi-team environment, RBAC must be used to restrict tenants' access to the appropriatenamespaces, and ensure that cluster-wide resources can only be accessed or modified by privilegedusers such as cluster administrators. If a policy ends up granting a user more permissions than they need, this is likely a signal thatthe namespace containing the affected resources should be refactored into finer-grainednamespaces. Namespace management tools may simplify the management of these finer-grainednamespaces by applying common RBAC policies to different namespaces, while still allowingfine-grained policies where necessary.",302
9.10 - Multi-tenancy,Quotas,"Quotas Kubernetes workloads consume node resources, like CPU and memory. In a multi-tenant environment,you can use Resource Quotas to manage resource usage oftenant workloads. For the multiple teams use case, where tenants have access to the KubernetesAPI, you can use resource quotas to limit the number of API resources (for example: the number ofPods, or the number of ConfigMaps) that a tenant can create. Limits on object count ensurefairness and aim to avoid noisy neighbor issues from affecting other tenants that share acontrol plane. Resource quotas are namespaced objects. By mapping tenants to namespaces, cluster admins can usequotas to ensure that a tenant cannot monopolize a cluster's resources or overwhelm its controlplane. Namespace management tools simplify the administration of quotas. In addition, whileKubernetes quotas only apply within a single namespace, some namespace management tools allowgroups of namespaces to share quotas, giving administrators far more flexibility with less effortthan built-in quotas. Quotas prevent a single tenant from consuming greater than their allocated share of resourceshence minimizing the “noisy neighbor” issue, where one tenant negatively impacts the performanceof other tenants' workloads. When you apply a quota to namespace, Kubernetes requires you to also specify resource requests andlimits for each container. Limits are the upper bound for the amount of resources that a containercan consume. Containers that attempt to consume resources that exceed the configured limits willeither be throttled or killed, based on the resource type. When resource requests are set lowerthan limits, each container is guaranteed the requested amount but there may still be somepotential for impact across workloads. Quotas cannot protect against all kinds of resource sharing, such as network traffic.Node isolation (described below) may be a better solution for this problem.",387
9.10 - Multi-tenancy,Network isolation,"Network isolation By default, all pods in a Kubernetes cluster are allowed to communicate with each other, and allnetwork traffic is unencrypted. This can lead to security vulnerabilities where traffic isaccidentally or maliciously sent to an unintended destination, or is intercepted by a workload ona compromised node. Pod-to-pod communication can be controlled using Network Policies,which restrict communication between pods using namespace labels or IP address ranges.In a multi-tenant environment where strict network isolation between tenants is required, startingwith a default policy that denies communication between pods is recommended with another rule thatallows all pods to query the DNS server for name resolution. With such a default policy in place,you can begin adding more permissive rules that allow for communication within a namespace.It is also recommended not to use empty label selector '{}' for namespaceSelector field in network policy definition,in case traffic need to be allowed between namespaces.This scheme can be further refined as required. Note that this only applies to pods within a singlecontrol plane; pods that belong to different virtual control planes cannot talk to each other viaKubernetes networking. Namespace management tools may simplify the creation of default or common network policies.In addition, some of these tools allow you to enforce a consistent set of namespace labels acrossyour cluster, ensuring that they are a trusted basis for your policies. Warning: Network policies require a CNI pluginthat supports the implementation of network policies. Otherwise, NetworkPolicy resources will be ignored. More advanced network isolation may be provided by service meshes, which provide OSI Layer 7policies based on workload identity, in addition to namespaces. These higher-level policies canmake it easier to manage namespace-based multi-tenancy, especially when multiple namespaces arededicated to a single tenant. They frequently also offer encryption using mutual TLS, protectingyour data even in the presence of a compromised node, and work across dedicated or virtual clusters.However, they can be significantly more complex to manage and may not be appropriate for all users.",416
9.10 - Multi-tenancy,Storage isolation,"Storage isolation Kubernetes offers several types of volumes that can be used as persistent storage for workloads.For security and data-isolation, dynamic volume provisioningis recommended and volume types that use node resources should be avoided. StorageClasses allow you to describe custom ""classes""of storage offered by your cluster, based on quality-of-service levels, backup policies, or custompolicies determined by the cluster administrators. Pods can request storage using a PersistentVolumeClaim.A PersistentVolumeClaim is a namespaced resource, which enables isolating portions of the storagesystem and dedicating it to tenants within the shared Kubernetes cluster.However, it is important to note that a PersistentVolume is a cluster-wide resource and has alifecycle independent of workloads and namespaces. For example, you can configure a separate StorageClass for each tenant and use this to strengthen isolation.If a StorageClass is shared, you should set a reclaim policy of Deleteto ensure that a PersistentVolume cannot be reused across different namespaces.",219
9.10 - Multi-tenancy,Sandboxing containers,"Sandboxing containers Note:This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information. Kubernetes pods are composed of one or more containers that execute on worker nodes.Containers utilize OS-level virtualization and hence offer a weaker isolation boundary thanvirtual machines that utilize hardware-based virtualization. In a shared environment, unpatched vulnerabilities in the application and system layers can beexploited by attackers for container breakouts and remote code execution that allow access to hostresources. In some applications, like a Content Management System (CMS), customers may be allowedthe ability to upload and execute untrusted scripts or code. In either case, mechanisms to furtherisolate and protect workloads using strong isolation are desirable. Sandboxing provides a way to isolate workloads running in a shared cluster. It typically involvesrunning each pod in a separate execution environment such as a virtual machine or a userspacekernel. Sandboxing is often recommended when you are running untrusted code, where workloads areassumed to be malicious. Part of the reason this type of isolation is necessary is becausecontainers are processes running on a shared kernel; they mount file systems like /sys and /procfrom the underlying host, making them less secure than an application that runs on a virtualmachine which has its own kernel. While controls such as seccomp, AppArmor, and SELinux can beused to strengthen the security of containers, it is hard to apply a universal set of rules to allworkloads running in a shared cluster. Running workloads in a sandbox environment helps toinsulate the host from container escapes, where an attacker exploits a vulnerability to gainaccess to the host system and all the processes/files running on that host. Virtual machines and userspace kernels are 2 popular approaches to sandboxing. The followingsandboxing implementations are available: gVisor intercepts syscalls from containers and runs them through auserspace kernel, written in Go, with limited access to the underlying host.Kata Containers is an OCI compliant runtime that allows you to runcontainers in a VM. The hardware virtualization available in Kata offers an added layer ofsecurity for containers running untrusted code.",493
9.10 - Multi-tenancy,Node Isolation,"Node Isolation Node isolation is another technique that you can use to isolate tenant workloads from each other.With node isolation, a set of nodes is dedicated to running pods from a particular tenant andco-mingling of tenant pods is prohibited. This configuration reduces the noisy tenant issue, asall pods running on a node will belong to a single tenant. The risk of information disclosure isslightly lower with node isolation because an attacker that manages to escape from a containerwill only have access to the containers and volumes mounted to that node. Although workloads from different tenants are running on different nodes, it is important to beaware that the kubelet and (unless using virtual control planes) the API service are still sharedservices. A skilled attacker could use the permissions assigned to the kubelet or other podsrunning on the node to move laterally within the cluster and gain access to tenant workloadsrunning on other nodes. If this is a major concern, consider implementing compensating controlssuch as seccomp, AppArmor or SELinux or explore using sandboxed containers or creating separateclusters for each tenant. Node isolation is a little easier to reason about from a billing standpoint than sandboxingcontainers since you can charge back per node rather than per pod. It also has fewer compatibilityand performance issues and may be easier to implement than sandboxing containers.For example, nodes for each tenant can be configured with taints so that only pods with thecorresponding toleration can run on them. A mutating webhook could then be used to automaticallyadd tolerations and node affinities to pods deployed into tenant namespaces so that they run on aspecific set of nodes designated for that tenant. Node isolation can be implemented using an pod node selectorsor a Virtual Kubelet.",363
9.10 - Multi-tenancy,API Priority and Fairness,"API Priority and Fairness API priority and fairness is a Kubernetesfeature that allows you to assign a priority to certain pods running within the cluster.When an application calls the Kubernetes API, the API server evaluates the priority assigned to pod.Calls from pods with higher priority are fulfilled before those with a lower priority.When contention is high, lower priority calls can be queued until the server is less busy or youcan reject the requests. Using API priority and fairness will not be very common in SaaS environments unless you areallowing customers to run applications that interface with the Kubernetes API, for example,a controller.",133
9.10 - Multi-tenancy,Quality-of-Service (QoS),"Quality-of-Service (QoS) When you’re running a SaaS application, you may want the ability to offer differentQuality-of-Service (QoS) tiers of service to different tenants. For example, you may have freemiumservice that comes with fewer performance guarantees and features and a for-fee service tier withspecific performance guarantees. Fortunately, there are several Kubernetes constructs that canhelp you accomplish this within a shared cluster, including network QoS, storage classes, and podpriority and preemption. The idea with each of these is to provide tenants with the quality ofservice that they paid for. Let’s start by looking at networking QoS. Typically, all pods on a node share a network interface. Without network QoS, some pods mayconsume an unfair share of the available bandwidth at the expense of other pods.The Kubernetes bandwidth plugin creates anextended resourcefor networking that allows you to use Kubernetes resources constructs, i.e. requests/limits, toapply rate limits to pods by using Linux tc queues.Be aware that the plugin is considered experimental as per theNetwork Pluginsdocumentation and should be thoroughly tested before use in production environments. For storage QoS, you will likely want to create different storage classes or profiles withdifferent performance characteristics. Each storage profile can be associated with a differenttier of service that is optimized for different workloads such IO, redundancy, or throughput.Additional logic might be necessary to allow the tenant to associate the appropriate storageprofile with their workload. Finally, there’s pod priority and preemptionwhere you can assign priority values to pods. When scheduling pods, the scheduler will tryevicting pods with lower priority when there are insufficient resources to schedule pods that areassigned a higher priority. If you have a use case where tenants have different service tiers in ashared cluster e.g. free and paid, you may want to give higher priority to certain tiers usingthis feature.",409
9.10 - Multi-tenancy,DNS,"DNS Kubernetes clusters include a Domain Name System (DNS) service to provide translations from namesto IP addresses, for all Services and Pods. By default, the Kubernetes DNS service allows lookupsacross all namespaces in the cluster. In multi-tenant environments where tenants can access pods and other Kubernetes resources, or wherestronger isolation is required, it may be necessary to prevent pods from looking up services in otherNamespaces.You can restrict cross-namespace DNS lookups by configuring security rules for the DNS service.For example, CoreDNS (the default DNS service for Kubernetes) can leverage Kubernetes metadatato restrict queries to Pods and Services within a namespace. For more information, read anexample ofconfiguring this within the CoreDNS documentation. When a Virtual Control Plane per tenant model is used, a DNSservice must be configured per tenant or a multi-tenant DNS service must be used.Here is an example of a customized version of CoreDNSthat supports multiple tenants.",224
9.10 - Multi-tenancy,Operators,"Operators Operators are Kubernetes controllers that manageapplications. Operators can simplify the management of multiple instances of an application, likea database service, which makes them a common building block in the multi-consumer (SaaS)multi-tenancy use case. Operators used in a multi-tenant environment should follow a stricter set of guidelines.Specifically, the Operator should: Support creating resources within different tenant namespaces, rather than just in the namespacein which the Operator is deployed.Ensure that the Pods are configured with resource requests and limits, to ensure scheduling and fairness.Support configuration of Pods for data-plane isolation techniques such as node isolation andsandboxed containers.",145
9.10 - Multi-tenancy,Implementations,"Implementations Note:This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information. There are two primary ways to share a Kubernetes cluster for multi-tenancy: using Namespaces(that is, a Namespace per tenant) or by virtualizing the control plane (that is, virtual controlplane per tenant). In both cases, data plane isolation, and management of additional considerations such as APIPriority and Fairness, is also recommended. Namespace isolation is well-supported by Kubernetes, has a negligible resource cost, and providesmechanisms to allow tenants to interact appropriately, such as by allowing service-to-servicecommunication. However, it can be difficult to configure, and doesn't apply to Kubernetesresources that can't be namespaced, such as Custom Resource Definitions, Storage Classes, and Webhooks. Control plane virtualization allows for isolation of non-namespaced resources at the cost ofsomewhat higher resource usage and more difficult cross-tenant sharing. It is a good option whennamespace isolation is insufficient but dedicated clusters are undesirable, due to the high costof maintaining them (especially on-prem) or due to their higher overhead and lack of resourcesharing. However, even within a virtualized control plane, you will likely see benefits by usingnamespaces as well. The two options are discussed in more detail in the following sections.",331
9.10 - Multi-tenancy,Namespace per tenant,"Namespace per tenant As previously mentioned, you should consider isolating each workload in its own namespace, even ifyou are using dedicated clusters or virtualized control planes. This ensures that each workloadonly has access to its own resources, such as Config Maps and Secrets, and allows you to tailordedicated security policies for each workload. In addition, it is a best practice to give eachnamespace names that are unique across your entire fleet (that is, even if they are in separateclusters), as this gives you the flexibility to switch between dedicated and shared clusters inthe future, or to use multi-cluster tooling such as service meshes. Conversely, there are also advantages to assigning namespaces at the tenant level, not just theworkload level, since there are often policies that apply to all workloads owned by a singletenant. However, this raises its own problems. Firstly, this makes it difficult or impossible tocustomize policies to individual workloads, and secondly, it may be challenging to come up with asingle level of ""tenancy"" that should be given a namespace. For example, an organization may havedivisions, teams, and subteams - which should be assigned a namespace? To solve this, Kubernetes provides the Hierarchical Namespace Controller (HNC),which allows you to organize your namespaces into hierarchies, and share certain policies andresources between them. It also helps you manage namespace labels, namespace lifecycles, anddelegated management, and share resource quotas across related namespaces. These capabilities canbe useful in both multi-team and multi-customer scenarios. Other projects that provide similar capabilities and aid in managing namespaced resources arelisted below.",352
9.10 - Multi-tenancy,Virtual control plane per tenant,"Virtual control plane per tenant Another form of control-plane isolation is to use Kubernetes extensions to provide each tenant avirtual control-plane that enables segmentation of cluster-wide API resources.Data plane isolation techniques can be used with this model to securelymanage worker nodes across tenants. The virtual control plane based multi-tenancy model extends namespace-based multi-tenancy byproviding each tenant with dedicated control plane components, and hence complete control overcluster-wide resources and add-on services. Worker nodes are shared across all tenants, and aremanaged by a Kubernetes cluster that is normally inaccessible to tenants.This cluster is often referred to as a super-cluster (or sometimes as a host-cluster).Since a tenant’s control-plane is not directly associated with underlying compute resources it isreferred to as a virtual control plane. A virtual control plane typically consists of the Kubernetes API server, the controller manager,and the etcd data store. It interacts with the super cluster via a metadata synchronizationcontroller which coordinates changes across tenant control planes and the control plane of thesuper-cluster. By using per-tenant dedicated control planes, most of the isolation problems due to sharing oneAPI server among all tenants are solved. Examples include noisy neighbors in the control plane,uncontrollable blast radius of policy misconfigurations, and conflicts between cluster scopeobjects such as webhooks and CRDs. Hence, the virtual control plane model is particularlysuitable for cases where each tenant requires access to a Kubernetes API server and expects thefull cluster manageability. The improved isolation comes at the cost of running and maintaining an individual virtual controlplane per tenant. In addition, per-tenant control planes do not solve isolation problems in thedata plane, such as node-level noisy neighbors or security threats. These must still be addressedseparately. The Kubernetes Cluster API - Nested (CAPN)project provides an implementation of virtual control planes.",410
9.11 - Kubernetes API Server Bypass Risks,default,"Security architecture information relating to the API server and other components The Kubernetes API server is the main point of entry to a cluster for external parties(users and services) interacting with it. As part of this role, the API server has several key built-in security controls, such asaudit logging and admission controllers.However, there are ways to modify the configurationor content of the cluster that bypass these controls. This page describes the ways in which the security controls built into theKubernetes API server can be bypassed, so that cluster operatorsand security architects can ensure that these bypasses are appropriately restricted.",126
9.11 - Kubernetes API Server Bypass Risks,Static Pods,"Static Pods The kubelet on each node loads anddirectly manages any manifests that are stored in a named directory or fetched froma specific URL as static Pods inyour cluster. The API server doesn't manage these static Pods. An attacker with writeaccess to this location could modify the configuration of static pods loaded from thatsource, or could introduce new static Pods. Static Pods are restricted from accessing other objects in the Kubernetes API. For example,you can't configure a static Pod to mount a Secret from the cluster. However, these Pods cantake other security sensitive actions, such as using hostPath mounts from the underlyingnode. By default, the kubelet creates a mirror podso that the static Pods are visible in the Kubernetes API. However, if the attacker uses an invalidnamespace name when creating the Pod, it will not be visible in the Kubernetes API and can onlybe discovered by tooling that has access to the affected host(s). If a static Pod fails admission control, the kubelet won't register the Pod with theAPI server. However, the Pod still runs on the node. For more information, refer tokubeadm issue #1541.",256
9.11 - Kubernetes API Server Bypass Risks,Mitigations,"Mitigations Only enable the kubelet static Pod manifest functionalityif required by the node.If a node uses the static Pod functionality, restrict filesystem access to the static Pod manifest directoryor URL to users who need the access.Restrict access to kubelet configuration parameters and files to prevent an attacker settinga static Pod path or URL.Regularly audit and centrally report all access to directories or web storage locations that hoststatic Pod manifests and kubelet configuration files. Mitigations Restrict access to sub-resources of the nodes API object using mechanisms such asRBAC. Only grant this access when required,such as by monitoring services.Restrict access to the kubelet port. Only allow specified and trusted IP addressranges to access the port.Ensure that kubelet authentication.is set to webhook or certificate mode.Ensure that the unauthenticated ""read-only"" Kubelet port is not enabled on the cluster. Mitigations Ensure that the certificate authority trusted by etcd is used only for the purposes ofauthentication to that service.Control access to the private key for the etcd server certificate, and to the API server'sclient certificate and key.Consider restricting access to the etcd port at a network level, to only allow accessfrom specified and trusted IP address ranges. Mitigations Ensure that you tightly control filesystem access to container runtime sockets.When possible, restrict this access to the root user.Isolate the kubelet from other components running on the node, usingmechanisms such as Linux kernel namespaces.Ensure that you restrict or forbid the use of hostPath mountsthat include the container runtime socket, either directly or by mounting a parentdirectory. Also hostPath mounts must be set as read-only to mitigate risksof attackers bypassing directory restrictions.Restrict user access to nodes, and especially restrict superuser access to nodes.",386
9.11 - Kubernetes API Server Bypass Risks,The kubelet API,"The kubelet API The kubelet provides an HTTP API that is typically exposed on TCP port 10250 on clusterworker nodes. The API might also be exposed on control plane nodes depending on the Kubernetesdistribution in use. Direct access to the API allows for disclosure of information aboutthe pods running on a node, the logs from those pods, and execution of commands inevery container running on the node. When Kubernetes cluster users have RBAC access to Node object sub-resources, that accessserves as authorization to interact with the kubelet API. The exact access depends onwhich sub-resource access has been granted, as detailed inkubelet authorization. Direct access to the kubelet API is not subject to admission control and is not loggedby Kubernetes audit logging. An attacker with direct access to this API may be able tobypass controls that detect or prevent certain actions. The kubelet API can be configured to authenticate requests in a number of ways.By default, the kubelet configuration allows anonymous access. Most Kubernetes providerschange the default to use webhook and certificate authentication. This lets the control planeensure that the caller is authorized to access the nodes API resource or sub-resources.The default anonymous access doesn't make this assertion with the control plane.",271
9.11 - Kubernetes API Server Bypass Risks,The etcd API,"The etcd API Kubernetes clusters use etcd as a datastore. The etcd service listens on TCP port 2379.The only clients that need access are the Kubernetes API server and any backup toolingthat you use. Direct access to this API allows for disclosure or modification of anydata held in the cluster. Access to the etcd API is typically managed by client certificate authentication.Any certificate issued by a certificate authority that etcd trusts allows full accessto the data stored inside etcd. Direct access to etcd is not subject to Kubernetes admission control and is not loggedby Kubernetes audit logging. An attacker who has read access to the API server'setcd client certificate private key (or can create a new trusted client certificate) can gaincluster admin rights by accessing cluster secrets or modifying access rules. Even withoutelevating their Kubernetes RBAC privileges, an attacker who can modify etcd can retrieve any API objector create new workloads inside the cluster. Many Kubernetes providers configureetcd to use mutual TLS (both client and server verify each other's certificate for authentication).There is no widely accepted implementation of authorization for the etcd API, althoughthe feature exists. Since there is no authorization model, any certificatewith client access to etcd can be used to gain full access to etcd. Typically, etcd client certificatesthat are only used for health checking can also grant full read and write access.",307
9.11 - Kubernetes API Server Bypass Risks,Container runtime socket,"Container runtime socket On each node in a Kubernetes cluster, access to interact with containers is controlledby the container runtime (or runtimes, if you have configured more than one). Typically,the container runtime exposes a Unix socket that the kubelet can access. An attacker withaccess to this socket can launch new containers or interact with running containers. At the cluster level, the impact of this access depends on whether the containers thatrun on the compromised node have access to Secrets or other confidentialdata that an attacker could use to escalate privileges to other worker nodes or tocontrol plane components.",119
9.12 - Security Checklist,default,"Baseline checklist for ensuring security in Kubernetes clusters. This checklist aims at providing a basic list of guidance with links to morecomprehensive documentation on each topic. It does not claim to be exhaustiveand is meant to evolve. On how to read and use this document: The order of topics does not reflect an order of priority.Some checklist items are detailed in the paragraph below the list of each section. Caution: Checklists are not sufficient for attaining a good security posture on theirown. A good security posture requires constant attention and improvement, but achecklist can be the first step on the never-ending journey towards securitypreparedness. Some of the recommendations in this checklist may be toorestrictive or too lax for your specific security needs. Since Kubernetessecurity is not ""one size fits all"", each category of checklist items should beevaluated on its merits.",186
9.12 - Security Checklist,Authentication & Authorization,"Authentication & Authorization system:masters group is not used for user or component authentication after bootstrapping. The kube-controller-manager is running with --use-service-account-credentialsenabled. The root certificate is protected (either an offline CA, or a managedonline CA with effective access controls). Intermediate and leaf certificates have an expiry date no more than 3years in the future. A process exists for periodic access review, and reviews occur no morethan 24 months apart. The Role Based Access Control Good Practicesis followed for guidance related to authentication and authorization. After bootstrapping, neither users nor components should authenticate to theKubernetes API as system:masters. Similarly, running all ofkube-controller-manager as system:masters should be avoided. In fact,system:masters should only be used as a break-glass mechanism, as opposed toan admin user.",185
9.12 - Security Checklist,Network security,"Network security CNI plugins in-use supports network policies. Ingress and egress network policies are applied to all workloads in thecluster. Default network policies within each namespace, selecting all pods, denyingeverything, are in place. If appropriate, a service mesh is used to encrypt all communications inside of the cluster. The Kubernetes API, kubelet API and etcd are not exposed publicly on Internet. Access from the workloads to the cloud metadata API is filtered. Use of LoadBalancer and ExternalIPs is restricted. A number of Container Network Interface (CNI) pluginsplugins provide the functionality torestrict network resources that pods may communicate with. This is most commonly donethrough Network Policieswhich provide a namespaced resource to define rules. Default network policiesblocking everything egress and ingress, in each namespace, selecting all thepods, can be useful to adopt an allow list approach, ensuring that no workloadsis missed. Not all CNI plugins provide encryption in transit. If the chosen plugin lacks thisfeature, an alternative solution could be to use a service mesh to provide thatfunctionality. The etcd datastore of the control plane should have controls to limit access andnot be publicly exposed on the Internet. Furthermore, mutual TLS (mTLS) shouldbe used to communicate securely with it. The certificate authority for thisshould be unique to etcd. External Internet access to the Kubernetes API server should be restricted tonot expose the API publicly. Be careful as many managed Kubernetes distributionare publicly exposing the API server by default. You can then use a bastion hostto access the server. The kubelet API accessshould be restricted and not publicly exposed, the defaults authentication andauthorization settings, when no configuration file specified with the --configflag, are overly permissive. If a cloud provider is used for hosting Kubernetes, the access from pods to the cloudmetadata API 169.254.169.254 should also be restricted or blocked if not neededbecause it may leak information. For restricted LoadBalancer and ExternalIPs use, seeCVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPsand the DenyServiceExternalIPs admission controllerfor further information.",462
9.12 - Security Checklist,Pod security,"Pod security RBAC rights to create, update, patch, delete workloads is only granted if necessary. Appropriate Pod Security Standards policy is applied for all namespaces and enforced. Memory limit is set for the workloads with a limit equal or inferior to the request. CPU limit might be set on sensitive workloads. For nodes that support it, Seccomp is enabled with appropriate syscallsprofile for programs. For nodes that support it, AppArmor or SELinux is enabled with appropriateprofile for programs. RBAC authorization is crucial butcannot be granular enough to have authorization on the Pods' resources(or on any resource that manages Pods). The only granularity is the API verbson the resource itself, for example, create on Pods. Withoutadditional admission, the authorization to create these resources allows directunrestricted access to the schedulable nodes of a cluster. The Pod Security Standardsdefine three different policies, privileged, baseline and restricted that limithow fields can be set in the PodSpec regarding security.These standards can be enforced at the namespace level with the newPod Security admission,enabled by default, or by third-party admission webhook. Please note that,contrary to the removed PodSecurityPolicy admission it replaces,Pod Securityadmission can be easily combined with admission webhooks and external services. Pod Security admission restricted policy, the most restrictive policy of thePod Security Standards set,can operate in several modes,warn, audit or enforce to gradually apply the most appropriatesecurity contextaccording to security best practices. Nevertheless, pods'security contextshould be separately investigated to limit the privileges and access pods mayhave on top of the predefined security standards, for specific use cases. For a hands-on tutorial on Pod Security,see the blog postKubernetes 1.23: Pod Security Graduates to Beta. Memory and CPU limitsshould be set in order to restrict the memory and CPU resources a pod canconsume on a node, and therefore prevent potential DoS attacks from malicious orbreached workloads. Such policy can be enforced by an admission controller.Please note that CPU limits will throttle usage and thus can have unintendedeffects on auto-scaling features or efficiency i.e. running the process in besteffort with the CPU resource available. Caution: Memory limit superior to request can expose the whole node to OOM issues.",489
9.12 - Security Checklist,Enabling Seccomp,"Enabling Seccomp Seccomp can improve the security of your workloads by reducing the Linux kernelsyscall attack surface available inside containers. The seccomp filter modeleverages BPF to create an allow or deny list of specific syscalls, namedprofiles. Those seccomp profiles can be enabled on individual workloads,a security tutorial is available. Inaddition, the Kubernetes Security Profiles Operatoris a project to facilitate the management and use of seccomp in clusters. For historical context, please note that Docker has been usinga default seccomp profileto only allow a restricted set of syscalls since 2016 fromDocker Engine 1.10,but Kubernetes is still not confining workloads by default. The default seccompprofile can be found in containerdas well. Fortunately, Seccomp Default, anew alpha feature to use a default seccomp profile for all workloads can now beenabled and tested. Note: Seccomp is only available on Linux nodes.",214
9.12 - Security Checklist,AppArmor,"AppArmor AppArmor is a Linux kernel security module that canprovide an easy way to implement Mandatory Access Control (MAC) and betterauditing through system logs. To enable AppArmor in Kubernetes,at least version 1.4 is required. Like seccomp, AppArmor is also configuredthrough profiles, where each profile is either running in enforcing mode, whichblocks access to disallowed resources or complain mode, which only reportsviolations. AppArmor profiles are enforced on a per-container basis, with anannotation, allowing for processes to gain just the right privileges. Note: AppArmor is only available on Linux nodes, and enabled insome Linux distributions.",137
9.12 - Security Checklist,SELinux,"SELinux SELinux is also aLinux kernel security module that can provide a mechanism for supporting accesscontrol security policies, including Mandatory Access Controls (MAC). SELinuxlabels can be assigned to containers or podsvia their securityContext section. Note: SELinux is only available on Linux nodes, and enabled insome Linux distributions.",73
9.12 - Security Checklist,Pod placement,"Pod placement Pod placement is done in accordance with the tiers of sensitivity of theapplication. Sensitive applications are running isolated on nodes or with specificsandboxed runtimes. Pods that are on different tiers of sensitivity, for example, an application podand the Kubernetes API server, should be deployed onto separate nodes. Thepurpose of node isolation is to prevent an application container breakout todirectly providing access to applications with higher level of sensitivity to easilypivot within the cluster. This separation should be enforced to prevent podsaccidentally being deployed onto the same node. This could be enforced with thefollowing features: Node SelectorsKey-value pairs, as part of the pod specification, that specify which nodes todeploy onto. These can be enforced at the namespace and cluster level with thePodNodeSelectoradmission controller.PodTolerationRestrictionAn admission controller that allows administrators to restrict permittedtolerations within anamespace. Pods within a namespace may only utilize the tolerations specified onthe namespace object annotation keys that provide a set of default and allowedtolerations.RuntimeClassRuntimeClass is a feature for selecting the container runtime configuration.The container runtime configuration is used to run a Pod's containers and canprovide more or less isolation from the host at the cost of performanceoverhead.",267
9.12 - Security Checklist,Secrets,"Secrets ConfigMaps are not used to hold confidential data. Encryption at rest is configured for the Secret API. If appropriate, a mechanism to inject secrets stored in third-party storageis deployed and available. Service account tokens are not mounted in pods that don't require them. Bound service account token volumeis in-use instead of non-expiring tokens. Secrets required for pods should be stored within Kubernetes Secrets as opposedto alternatives such as ConfigMap. Secret resources stored within etcd shouldbe encrypted at rest. Pods needing secrets should have these automatically mounted through volumes,preferably stored in memory like with the emptyDir.medium option.Mechanism can be used to also inject secrets from third-party storages asvolume, like the Secrets Store CSI Driver.This should be done preferentially as compared to providing the pods serviceaccount RBAC access to secrets. This would allow adding secrets into the pod asenvironment variables or files. Please note that the environment variable methodmight be more prone to leakage due to crash dumps in logs and thenon-confidential nature of environment variable in Linux, as opposed to thepermission mechanism on files. Service account tokens should not be mounted into pods that do not require them. This can be configured by settingautomountServiceAccountTokento false either within the service account to apply throughout the namespaceor specifically for a pod. For Kubernetes v1.22 and above, useBound Service Accountsfor time-bound service account credentials.",306
9.12 - Security Checklist,Images,"Images Minimize unnecessary content in container images. Container images are configured to be run as unprivileged user. References to container images are made by sha256 digests (rather thantags) or the provenance of the image is validated by verifying the image'sdigital signature at deploy time via admission control. Container images are regularly scanned during creation and in deployment, andknown vulnerable software is patched. Container image should contain the bare minimum to run the program theypackage. Preferably, only the program and its dependencies, building the imagefrom the minimal possible base. In particular, image used in production should notcontain shells or debugging utilities, as anephemeral debug containercan be used for troubleshooting. Build images to directly start with an unprivileged user by using theUSER instruction in Dockerfile.The Security Contextallows a container image to be started with a specific user and group withrunAsUser and runAsGroup, even if not specified in the image manifest.However, the file permissions in the image layers might make it impossible to juststart the process with a new unprivileged user without image modification. Avoid using image tags to reference an image, especially the latest tag, theimage behind a tag can be easily modified in a registry. Prefer using thecomplete sha256 digest which is unique to the image manifest. This policy can beenforced via an ImagePolicyWebhook.Image signatures can also be automatically verified with an admission controllerat deploy time to validate their authenticity and integrity. Scanning a container image can prevent critical vulnerabilities from beingdeployed to the cluster alongside the container image. Image scanning should becompleted before deploying a container image to a cluster and is usually doneas part of the deployment process in a CI/CD pipeline. The purpose of an imagescan is to obtain information about possible vulnerabilities and theirprevention in the container image, such as aCommon Vulnerability Scoring System (CVSS)score. If the result of the image scans is combined with the pipelinecompliance rules, only properly patched container images will end up inProduction.",422
9.12 - Security Checklist,Admission controllers,"Admission controllers An appropriate selection of admission controllers is enabled. A pod security policy is enforced by the Pod Security Admission or/and awebhook admission controller. The admission chain plugins and webhooks are securely configured. Admission controllers can help to improve the security of the cluster. However,they can present risks themselves as they extend the API server andshould be properly secured. The following lists present a number of admission controllers that could beconsidered to enhance the security posture of your cluster and application. Itincludes controllers that may be referenced in other parts of this document. This first group of admission controllers includes pluginsenabled by default,consider to leave them enabled unless you know what you are doing: CertificateApprovalPerforms additional authorization checks to ensure the approving user haspermission to approve certificate request.CertificateSigningPerforms additional authorization checks to ensure the signing user haspermission to sign certificate requests.CertificateSubjectRestrictionRejects any certificate request that specifies a 'group' (or 'organizationattribute') of system:masters.LimitRangerEnforce the LimitRange API constraints.MutatingAdmissionWebhookAllows the use of custom controllers through webhooks, these controllers maymutate requests that it reviews.PodSecurityReplacement for Pod Security Policy, restricts security contexts of deployedPods.ResourceQuotaEnforces resource quotas to prevent over-usage of resources.ValidatingAdmissionWebhookAllows the use of custom controllers through webhooks, these controllers donot mutate requests that it reviews. The second group includes plugin that are not enabled by default but in generalavailability state and recommended to improve your security posture: DenyServiceExternalIPsRejects all net-new usage of the Service.spec.externalIPs field. This is a mitigation forCVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs.NodeRestrictionRestricts kubelet's permissions to only modify the pods API resources they ownor the node API ressource that represent themselves. It also prevents kubeletfrom using the node-restriction.kubernetes.io/ annotation, which can be usedby an attacker with access to the kubelet's credentials to influence podplacement to the controlled node. The third group includes plugins that are not enabled by default but could beconsidered for certain use cases: AlwaysPullImagesEnforces the usage of the latest version of a tagged image and ensures that the deployerhas permissions to use the image.ImagePolicyWebhookAllows enforcing additional controls for images through webhooks. RBAC Good Practices forfurther information on authorization.Cluster Multi-tenancy guide forconfiguration options recommendations and best practices on multi-tenancy.Blog post ""A Closer Look at NSA/CISA Kubernetes Hardening Guidance""for complementary resource on hardening Kubernetes clusters.",592
10.1 - Limit Ranges,default,"By default, containers run with unbounded compute resources on a Kubernetes cluster.Using Kubernetes resource quotas,administrators (also termed cluster operators) can restrict consumption and creationof cluster resources (such as CPU time, memory, and persistent storage) within a specifiednamespace.Within a namespace, a Pod can consume as much CPU and memory as is allowed by the ResourceQuotas that apply to that namespace. As a cluster operator, or as a namespace-level administrator, you might also be concerned about making sure that a single object cannot monopolize all available resources within a namespace. A LimitRange is a policy to constrain the resource allocations (limits and requests) that you can specify for each applicable object kind (such as Pod or PersistentVolumeClaim) in a namespace. A LimitRange provides constraints that can: Enforce minimum and maximum compute resources usage per Pod or Container in a namespace.Enforce minimum and maximum storage request per PersistentVolumeClaim in a namespace.Enforce a ratio between request and limit for a resource in a namespace.Set default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime. A LimitRange is enforced in a particular namespace when there is aLimitRange object in that namespace. The name of a LimitRange object must be a validDNS subdomain name.",276
10.1 - Limit Ranges,Constraints on resource limits and requests,"Constraints on resource limits and requests The administrator creates a LimitRange in a namespace.Users create (or try to create) objects in that namespace, such as Pods or PersistentVolumeClaims.First, the LimitRange admission controller applies default request and limit values for all Pods (and their containers) that do not set compute resource requirements.Second, the LimitRange tracks usage to ensure it does not exceed resource minimum, maximum and ratio defined in any LimitRange present in the namespace.If you attempt to create or update an object (Pod or PersistentVolumeClaim) that violates a LimitRange constraint, your request to the API server will fail with an HTTP status code 403 Forbidden and a message explaining the constraint that has been violated.If you add a LimitRange in a namespace that applies to compute-related resources such ascpu and memory, you must specifyrequests or limits for those values. Otherwise, the system may reject Pod creation.LimitRange validations occur only at Pod admission stage, not on running Pods.If you add or modify a LimitRange, the Pods that already exist in that namespacecontinue unchanged.If two or more LimitRange objects exist in the namespace, it is not deterministic which default value will be applied.",255
10.1 - Limit Ranges,LimitRange and admission checks for Pods,"LimitRange and admission checks for Pods A LimitRange does not check the consistency of the default values it applies. This means that a default value for the limit that is set by LimitRange may be less than the request value specified for the container in the spec that a client submits to the API server. If that happens, the final Pod will not be schedulable. For example, you define a LimitRange with this manifest: concepts/policy/limit-range/problematic-limit-range.yamlapiVersion: v1kind: LimitRangemetadata:  name: cpu-resource-constraintspec:  limits:  - default: # this section defines default limits      cpu: 500m    defaultRequest: # this section defines default requests      cpu: 500m    max: # max and min define the limit range      cpu: ""1""    min:      cpu: 100m    type: Container along with a Pod that declares a CPU resource request of 700m, but not a limit: concepts/policy/limit-range/example-conflict-with-limitrange-cpu.yamlapiVersion: v1kind: Podmetadata:  name: example-conflict-with-limitrange-cpuspec:  containers:  - name: demo    image: registry.k8s.io/pause:2.0    resources:      requests:        cpu: 700m then that Pod will not be scheduled, failing with an error similar to: Pod ""example-conflict-with-limitrange-cpu"" is invalid: spec.containers[0].resources.requests: Invalid value: ""700m"": must be less than or equal to cpu limit If you set both request and limit, then that new Pod will be scheduled successfully even with the same LimitRange in place: concepts/policy/limit-range/example-no-conflict-with-limitrange-cpu.yamlapiVersion: v1kind: Podmetadata:  name: example-no-conflict-with-limitrange-cpuspec:  containers:  - name: demo    image: registry.k8s.io/pause:2.0    resources:      requests:        cpu: 700m      limits:        cpu: 700m",476
10.1 - Limit Ranges,Example resource constraints,"Example resource constraints Examples of policies that could be created using LimitRange are: In a 2 node cluster with a capacity of 8 GiB RAM and 16 cores, constrain Pods in a namespace to request 100m of CPU with a max limit of 500m for CPU and request 200Mi for Memory with a max limit of 600Mi for Memory.Define default CPU limit and request to 150m and memory default request to 300Mi for Containers started with no cpu and memory requests in their specs. In the case where the total limits of the namespace is less than the sum of the limits of the Pods/Containers,there may be contention for resources. In this case, the Containers or Pods will not be created. Neither contention nor changes to a LimitRange will affect already created resources. For examples on using limits, see: how to configure minimum and maximum CPU constraints per namespace.how to configure minimum and maximum Memory constraints per namespace.how to configure default CPU Requests and Limits per namespace.how to configure default Memory Requests and Limits per namespace.how to configure minimum and maximum Storage consumption per namespace.a detailed example on configuring quota per namespace. Refer to the LimitRanger design document for context and historical information.",252
10.2 - Resource Quotas,default,"When several users or teams share a cluster with a fixed number of nodes,there is a concern that one team could use more than its fair share of resources. Resource quotas are a tool for administrators to address this concern. A resource quota, defined by a ResourceQuota object, provides constraints that limitaggregate resource consumption per namespace. It can limit the quantity of objects that canbe created in a namespace by type, as well as the total amount of compute resources that maybe consumed by resources in that namespace. Resource quotas work like this: Different teams work in different namespaces. This can be enforced with RBAC.The administrator creates one ResourceQuota for each namespace.Users create resources (pods, services, etc.) in the namespace, and the quota systemtracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.If creating or updating a resource violates a quota constraint, the request will fail with HTTPstatus code 403 FORBIDDEN with a message explaining the constraint that would have been violated.If quota is enabled in a namespace for compute resources like cpu and memory, users must specifyrequests or limits for those values; otherwise, the quota system may reject pod creation. Hint: Usethe LimitRanger admission controller to force defaults for pods that make no compute resource requirements.See the walkthroughfor an example of how to avoid this problem. Note:For cpu and memory resources, ResourceQuotas enforce that every(new) pod in that namespace sets a limit for that resource.If you enforce a resource quota in a namespace for either cpu or memory,you, and other clients, must specify either requests or limits for that resource,for every new Pod you submit. If you don't, the control plane may reject admissionfor that Pod.For other resources: ResourceQuota works and will ignore pods in the namespace without setting a limit or request for that resource. It means that you can create a new pod without limit/request ephemeral storage if the resource quota limits the ephemeral storage of this namespace.You can use a LimitRange to automatically seta default request for these resources. The name of a ResourceQuota object must be a validDNS subdomain name. Examples of policies that could be created using namespaces and quotas are: In a cluster with a capacity of 32 GiB RAM, and 16 cores, let team A use 20 GiB and 10 cores,let B use 10GiB and 4 cores, and hold 2GiB and 2 cores in reserve for future allocation.Limit the ""testing"" namespace to using 1 core and 1GiB RAM. Let the ""production"" namespaceuse any amount. In the case where the total capacity of the cluster is less than the sum of the quotas of the namespaces,there may be contention for resources. This is handled on a first-come-first-served basis. Neither contention nor changes to quota will affect already created resources.",600
10.2 - Resource Quotas,Enabling Resource Quota,Enabling Resource Quota Resource Quota support is enabled by default for many Kubernetes distributions. It isenabled when the API server--enable-admission-plugins= flag has ResourceQuota asone of its arguments. A resource quota is enforced in a particular namespace when there is aResourceQuota in that namespace.,67
10.2 - Resource Quotas,Compute Resource Quota,"Compute Resource Quota You can limit the total sum ofcompute resourcesthat can be requested in a given namespace. The following resource types are supported: Resource NameDescriptionlimits.cpuAcross all pods in a non-terminal state, the sum of CPU limits cannot exceed this value.limits.memoryAcross all pods in a non-terminal state, the sum of memory limits cannot exceed this value.requests.cpuAcross all pods in a non-terminal state, the sum of CPU requests cannot exceed this value.requests.memoryAcross all pods in a non-terminal state, the sum of memory requests cannot exceed this value.hugepages-<size>Across all pods in a non-terminal state, the number of huge page requests of the specified size cannot exceed this value.cpuSame as requests.cpumemorySame as requests.memory",180
10.2 - Resource Quotas,Resource Quota For Extended Resources,"Resource Quota For Extended Resources In addition to the resources mentioned above, in release 1.10, quota support forextended resources is added. As overcommit is not allowed for extended resources, it makes no sense to specify both requestsand limits for the same extended resource in a quota. So for extended resources, only quota itemswith prefix requests. is allowed for now. Take the GPU resource as an example, if the resource name is nvidia.com/gpu, and you want tolimit the total number of GPUs requested in a namespace to 4, you can define a quota as follows: requests.nvidia.com/gpu: 4 See Viewing and Setting Quotas for more detail information.",144
10.2 - Resource Quotas,Storage Resource Quota,"Storage Resource Quota You can limit the total sum of storage resources that can be requested in a given namespace. In addition, you can limit consumption of storage resources based on associated storage-class. Resource NameDescriptionrequests.storageAcross all persistent volume claims, the sum of storage requests cannot exceed this value.persistentvolumeclaimsThe total number of PersistentVolumeClaims that can exist in the namespace.<storage-class-name>.storageclass.storage.k8s.io/requests.storageAcross all persistent volume claims associated with the <storage-class-name>, the sum of storage requests cannot exceed this value.<storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaimsAcross all persistent volume claims associated with the storage-class-name, the total number of persistent volume claims that can exist in the namespace. For example, if an operator wants to quota storage with gold storage class separate from bronze storage class, the operator candefine a quota as follows: gold.storageclass.storage.k8s.io/requests.storage: 500Gibronze.storageclass.storage.k8s.io/requests.storage: 100Gi In release 1.8, quota support for local ephemeral storage is added as an alpha feature: Resource NameDescriptionrequests.ephemeral-storageAcross all pods in the namespace, the sum of local ephemeral storage requests cannot exceed this value.limits.ephemeral-storageAcross all pods in the namespace, the sum of local ephemeral storage limits cannot exceed this value.ephemeral-storageSame as requests.ephemeral-storage. Note: When using a CRI container runtime, container logs will count against the ephemeral storage quota.This can result in the unexpected eviction of pods that have exhausted their storage quotas.Refer to Logging Architecture for details.",393
10.2 - Resource Quotas,Object Count Quota,"Object Count Quota You can set quota for the total number of certain resources of all standard,namespaced resource types using the following syntax: count/<resource>.<group> for resources from non-core groupscount/<resource> for resources from the core group Here is an example set of resources users may want to put under object count quota: count/persistentvolumeclaimscount/servicescount/secretscount/configmapscount/replicationcontrollerscount/deployments.appscount/replicasets.appscount/statefulsets.appscount/jobs.batchcount/cronjobs.batch The same syntax can be used for custom resources.For example, to create a quota on a widgets custom resource in the example.com API group, use count/widgets.example.com. When using count/* resource quota, an object is charged against the quota if it exists in server storage.These types of quotas are useful to protect against exhaustion of storage resources. For example, you maywant to limit the number of Secrets in a server given their large size. Too many Secrets in a cluster canactually prevent servers and controllers from starting. You can set a quota for Jobs to protect againsta poorly configured CronJob. CronJobs that create too many Jobs in a namespace can lead to a denial of service. It is also possible to do generic object count quota on a limited set of resources.The following types are supported: Resource NameDescriptionconfigmapsThe total number of ConfigMaps that can exist in the namespace.persistentvolumeclaimsThe total number of PersistentVolumeClaims that can exist in the namespace.podsThe total number of Pods in a non-terminal state that can exist in the namespace. A pod is in a terminal state if .status.phase in (Failed, Succeeded) is true.replicationcontrollersThe total number of ReplicationControllers that can exist in the namespace.resourcequotasThe total number of ResourceQuotas that can exist in the namespace.servicesThe total number of Services that can exist in the namespace.services.loadbalancersThe total number of Services of type LoadBalancer that can exist in the namespace.services.nodeportsThe total number of Services of type NodePort that can exist in the namespace.secretsThe total number of Secrets that can exist in the namespace. For example, pods quota counts and enforces a maximum on the number of podscreated in a single namespace that are not terminal. You might want to set a podsquota on a namespace to avoid the case where a user creates many small pods andexhausts the cluster's supply of Pod IPs.",557
10.2 - Resource Quotas,Quota Scopes,"Quota Scopes Each quota can have an associated set of scopes. A quota will only measure usage for a resource if it matchesthe intersection of enumerated scopes. When a scope is added to the quota, it limits the number of resources it supports to those that pertain to the scope.Resources specified on the quota outside of the allowed set results in a validation error. ScopeDescriptionTerminatingMatch pods where .spec.activeDeadlineSeconds >= 0NotTerminatingMatch pods where .spec.activeDeadlineSeconds is nilBestEffortMatch pods that have best effort quality of service.NotBestEffortMatch pods that do not have best effort quality of service.PriorityClassMatch pods that references the specified priority class.CrossNamespacePodAffinityMatch pods that have cross-namespace pod (anti)affinity terms. The BestEffort scope restricts a quota to tracking the following resource: pods The Terminating, NotTerminating, NotBestEffort and PriorityClassscopes restrict a quota to tracking the following resources: podscpumemoryrequests.cpurequests.memorylimits.cpulimits.memory Note that you cannot specify both the Terminating and the NotTerminatingscopes in the same quota, and you cannot specify both the BestEffort andNotBestEffort scopes in the same quota either. The scopeSelector supports the following values in the operator field: InNotInExistsDoesNotExist When using one of the following values as the scopeName when defining thescopeSelector, the operator must be Exists. TerminatingNotTerminatingBestEffortNotBestEffort If the operator is In or NotIn, the values field must have at leastone value. For example: scopeSelector:    matchExpressions:      - scopeName: PriorityClass        operator: In        values:          - middle If the operator is Exists or DoesNotExist, the values field must NOT bespecified.",418
10.2 - Resource Quotas,Resource Quota Per PriorityClass,"Resource Quota Per PriorityClass FEATURE STATE: Kubernetes v1.17 [stable] Pods can be created at a specific priority.You can control a pod's consumption of system resources based on a pod's priority, by using the scopeSelectorfield in the quota spec. A quota is matched and consumed only if scopeSelector in the quota spec selects the pod. When quota is scoped for priority class using scopeSelector field, quota objectis restricted to track only following resources: podscpumemoryephemeral-storagelimits.cpulimits.memorylimits.ephemeral-storagerequests.cpurequests.memoryrequests.ephemeral-storage This example creates a quota object and matches it with pods at specific priorities. The exampleworks as follows: Pods in the cluster have one of the three priority classes, ""low"", ""medium"", ""high"".One quota object is created for each priority. Save the following YAML to a file quota.yml. apiVersion: v1kind: Listitems:- apiVersion: v1  kind: ResourceQuota  metadata:    name: pods-high  spec:    hard:      cpu: ""1000""      memory: 200Gi      pods: ""10""    scopeSelector:      matchExpressions:      - operator : In        scopeName: PriorityClass        values: [""high""]- apiVersion: v1  kind: ResourceQuota  metadata:    name: pods-medium  spec:    hard:      cpu: ""10""      memory: 20Gi      pods: ""10""    scopeSelector:      matchExpressions:      - operator : In        scopeName: PriorityClass        values: [""medium""]- apiVersion: v1  kind: ResourceQuota  metadata:    name: pods-low  spec:    hard:      cpu: ""5""      memory: 10Gi      pods: ""10""    scopeSelector:      matchExpressions:      - operator : In        scopeName: PriorityClass        values: [""low""] Apply the YAML using kubectl create. kubectl create -f ./quota.yml resourcequota/pods-high createdresourcequota/pods-medium createdresourcequota/pods-low created Verify that Used quota is 0 using kubectl describe quota. kubectl describe quota Name:       pods-highNamespace:  defaultResource    Used  Hard--------    ----  ----cpu         0     1kmemory      0     200Gipods        0     10Name:       pods-lowNamespace:  defaultResource    Used  Hard--------    ----  ----cpu         0     5memory      0     10Gipods        0     10Name:       pods-mediumNamespace:  defaultResource    Used  Hard--------    ----  ----cpu         0     10memory      0     20Gipods        0     10 Create a pod with priority ""high"". Save the following YAML to afile high-priority-pod.yml. apiVersion: v1kind: Podmetadata:  name: high-priorityspec:  containers:  - name: high-priority    image: ubuntu    command: [""/bin/sh""]    args: [""-c"", ""while true; do echo hello; sleep 10;done""]    resources:      requests:        memory: ""10Gi""        cpu: ""500m""      limits:        memory: ""10Gi""        cpu: ""500m""  priorityClassName: high Apply it with kubectl create. kubectl create -f ./high-priority-pod.yml Verify that ""Used"" stats for ""high"" priority quota, pods-high, has changed and thatthe other two quotas are unchanged. kubectl describe quota Name:       pods-highNamespace:  defaultResource    Used  Hard--------    ----  ----cpu         500m  1kmemory      10Gi  200Gipods        1     10Name:       pods-lowNamespace:  defaultResource    Used  Hard--------    ----  ----cpu         0     5memory      0     10Gipods        0     10Name:       pods-mediumNamespace:  defaultResource    Used  Hard--------    ----  ----cpu         0     10memory      0     20Gipods        0     10",932
10.2 - Resource Quotas,Cross-namespace Pod Affinity Quota,"Cross-namespace Pod Affinity Quota FEATURE STATE: Kubernetes v1.24 [stable] Operators can use CrossNamespacePodAffinity quota scope to limit which namespaces are allowed tohave pods with affinity terms that cross namespaces. Specifically, it controls which pods are allowedto set namespaces or namespaceSelector fields in pod affinity terms. Preventing users from using cross-namespace affinity terms might be desired since a podwith anti-affinity constraints can block pods from all other namespacesfrom getting scheduled in a failure domain. Using this scope operators can prevent certain namespaces (foo-ns in the example below)from having pods that use cross-namespace pod affinity by creating a resource quota object inthat namespace with CrossNamespaceAffinity scope and hard limit of 0: apiVersion: v1kind: ResourceQuotametadata:  name: disable-cross-namespace-affinity  namespace: foo-nsspec:  hard:    pods: ""0""  scopeSelector:    matchExpressions:    - scopeName: CrossNamespaceAffinity If operators want to disallow using namespaces and namespaceSelector by default, andonly allow it for specific namespaces, they could configure CrossNamespaceAffinityas a limited resource by setting the kube-apiserver flag --admission-control-config-fileto the path of the following configuration file: apiVersion: apiserver.config.k8s.io/v1kind: AdmissionConfigurationplugins:- name: ""ResourceQuota""  configuration:    apiVersion: apiserver.config.k8s.io/v1    kind: ResourceQuotaConfiguration    limitedResources:    - resource: pods      matchScopes:      - scopeName: CrossNamespaceAffinity With the above configuration, pods can use namespaces and namespaceSelector in pod affinity onlyif the namespace where they are created have a resource quota object withCrossNamespaceAffinity scope and a hard limit greater than or equal to the number of pods using those fields.",431
10.2 - Resource Quotas,Requests compared to Limits,"Requests compared to Limits When allocating compute resources, each container may specify a request and a limit value for either CPU or memory.The quota can be configured to quota either value. If the quota has a value specified for requests.cpu or requests.memory, then it requires that every incomingcontainer makes an explicit request for those resources. If the quota has a value specified for limits.cpu or limits.memory,then it requires that every incoming container specifies an explicit limit for those resources.",99
10.2 - Resource Quotas,Viewing and Setting Quotas,"Viewing and Setting Quotas Kubectl supports creating, updating, and viewing quotas: kubectl create namespace myspace cat <<EOF > compute-resources.yamlapiVersion: v1kind: ResourceQuotametadata:  name: compute-resourcesspec:  hard:    requests.cpu: ""1""    requests.memory: 1Gi    limits.cpu: ""2""    limits.memory: 2Gi    requests.nvidia.com/gpu: 4EOF kubectl create -f ./compute-resources.yaml --namespace=myspace cat <<EOF > object-counts.yamlapiVersion: v1kind: ResourceQuotametadata:  name: object-countsspec:  hard:    configmaps: ""10""    persistentvolumeclaims: ""4""    pods: ""4""    replicationcontrollers: ""20""    secrets: ""10""    services: ""10""    services.loadbalancers: ""2""EOF kubectl create -f ./object-counts.yaml --namespace=myspace kubectl get quota --namespace=myspace NAME                    AGEcompute-resources       30sobject-counts           32s kubectl describe quota compute-resources --namespace=myspace Name:                    compute-resourcesNamespace:               myspaceResource                 Used  Hard--------                 ----  ----limits.cpu               0     2limits.memory            0     2Girequests.cpu             0     1requests.memory          0     1Girequests.nvidia.com/gpu  0     4 kubectl describe quota object-counts --namespace=myspace Name:                   object-countsNamespace:              myspaceResource                Used    Hard--------                ----    ----configmaps              0       10persistentvolumeclaims  0       4pods                    0       4replicationcontrollers  0       20secrets                 1       10services                0       10services.loadbalancers  0       2 Kubectl also supports object count quota for all standard namespaced resourcesusing the syntax count/<resource>.<group>: kubectl create namespace myspace kubectl create quota test --hard=count/deployments.apps=2,count/replicasets.apps=4,count/pods=3,count/secrets=4 --namespace=myspace kubectl create deployment nginx --image=nginx --namespace=myspace --replicas=2 kubectl describe quota --namespace=myspace Name:                         testNamespace:                    myspaceResource                      Used  Hard--------                      ----  ----count/deployments.apps        1     2count/pods                    2     3count/replicasets.apps        1     4count/secrets                 1     4",621
10.2 - Resource Quotas,Quota and Cluster Capacity,"Quota and Cluster Capacity ResourceQuotas are independent of the cluster capacity. They areexpressed in absolute units. So, if you add nodes to your cluster, this does notautomatically give each namespace the ability to consume more resources. Sometimes more complex policies may be desired, such as: Proportionally divide total cluster resources among several teams.Allow each tenant to grow resource usage as needed, but have a generouslimit to prevent accidental resource exhaustion.Detect demand from one namespace, add nodes, and increase quota. Such policies could be implemented using ResourceQuotas as building blocks, bywriting a ""controller"" that watches the quota usage and adjusts the quotahard limits of each namespace according to other signals. Note that resource quota divides up aggregate cluster resources, but it creates norestrictions around nodes: pods from several namespaces may run on the same node.",178
10.2 - Resource Quotas,Limit Priority Class consumption by default,"Limit Priority Class consumption by default It may be desired that pods at a particular priority, eg. ""cluster-services"",should be allowed in a namespace, if and only if, a matching quota object exists. With this mechanism, operators are able to restrict usage of certain highpriority classes to a limited number of namespaces and not every namespacewill be able to consume these priority classes by default. To enforce this, kube-apiserver flag --admission-control-config-file should beused to pass path to the following configuration file: apiVersion: apiserver.config.k8s.io/v1kind: AdmissionConfigurationplugins:- name: ""ResourceQuota""  configuration:    apiVersion: apiserver.config.k8s.io/v1    kind: ResourceQuotaConfiguration    limitedResources:    - resource: pods      matchScopes:      - scopeName: PriorityClass        operator: In        values: [""cluster-services""] Then, create a resource quota object in the kube-system namespace: policy/priority-class-resourcequota.yamlapiVersion: v1kind: ResourceQuotametadata:  name: pods-cluster-servicesspec:  scopeSelector:    matchExpressions:      - operator : In        scopeName: PriorityClass        values: [""cluster-services""] kubectl apply -f https://k8s.io/examples/policy/priority-class-resourcequota.yaml -n kube-system resourcequota/pods-cluster-services created In this case, a pod creation will be allowed if: the Pod's priorityClassName is not specified.the Pod's priorityClassName is specified to a value other than cluster-services.the Pod's priorityClassName is set to cluster-services, it is to be createdin the kube-system namespace, and it has passed the resource quota check. A Pod creation request is rejected if its priorityClassName is set to cluster-servicesand it is to be created in a namespace other than kube-system. See ResourceQuota design doc for more information.See a detailed example for how to use resource quota.Read Quota support for priority class design doc.See LimitedResources",479
10.3 - Process ID Limits And Reservations,default,"FEATURE STATE: Kubernetes v1.20 [stable] Kubernetes allow you to limit the number of process IDs (PIDs) that aPod can use.You can also reserve a number of allocatable PIDs for each nodefor use by the operating system and daemons (rather than by Pods). Process IDs (PIDs) are a fundamental resource on nodes. It is trivial to hit thetask limit without hitting any other resource limits, which can then causeinstability to a host machine. Cluster administrators require mechanisms to ensure that Pods running in thecluster cannot induce PID exhaustion that prevents host daemons (such as thekubelet orkube-proxy,and potentially also the container runtime) from running.In addition, it is important to ensure that PIDs are limited among Pods in orderto ensure they have limited impact on other workloads on the same node. Note: On certain Linux installations, the operating system sets the PIDs limit to a low default,such as 32768. Consider raising the value of /proc/sys/kernel/pid_max. You can configure a kubelet to limit the number of PIDs a given Pod can consume.For example, if your node's host OS is set to use a maximum of 262144 PIDs andexpect to host less than 250 Pods, one can give each Pod a budget of 1000PIDs to prevent using up that node's overall number of available PIDs. If theadmin wants to overcommit PIDs similar to CPU or memory, they may do so as wellwith some additional risks. Either way, a single Pod will not be able to bringthe whole machine down. This kind of resource limiting helps to prevent simplefork bombs from affecting operation of an entire cluster. Per-Pod PID limiting allows administrators to protect one Pod from another, butdoes not ensure that all Pods scheduled onto that host are unable to impact the node overall.Per-Pod limiting also does not protect the node agents themselves from PID exhaustion. You can also reserve an amount of PIDs for node overhead, separate from theallocation to Pods. This is similar to how you can reserve CPU, memory, or otherresources for use by the operating system and other facilities outside of Podsand their containers. PID limiting is a an important sibling to computeresource requestsand limits. However, you specify it in a different way: rather than defining aPod's resource limit in the .spec for a Pod, you configure the limit as asetting on the kubelet. Pod-defined PID limits are not currently supported. Caution: This means that the limit that applies to a Pod may be different depending onwhere the Pod is scheduled. To make things simple, it's easiest if all Nodes usethe same PID resource limits and reservations.",580
10.3 - Process ID Limits And Reservations,Node PID limits,"Node PID limits Kubernetes allows you to reserve a number of process IDs for the system use. Toconfigure the reservation, use the parameter pid=<number> in the--system-reserved and --kube-reserved command line options to the kubelet.The value you specified declares that the specified number of process IDs willbe reserved for the system as a whole and for Kubernetes system daemonsrespectively.",92
10.3 - Process ID Limits And Reservations,Pod PID limits,"Pod PID limits Kubernetes allows you to limit the number of processes running in a Pod. Youspecify this limit at the node level, rather than configuring it as a resourcelimit for a particular Pod. Each Node can have a different PID limit.To configure the limit, you can specify the command line parameter --pod-max-pidsto the kubelet, or set PodPidsLimit in the kubeletconfiguration file.",94
10.3 - Process ID Limits And Reservations,PID based eviction,"PID based eviction You can configure kubelet to start terminating a Pod when it is misbehaving and consuming abnormal amount of resources.This feature is called eviction. You canConfigure Out of Resource Handlingfor various eviction signals.Use pid.available eviction signal to configure the threshold for number of PIDs used by Pod.You can set soft and hard eviction policies.However, even with the hard eviction policy, if the number of PIDs growing very fast,node can still get into unstable state by hitting the node PIDs limit.Eviction signal value is calculated periodically and does NOT enforce the limit. PID limiting - per Pod and per Node sets the hard limit.Once the limit is hit, workload will start experiencing failures when trying to get a new PID.It may or may not lead to rescheduling of a Pod,depending on how workload reacts on these failures and how liveleness and readinessprobes are configured for the Pod. However, if limits were set correctly,you can guarantee that other Pods workload and system processes will not run out of PIDswhen one Pod is misbehaving. Refer to the PID Limiting enhancement document for more information.For historical context, readProcess ID Limiting for Stability Improvements in Kubernetes 1.14.Read Managing Resources for Containers.Learn how to Configure Out of Resource Handling.",275
10.4 - Node Resource Managers,default,"In order to support latency-critical and high-throughput workloads, Kubernetes offers a suite of Resource Managers. The managers aim to co-ordinate and optimise node's resources alignment for pods configured with a specific requirement for CPUs, devices, and memory (hugepages) resources. The main manager, the Topology Manager, is a Kubelet component that co-ordinates the overall resource management process through its policy. The configuration of individual managers is elaborated in dedicated documents: CPU Manager PoliciesDevice ManagerMemory Manager Policies",109
"11 - Scheduling, Preemption and Eviction",default,"In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating Pods with lower Priority so that Pods with higher Priority can schedule on Nodes. Eviction is the process of proactively terminating one or more Pods on resource-starved Nodes. In Kubernetes, scheduling refers to making sure that Podsare matched to Nodes so that thekubelet can run them. Preemptionis the process of terminating Pods with lower Priorityso that Pods with higher Priority can schedule on Nodes. Eviction is the processof terminating one or more Pods on Nodes.",146
"11 - Scheduling, Preemption and Eviction",Scheduling,Scheduling Kubernetes SchedulerAssigning Pods to NodesPod OverheadPod Topology Spread ConstraintsTaints and TolerationsScheduling FrameworkDynamic Resource AllocationScheduler Performance TuningResource Bin Packing for Extended ResourcesPod Scheduling Readiness,58
"11 - Scheduling, Preemption and Eviction",Pod Disruption,"Pod Disruption Pod disruption is the process by whichPods on Nodes are terminated either voluntarily or involuntarily. Voluntary disruptions are started intentionally by application owners or clusteradministrators. Involuntary disruptions are unintentional and can be triggered byunavoidable issues like Nodes running out of resources, or by accidental deletions. Pod Priority and PreemptionNode-pressure EvictionAPI-initiated Eviction",83
11.1 - Kubernetes Scheduler,Scheduling overview,"Scheduling overview A scheduler watches for newly created Pods that have no Node assigned. Forevery Pod that the scheduler discovers, the scheduler becomes responsiblefor finding the best Node for that Pod to run on. The scheduler reachesthis placement decision taking into account the scheduling principlesdescribed below. If you want to understand why Pods are placed onto a particular Node,or if you're planning to implement a custom scheduler yourself, thispage will help you learn about scheduling.",100
11.1 - Kubernetes Scheduler,kube-scheduler,"kube-scheduler kube-scheduleris the default scheduler for Kubernetes and runs as part of thecontrol plane.kube-scheduler is designed so that, if you want and need to, you canwrite your own scheduling component and use that instead. Kube-scheduler selects an optimal node to run newly created or not yetscheduled (unscheduled) pods. Since containers in pods - and pods themselves -can have different requirements, the scheduler filters out any nodes thatdon't meet a Pod's specific scheduling needs. Alternatively, the API letsyou specify a node for a Pod when you create it, but this is unusualand is only done in special cases. In a cluster, Nodes that meet the scheduling requirements for a Podare called feasible nodes. If none of the nodes are suitable, the podremains unscheduled until the scheduler is able to place it. The scheduler finds feasible Nodes for a Pod and then runs a set offunctions to score the feasible Nodes and picks a Node with the highestscore among the feasible ones to run the Pod. The scheduler then notifiesthe API server about this decision in a process called binding. Factors that need to be taken into account for scheduling decisions includeindividual and collective resource requirements, hardware / software /policy constraints, affinity and anti-affinity specifications, datalocality, inter-workload interference, and so on.",301
11.1 - Kubernetes Scheduler,Node selection in kube-scheduler,"Node selection in kube-scheduler kube-scheduler selects a node for the pod in a 2-step operation: FilteringScoring The filtering step finds the set of Nodes where it's feasible toschedule the Pod. For example, the PodFitsResources filter checks whether acandidate Node has enough available resource to meet a Pod's specificresource requests. After this step, the node list contains any suitableNodes; often, there will be more than one. If the list is empty, thatPod isn't (yet) schedulable. In the scoring step, the scheduler ranks the remaining nodes to choosethe most suitable Pod placement. The scheduler assigns a score to each Nodethat survived filtering, basing this score on the active scoring rules. Finally, kube-scheduler assigns the Pod to the Node with the highest ranking.If there is more than one node with equal scores, kube-scheduler selectsone of these at random. There are two supported ways to configure the filtering and scoring behaviorof the scheduler: Scheduling Policies allow you to configure Predicates for filtering and Priorities for scoring.Scheduling Profiles allow you to configure Plugins that implement different scheduling stages, including: QueueSort, Filter, Score, Bind, Reserve, Permit, and others. You can also configure the kube-scheduler to run different profiles. Read about scheduler performance tuningRead about Pod topology spread constraintsRead the reference documentation for kube-schedulerRead the kube-scheduler config (v1beta3) referenceLearn about configuring multiple schedulersLearn about topology management policiesLearn about Pod OverheadLearn about scheduling of Pods that use volumes in:Volume Topology SupportStorage Capacity TrackingNode-specific Volume Limits",376
11.2 - Assigning Pods to Nodes,default,"You can constrain a Pod so that it isrestricted to run on particular node(s),or to prefer to run on particular nodes.There are several ways to do this and the recommended approaches all uselabel selectors to facilitate the selection.Often, you do not need to set any such constraints; thescheduler will automatically do a reasonable placement(for example, spreading your Pods across nodes so as not place Pods on a node with insufficient free resources).However, there are some circumstances where you may want to control which nodethe Pod deploys to, for example, to ensure that a Pod ends up on a node with an SSD attached to it,or to co-locate Pods from two different services that communicate a lot into the same availability zone. You can use any of the following methods to choose where Kubernetes schedulesspecific Pods: nodeSelector field matching against node labelsAffinity and anti-affinitynodeName fieldPod topology spread constraints",204
11.2 - Assigning Pods to Nodes,Node labels,"Node labels Like many other Kubernetes objects, nodes havelabels. You can attach labels manually.Kubernetes also populates a standard set of labels on all nodes in a cluster. See Well-Known Labels, Annotations and Taintsfor a list of common node labels. Note: The value of these labels is cloud provider specific and is not guaranteed to be reliable.For example, the value of kubernetes.io/hostname may be the same as the node name in some environmentsand a different value in other environments.",113
11.2 - Assigning Pods to Nodes,Node isolation/restriction,"Node isolation/restriction Adding labels to nodes allows you to target Pods for scheduling on specificnodes or groups of nodes. You can use this functionality to ensure that specificPods only run on nodes with certain isolation, security, or regulatoryproperties. If you use labels for node isolation, choose label keys that the kubeletcannot modify. This prevents a compromised node from setting those labels onitself so that the scheduler schedules workloads onto the compromised node. The NodeRestriction admission pluginprevents the kubelet from setting or modifying labels with anode-restriction.kubernetes.io/ prefix. To make use of that label prefix for node isolation: Ensure you are using the Node authorizer and have enabled the NodeRestriction admission plugin.Add labels with the node-restriction.kubernetes.io/ prefix to your nodes, and use those labels in your node selectors.For example, example.com.node-restriction.kubernetes.io/fips=true or example.com.node-restriction.kubernetes.io/pci-dss=true.",237
11.2 - Assigning Pods to Nodes,nodeSelector,nodeSelector nodeSelector is the simplest recommended form of node selection constraint.You can add the nodeSelector field to your Pod specification and specify thenode labels you want the target node to have.Kubernetes only schedules the Pod onto nodes that have each of the labels youspecify. See Assign Pods to Nodes for moreinformation.,73
11.2 - Assigning Pods to Nodes,Affinity and anti-affinity,"Affinity and anti-affinity nodeSelector is the simplest way to constrain Pods to nodes with specificlabels. Affinity and anti-affinity expands the types of constraints you candefine. Some of the benefits of affinity and anti-affinity include: The affinity/anti-affinity language is more expressive. nodeSelector onlyselects nodes with all the specified labels. Affinity/anti-affinity gives youmore control over the selection logic.You can indicate that a rule is soft or preferred, so that the schedulerstill schedules the Pod even if it can't find a matching node.You can constrain a Pod using labels on other Pods running on the node (or other topological domain),instead of just node labels, which allows you to define rules for which Podscan be co-located on a node. The affinity feature consists of two types of affinity: Node affinity functions like the nodeSelector field but is more expressive andallows you to specify soft rules.Inter-pod affinity/anti-affinity allows you to constrain Pods against labelson other Pods.",229
11.2 - Assigning Pods to Nodes,Node affinity,"Node affinity Node affinity is conceptually similar to nodeSelector, allowing you to constrain which nodes yourPod can be scheduled on based on node labels. There are two types of nodeaffinity: requiredDuringSchedulingIgnoredDuringExecution: The scheduler can'tschedule the Pod unless the rule is met. This functions like nodeSelector,but with a more expressive syntax.preferredDuringSchedulingIgnoredDuringExecution: The scheduler tries tofind a node that meets the rule. If a matching node is not available, thescheduler still schedules the Pod. Note: In the preceding types, IgnoredDuringExecution means that if the node labelschange after Kubernetes schedules the Pod, the Pod continues to run. You can specify node affinities using the .spec.affinity.nodeAffinity field inyour Pod spec. For example, consider the following Pod spec: pods/pod-with-node-affinity.yamlapiVersion: v1kind: Podmetadata:  name: with-node-affinityspec:  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:        - matchExpressions:          - key: topology.kubernetes.io/zone            operator: In            values:            - antarctica-east1            - antarctica-west1      preferredDuringSchedulingIgnoredDuringExecution:      - weight: 1        preference:          matchExpressions:          - key: another-node-label-key            operator: In            values:            - another-node-label-value  containers:  - name: with-node-affinity    image: registry.k8s.io/pause:2.0 In this example, the following rules apply: The node must have a label with the key topology.kubernetes.io/zone andthe value of that label must be either antarctica-east1 or antarctica-west1.The node preferably has a label with the key another-node-label-key andthe value another-node-label-value. You can use the operator field to specify a logical operator for Kubernetes to use wheninterpreting the rules. You can use In, NotIn, Exists, DoesNotExist,Gt and Lt. NotIn and DoesNotExist allow you to define node anti-affinity behavior.Alternatively, you can use node taintsto repel Pods from specific nodes. Note:If you specify both nodeSelector and nodeAffinity, both must be satisfiedfor the Pod to be scheduled onto a node.If you specify multiple terms in nodeSelectorTerms associated with nodeAffinitytypes, then the Pod can be scheduled onto a node if one of the specified termscan be satisfied (terms are ORed).If you specify multiple expressions in a single matchExpressions field associated with aterm in nodeSelectorTerms, then the Pod can be scheduled onto a node onlyif all the expressions are satisfied (expressions are ANDed). See Assign Pods to Nodes using Node Affinityfor more information.",668
11.2 - Assigning Pods to Nodes,Node affinity weight,"Node affinity weight You can specify a weight between 1 and 100 for each instance of thepreferredDuringSchedulingIgnoredDuringExecution affinity type. When thescheduler finds nodes that meet all the other scheduling requirements of the Pod, thescheduler iterates through every preferred rule that the node satisfies and adds thevalue of the weight for that expression to a sum. The final sum is added to the score of other priority functions for the node.Nodes with the highest total score are prioritized when the scheduler makes ascheduling decision for the Pod. For example, consider the following Pod spec: pods/pod-with-affinity-anti-affinity.yamlapiVersion: v1kind: Podmetadata:  name: with-affinity-anti-affinityspec:  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:        - matchExpressions:          - key: kubernetes.io/os            operator: In            values:            - linux      preferredDuringSchedulingIgnoredDuringExecution:      - weight: 1        preference:          matchExpressions:          - key: label-1            operator: In            values:            - key-1      - weight: 50        preference:          matchExpressions:          - key: label-2            operator: In            values:            - key-2  containers:  - name: with-node-affinity    image: registry.k8s.io/pause:2.0 If there are two possible nodes that match thepreferredDuringSchedulingIgnoredDuringExecution rule, one with thelabel-1:key-1 label and another with the label-2:key-2 label, the schedulerconsiders the weight of each node and adds the weight to the other scores forthat node, and schedules the Pod onto the node with the highest final score. Note: If you want Kubernetes to successfully schedule the Pods in this example, youmust have existing nodes with the kubernetes.io/os=linux label.",445
11.2 - Assigning Pods to Nodes,Node affinity per scheduling profile,"Node affinity per scheduling profile FEATURE STATE: Kubernetes v1.20 [beta] When configuring multiple scheduling profiles, you can associatea profile with a node affinity, which is useful if a profile only applies to a specific set of nodes.To do so, add an addedAffinity to the args field of the NodeAffinity pluginin the scheduler configuration. For example: apiVersion: kubescheduler.config.k8s.io/v1beta3kind: KubeSchedulerConfigurationprofiles:  - schedulerName: default-scheduler  - schedulerName: foo-scheduler    pluginConfig:      - name: NodeAffinity        args:          addedAffinity:            requiredDuringSchedulingIgnoredDuringExecution:              nodeSelectorTerms:              - matchExpressions:                - key: scheduler-profile                  operator: In                  values:                  - foo The addedAffinity is applied to all Pods that set .spec.schedulerName to foo-scheduler, in addition to theNodeAffinity specified in the PodSpec.That is, in order to match the Pod, nodes need to satisfy addedAffinity andthe Pod's .spec.NodeAffinity. Since the addedAffinity is not visible to end users, its behavior might beunexpected to them. Use node labels that have a clear correlation to thescheduler profile name. Note: The DaemonSet controller, which creates Pods for DaemonSets,does not support scheduling profiles. When the DaemonSet controller createsPods, the default Kubernetes scheduler places those Pods and honors anynodeAffinity rules in the DaemonSet controller.",359
11.2 - Assigning Pods to Nodes,Inter-pod affinity and anti-affinity,"Inter-pod affinity and anti-affinity Inter-pod affinity and anti-affinity allow you to constrain which nodes yourPods can be scheduled on based on the labels of Pods already running on thatnode, instead of the node labels. Inter-pod affinity and anti-affinity rules take the form ""thisPod should (or, in the case of anti-affinity, should not) run in an X if that Xis already running one or more Pods that meet rule Y"", where X is a topologydomain like node, rack, cloud provider zone or region, or similar and Y is therule Kubernetes tries to satisfy. You express these rules (Y) as label selectorswith an optional associated list of namespaces. Pods are namespaced objects inKubernetes, so Pod labels also implicitly have namespaces. Any label selectorsfor Pod labels should specify the namespaces in which Kubernetes should look for thoselabels. You express the topology domain (X) using a topologyKey, which is the key forthe node label that the system uses to denote the domain. For examples, seeWell-Known Labels, Annotations and Taints. Note: Inter-pod affinity and anti-affinity require substantial amount ofprocessing which can slow down scheduling in large clusters significantly. We donot recommend using them in clusters larger than several hundred nodes. Note: Pod anti-affinity requires nodes to be consistently labelled, in other words,every node in the cluster must have an appropriate label matching topologyKey.If some or all nodes are missing the specified topologyKey label, it can leadto unintended behavior.",343
11.2 - Assigning Pods to Nodes,Types of inter-pod affinity and anti-affinity,"Types of inter-pod affinity and anti-affinity Similar to node affinity are two types of Pod affinity andanti-affinity as follows: requiredDuringSchedulingIgnoredDuringExecutionpreferredDuringSchedulingIgnoredDuringExecution For example, you could userequiredDuringSchedulingIgnoredDuringExecution affinity to tell the scheduler toco-locate Pods of two services in the same cloud provider zone because theycommunicate with each other a lot. Similarly, you could usepreferredDuringSchedulingIgnoredDuringExecution anti-affinity to spread Podsfrom a service across multiple cloud provider zones. To use inter-pod affinity, use the affinity.podAffinity field in the Pod spec.For inter-pod anti-affinity, use the affinity.podAntiAffinity field in the Podspec.",174
11.2 - Assigning Pods to Nodes,Pod affinity example,"Pod affinity example Consider the following Pod spec: pods/pod-with-pod-affinity.yamlapiVersion: v1kind: Podmetadata:  name: with-pod-affinityspec:  affinity:    podAffinity:      requiredDuringSchedulingIgnoredDuringExecution:      - labelSelector:          matchExpressions:          - key: security            operator: In            values:            - S1        topologyKey: topology.kubernetes.io/zone    podAntiAffinity:      preferredDuringSchedulingIgnoredDuringExecution:      - weight: 100        podAffinityTerm:          labelSelector:            matchExpressions:            - key: security              operator: In              values:              - S2          topologyKey: topology.kubernetes.io/zone  containers:  - name: with-pod-affinity    image: registry.k8s.io/pause:2.0 This example defines one Pod affinity rule and one Pod anti-affinity rule. ThePod affinity rule uses the ""hard""requiredDuringSchedulingIgnoredDuringExecution, while the anti-affinity ruleuses the ""soft"" preferredDuringSchedulingIgnoredDuringExecution. The affinity rule says that the scheduler can only schedule a Pod onto a node ifthe node is in the same zone as one or more existing Pods with the labelsecurity=S1. More precisely, the scheduler must place the Pod on a node that has thetopology.kubernetes.io/zone=V label, as long as there is at least one node inthat zone that currently has one or more Pods with the Pod label security=S1. The anti-affinity rule says that the scheduler should try to avoid schedulingthe Pod onto a node that is in the same zone as one or more Pods with the labelsecurity=S2. More precisely, the scheduler should try to avoid placing the Pod on a node that has thetopology.kubernetes.io/zone=R label if there are other nodes in thesame zone currently running Pods with the Security=S2 Pod label. To get yourself more familiar with the examples of Pod affinity and anti-affinity,refer to the design proposal. You can use the In, NotIn, Exists and DoesNotExist values in theoperator field for Pod affinity and anti-affinity. In principle, the topologyKey can be any allowed label key with the followingexceptions for performance and security reasons: For Pod affinity and anti-affinity, an empty topologyKey field is not allowed in both requiredDuringSchedulingIgnoredDuringExecutionand preferredDuringSchedulingIgnoredDuringExecution.For requiredDuringSchedulingIgnoredDuringExecution Pod anti-affinity rules,the admission controller LimitPodHardAntiAffinityTopology limitstopologyKey to kubernetes.io/hostname. You can modify or disable theadmission controller if you want to allow custom topologies. In addition to labelSelector and topologyKey, you can optionally specify a listof namespaces which the labelSelector should match against using thenamespaces field at the same level as labelSelector and topologyKey.If omitted or empty, namespaces defaults to the namespace of the Pod where theaffinity/anti-affinity definition appears.",714
11.2 - Assigning Pods to Nodes,Namespace selector,"Namespace selector FEATURE STATE: Kubernetes v1.24 [stable] You can also select matching namespaces using namespaceSelector, which is a label query over the set of namespaces.The affinity term is applied to namespaces selected by both namespaceSelector and the namespaces field.Note that an empty namespaceSelector ({}) matches all namespaces, while a null or empty namespaces list andnull namespaceSelector matches the namespace of the Pod where the rule is defined.",101
11.2 - Assigning Pods to Nodes,More practical use-cases,"More practical use-cases Inter-pod affinity and anti-affinity can be even more useful when they are used with higherlevel collections such as ReplicaSets, StatefulSets, Deployments, etc. Theserules allow you to configure that a set of workloads shouldbe co-located in the same defined topology; for example, preferring to place two relatedPods onto the same node. For example: imagine a three-node cluster. You use the cluster to run a web applicationand also an in-memory cache (such as Redis). For this example, also assume that latency betweenthe web application and the memory cache should be as low as is practical. You could use inter-podaffinity and anti-affinity to co-locate the web servers with the cache as much as possible. In the following example Deployment for the Redis cache, the replicas get the label app=store. ThepodAntiAffinity rule tells the scheduler to avoid placing multiple replicaswith the app=store label on a single node. This creates each cache in aseparate node. apiVersion: apps/v1kind: Deploymentmetadata:  name: redis-cachespec:  selector:    matchLabels:      app: store  replicas: 3  template:    metadata:      labels:        app: store    spec:      affinity:        podAntiAffinity:          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: app                operator: In                values:                - store            topologyKey: ""kubernetes.io/hostname""      containers:      - name: redis-server        image: redis:3.2-alpine The following example Deployment for the web servers creates replicas with the label app=web-store.The Pod affinity rule tells the scheduler to place each replica on a node that has a Podwith the label app=store. The Pod anti-affinity rule tells the scheduler never to placemultiple app=web-store servers on a single node. apiVersion: apps/v1kind: Deploymentmetadata:  name: web-serverspec:  selector:    matchLabels:      app: web-store  replicas: 3  template:    metadata:      labels:        app: web-store    spec:      affinity:        podAntiAffinity:          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: app                operator: In                values:                - web-store            topologyKey: ""kubernetes.io/hostname""        podAffinity:          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: app                operator: In                values:                - store            topologyKey: ""kubernetes.io/hostname""      containers:      - name: web-app        image: nginx:1.16-alpine Creating the two preceding Deployments results in the following cluster layout,where each web server is co-located with a cache, on three separate nodes. node-1node-2node-3webserver-1webserver-2webserver-3cache-1cache-2cache-3 The overall effect is that each cache instance is likely to be accessed by a single client, thatis running on the same node. This approach aims to minimize both skew (imbalanced load) and latency. You might have other reasons to use Pod anti-affinity.See the ZooKeeper tutorialfor an example of a StatefulSet configured with anti-affinity for highavailability, using the same technique as this example.",803
11.2 - Assigning Pods to Nodes,nodeName,"nodeName nodeName is a more direct form of node selection than affinity ornodeSelector. nodeName is a field in the Pod spec. If the nodeName fieldis not empty, the scheduler ignores the Pod and the kubelet on the named nodetries to place the Pod on that node. Using nodeName overrules usingnodeSelector or affinity and anti-affinity rules. Some of the limitations of using nodeName to select nodes are: If the named node does not exist, the Pod will not run, and insome cases may be automatically deleted.If the named node does not have the resources to accommodate thePod, the Pod will fail and its reason will indicate why,for example OutOfmemory or OutOfcpu.Node names in cloud environments are not always predictable orstable. Note: nodeName is intended for use by custom schedulers or advanced use cases whereyou need to bypass any configured schedulers. Bypassing the schedulers might lead tofailed Pods if the assigned Nodes get oversubscribed. You can use node affinity or a the nodeselector field to assign a Pod to a specific Node without bypassing the schedulers. Here is an example of a Pod spec using the nodeName field: apiVersion: v1kind: Podmetadata:  name: nginxspec:  containers:  - name: nginx    image: nginx  nodeName: kube-01 The above Pod will only run on the node kube-01.",311
11.2 - Assigning Pods to Nodes,Pod topology spread constraints,"Pod topology spread constraints You can use topology spread constraints to control how Podsare spread across your cluster among failure-domains such as regions, zones, nodes, or among any othertopology domains that you define. You might do this to improve performance, expected availability, oroverall utilization. Read Pod topology spread constraintsto learn more about how these work. Read more about taints and tolerations .Read the design docs for node affinityand for inter-pod affinity/anti-affinity.Learn about how the topology manager takes part in node-levelresource allocation decisions.Learn how to use nodeSelector.Learn how to use affinity and anti-affinity.",142
11.3 - Pod Overhead,default,"FEATURE STATE: Kubernetes v1.24 [stable] When you run a Pod on a Node, the Pod itself takes an amount of system resources. Theseresources are additional to the resources needed to run the container(s) inside the Pod.In Kubernetes, Pod Overhead is a way to account for the resources consumed by the Podinfrastructure on top of the container requests & limits. In Kubernetes, the Pod's overhead is set atadmissiontime according to the overhead associated with the Pod'sRuntimeClass. A pod's overhead is considered in addition to the sum of container resource requests whenscheduling a Pod. Similarly, the kubelet will include the Pod overhead when sizing the Pod cgroup,and when carrying out Pod eviction ranking.",161
11.3 - Pod Overhead,Usage example,"Usage example To work with Pod overhead, you need a RuntimeClass that defines the overhead field. Asan example, you could use the following RuntimeClass definition with a virtualization containerruntime that uses around 120MiB per Pod for the virtual machine and the guest OS: apiVersion: node.k8s.io/v1kind: RuntimeClassmetadata:  name: kata-fchandler: kata-fcoverhead:  podFixed:    memory: ""120Mi""    cpu: ""250m"" Workloads which are created which specify the kata-fc RuntimeClass handler will take the memory andcpu overheads into account for resource quota calculations, node scheduling, as well as Pod cgroup sizing. Consider running the given example workload, test-pod: apiVersion: v1kind: Podmetadata:  name: test-podspec:  runtimeClassName: kata-fc  containers:  - name: busybox-ctr    image: busybox:1.28    stdin: true    tty: true    resources:      limits:        cpu: 500m        memory: 100Mi  - name: nginx-ctr    image: nginx    resources:      limits:        cpu: 1500m        memory: 100Mi At admission time the RuntimeClass admission controllerupdates the workload's PodSpec to include the overhead as described in the RuntimeClass. If the PodSpec already has this field defined,the Pod will be rejected. In the given example, since only the RuntimeClass name is specified, the admission controller mutates the Podto include an overhead. After the RuntimeClass admission controller has made modifications, you can check the updatedPod overhead value: kubectl get pod test-pod -o jsonpath='{.spec.overhead}' The output is: map[cpu:250m memory:120Mi] If a ResourceQuota is defined, the sum of container requests as well as theoverhead field are counted. When the kube-scheduler is deciding which node should run a new Pod, the scheduler considers that Pod'soverhead as well as the sum of container requests for that Pod. For this example, the scheduler adds therequests and the overhead, then looks for a node that has 2.25 CPU and 320 MiB of memory available. Once a Pod is scheduled to a node, the kubelet on that node creates a new cgroup for the Pod. It is within this pod that the underlyingcontainer runtime will create containers. If the resource has a limit defined for each container (Guaranteed QoS or Burstable QoS with limits defined),the kubelet will set an upper limit for the pod cgroup associated with that resource (cpu.cfs_quota_us for CPUand memory.limit_in_bytes memory). This upper limit is based on the sum of the container limits plus the overheaddefined in the PodSpec. For CPU, if the Pod is Guaranteed or Burstable QoS, the kubelet will set cpu.shares based on thesum of container requests plus the overhead defined in the PodSpec. Looking at our example, verify the container requests for the workload: kubectl get pod test-pod -o jsonpath='{.spec.containers[*].resources.limits}' The total container requests are 2000m CPU and 200MiB of memory: map[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi] Check this against what is observed by the node: kubectl describe node | grep test-pod -B2 The output shows requests for 2250m CPU, and for 320MiB of memory. The requests include Pod overhead: Namespace    Name       CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE  ---------    ----       ------------  ----------   ---------------  -------------  ---  default      test-pod   2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m",849
11.3 - Pod Overhead,Verify Pod cgroup limits,"Verify Pod cgroup limits Check the Pod's memory cgroups on the node where the workload is running. In the following example,crictlis used on the node, which provides a CLI for CRI-compatible container runtimes. This is anadvanced example to show Pod overhead behavior, and it is not expected that users should need to checkcgroups directly on the node. First, on the particular node, determine the Pod identifier: # Run this on the node where the Pod is scheduledPOD_ID=""$(sudo crictl pods --name test-pod -q)"" From this, you can determine the cgroup path for the Pod: # Run this on the node where the Pod is scheduledsudo crictl inspectp -o=json $POD_ID | grep cgroupsPath The resulting cgroup path includes the Pod's pause container. The Pod level cgroup is one directory above. ""cgroupsPath"": ""/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a"" In this specific case, the pod cgroup path is kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2.Verify the Pod level cgroup setting for memory: # Run this on the node where the Pod is scheduled.# Also, change the name of the cgroup to match the cgroup allocated for your pod. cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes This is 320 MiB, as expected: 335544320",415
11.3 - Pod Overhead,Observability,Observability Some kube_pod_overhead_* metrics are available in kube-state-metricsto help identify when Pod overhead is being utilized and to help observe stability of workloadsrunning with a defined overhead. Learn more about RuntimeClassRead the PodOverhead Designenhancement proposal for extra context,66
11.4 - Pod Scheduling Readiness,default,"FEATURE STATE: Kubernetes v1.26 [alpha] Pods were considered ready for scheduling once created. Kubernetes schedulerdoes its due diligence to find nodes to place all pending Pods. However, in areal-world case, some Pods may stay in a ""miss-essential-resources"" state for a long period.These Pods actually churn the scheduler (and downstream integrators like Cluster AutoScaler)in an unnecessary manner. By specifying/removing a Pod's .spec.schedulingGates, you can control when a Pod is readyto be considered for scheduling.",129
11.4 - Pod Scheduling Readiness,Configuring Pod schedulingGates,"Configuring Pod schedulingGates The schedulingGates field contains a list of strings, and each string literal is perceived as acriteria that Pod should be satisfied before considered schedulable. This field can be initializedonly when a Pod is created (either by the client, or mutated during admission). After creation,each schedulingGate can be removed in arbitrary order, but addition of a new scheduling gate is disallowed. Figure. Pod SchedulingGates",91
11.4 - Pod Scheduling Readiness,Usage example,"Usage example To mark a Pod not-ready for scheduling, you can create it with one or more scheduling gates like this: pods/pod-with-scheduling-gates.yamlapiVersion: v1kind: Podmetadata:  name: test-podspec:  schedulingGates:  - name: foo  - name: bar  containers:  - name: pause    image: registry.k8s.io/pause:3.6 After the Pod's creation, you can check its state using: kubectl get pod test-pod The output reveals it's in SchedulingGated state: NAME       READY   STATUS            RESTARTS   AGEtest-pod   0/1     SchedulingGated   0          7s You can also check its schedulingGates field by running: kubectl get pod test-pod -o jsonpath='{.spec.schedulingGates}' The output is: [{""name"":""foo""},{""name"":""bar""}] To inform scheduler this Pod is ready for scheduling, you can remove its schedulingGates entirelyby re-applying a modified manifest: pods/pod-without-scheduling-gates.yamlapiVersion: v1kind: Podmetadata:  name: test-podspec:  containers:  - name: pause    image: registry.k8s.io/pause:3.6 You can check if the schedulingGates is cleared by running: kubectl get pod test-pod -o jsonpath='{.spec.schedulingGates}' The output is expected to be empty. And you can check its latest status by running: kubectl get pod test-pod -o wide Given the test-pod doesn't request any CPU/memory resources, it's expected that this Pod's state gettransited from previous SchedulingGated to Running: NAME       READY   STATUS    RESTARTS   AGE   IP         NODE  test-pod   1/1     Running   0          15s   10.0.0.4   node-2",447
11.4 - Pod Scheduling Readiness,Observability,"Observability The metric scheduler_pending_pods comes with a new label ""gated"" to distinguish whether a Podhas been tried scheduling but claimed as unschedulable, or explicitly marked as not ready forscheduling. You can use scheduler_pending_pods{queue=""gated""} to check the metric result. Read the PodSchedulingReadiness KEP for more details",86
11.5 - Pod Topology Spread Constraints,default,"You can use topology spread constraints to control howPods are spread across your clusteramong failure-domains such as regions, zones, nodes, and other user-defined topologydomains. This can help to achieve high availability as well as efficient resourceutilization. You can set cluster-level constraints as a default,or configure topology spread constraints for individual workloads.",77
11.5 - Pod Topology Spread Constraints,Motivation,"Motivation Imagine that you have a cluster of up to twenty nodes, and you want to run aworkloadthat automatically scales how many replicas it uses. There could be as few astwo Pods or as many as fifteen.When there are only two Pods, you'd prefer not to have both of those Pods run on thesame node: you would run the risk that a single node failure takes your workloadoffline. In addition to this basic usage, there are some advanced usage examples thatenable your workloads to benefit on high availability and cluster utilization. As you scale up and run more Pods, a different concern becomes important. Imaginethat you have three nodes running five Pods each. The nodes have enough capacityto run that many replicas; however, the clients that interact with this workloadare split across three different datacenters (or infrastructure zones). Now youhave less concern about a single node failure, but you notice that latency ishigher than you'd like, and you are paying for network costs associated withsending network traffic between the different zones. You decide that under normal operation you'd prefer to have a similar number of replicasscheduled into each infrastructure zone,and you'd like the cluster to self-heal in the case that there is a problem. Pod topology spread constraints offer you a declarative way to configure that.",281
11.5 - Pod Topology Spread Constraints,topologySpreadConstraints field,"topologySpreadConstraints field The Pod API includes a field, spec.topologySpreadConstraints. The usage of this field looks likethe following: ---apiVersion: v1kind: Podmetadata:  name: example-podspec:  # Configure a topology spread constraint  topologySpreadConstraints:    - maxSkew: <integer>      minDomains: <integer> # optional; beta since v1.25      topologyKey: <string>      whenUnsatisfiable: <string>      labelSelector: <object>      matchLabelKeys: <list> # optional; alpha since v1.25      nodeAffinityPolicy: [Honor|Ignore] # optional; beta since v1.26      nodeTaintsPolicy: [Honor|Ignore] # optional; beta since v1.26  ### other Pod fields go here You can read more about this field by running kubectl explain Pod.spec.topologySpreadConstraints orrefer to scheduling section of the API reference for Pod.",227
11.5 - Pod Topology Spread Constraints,Spread constraint definition,"Spread constraint definition You can define one or multiple topologySpreadConstraints entries to instruct thekube-scheduler how to place each incoming Pod in relation to the existing Pods acrossyour cluster. Those fields are: maxSkew describes the degree to which Pods may be unevenly distributed. You mustspecify this field and the number must be greater than zero. Its semantics differaccording to the value of whenUnsatisfiable:if you select whenUnsatisfiable: DoNotSchedule, then maxSkew defines themaximum permitted difference between the number of matching pods in the targettopology and the global minimum(the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains).For example, if you have 3 zones with 2, 2 and 1 matching pods respectively,MaxSkew is set to 1 then the global minimum is 1.if you select whenUnsatisfiable: ScheduleAnyway, the scheduler gives higherprecedence to topologies that would help reduce the skew.minDomains indicates a minimum number of eligible domains. This field is optional.A domain is a particular instance of a topology. An eligible domain is a domain whosenodes match the node selector.Note: The minDomains field is a beta field and disabled by default in 1.25. You can enable it by enabling theMinDomainsInPodTopologySpread feature gate.The value of minDomains must be greater than 0, when specified.You can only specify minDomains in conjunction with whenUnsatisfiable: DoNotSchedule.When the number of eligible domains with match topology keys is less than minDomains,Pod topology spread treats global minimum as 0, and then the calculation of skew is performed.The global minimum is the minimum number of matching Pods in an eligible domain,or zero if the number of eligible domains is less than minDomains.When the number of eligible domains with matching topology keys equals or is greater thanminDomains, this value has no effect on scheduling.If you do not specify minDomains, the constraint behaves as if minDomains is 1.topologyKey is the key of node labels. Nodes that have a label with this keyand identical values are considered to be in the same topology.We call each instance of a topology (in other words, a <key, value> pair) a domain. The schedulerwill try to put a balanced number of pods into each domain.Also, we define an eligible domain as a domain whose nodes meet the requirements ofnodeAffinityPolicy and nodeTaintsPolicy.whenUnsatisfiable indicates how to deal with a Pod if it doesn't satisfy the spread constraint:DoNotSchedule (default) tells the scheduler not to schedule it.ScheduleAnyway tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.labelSelector is used to find matching Pods. Podsthat match this label selector are counted to determine thenumber of Pods in their corresponding topology domain.See Label Selectorsfor more details.matchLabelKeys is a list of pod label keys to select the pods over whichspreading will be calculated. The keys are used to lookup values from the pod labels, those key-value labels are ANDed with labelSelector to select the group of existing pods over which spreading will be calculated for the incoming pod. Keys that don't exist in the pod labels will be ignored. A null or empty list means only match against the labelSelector.With matchLabelKeys, users don't need to update the pod.spec between different revisions. The controller/operator just needs to set different values to the same label key for different revisions. The scheduler will assume the values automatically based on matchLabelKeys. For example, if users use Deployment, they can use the label keyed with pod-template-hash, which is added automatically by the Deployment controller, to distinguish between different revisions in a single Deployment.    topologySpreadConstraints:        - maxSkew: 1          topologyKey: kubernetes.io/hostname          whenUnsatisfiable: DoNotSchedule          matchLabelKeys:            - app            - pod-template-hashNote: The matchLabelKeys field is an alpha field added in 1.25. You have to enable theMatchLabelKeysInPodTopologySpread feature gatein order to use it.nodeAffinityPolicy indicates how we will treat Pod's nodeAffinity/nodeSelectorwhen calculating pod topology spread skew. Options are:Honor: only nodes matching nodeAffinity/nodeSelector are included in the calculations.Ignore: nodeAffinity/nodeSelector are ignored. All nodes are included in the calculations.If this value is null, the behavior is equivalent to the Honor policy.Note: The nodeAffinityPolicy is a beta-level field and enabled by default in 1.26. You can disable it by disabling theNodeInclusionPolicyInPodTopologySpread feature gate.nodeTaintsPolicy indicates how we will treat node taints when calculatingpod topology spread skew. Options are:Honor: nodes without taints, along with tainted nodes for which the incoming podhas a toleration, are included.Ignore: node taints are ignored. All nodes are included.If this value is null, the behavior is equivalent to the Ignore policy.Note: The nodeTaintsPolicy is a beta-level field and enabled by default in 1.26. You can disable it by disabling theNodeInclusionPolicyInPodTopologySpread feature gate. When a Pod defines more than one topologySpreadConstraint, those constraints arecombined using a logical AND operation: the kube-scheduler looks for a node for the incoming Podthat satisfies all the configured constraints.",1220
11.5 - Pod Topology Spread Constraints,Node labels,"Node labels Topology spread constraints rely on node labels to identify the topologydomain(s) that each node is in.For example, a node might have labels: region: us-east-1  zone: us-east-1a Note:For brevity, this example doesn't use thewell-known label keystopology.kubernetes.io/zone and topology.kubernetes.io/region. However,those registered label keys are nonetheless recommended rather than the private(unqualified) label keys region and zone that are used here.You can't make a reliable assumption about the meaning of a private label keybetween different contexts. Suppose you have a 4-node cluster with the following labels: NAME    STATUS   ROLES    AGE     VERSION   LABELSnode1   Ready    <none>   4m26s   v1.16.0   node=node1,zone=zoneAnode2   Ready    <none>   3m58s   v1.16.0   node=node2,zone=zoneAnode3   Ready    <none>   3m17s   v1.16.0   node=node3,zone=zoneBnode4   Ready    <none>   2m43s   v1.16.0   node=node4,zone=zoneB Then the cluster is logically viewed as below: graph TBsubgraph ""zoneB""n3(Node3)n4(Node4)endsubgraph ""zoneA""n1(Node1)n2(Node2)endclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;class n1,n2,n3,n4 k8s;class zoneA,zoneB cluster; JavaScript must be enabled to view this content",444
11.5 - Pod Topology Spread Constraints,Consistency,"Consistency You should set the same Pod topology spread constraints on all pods in a group. Usually, if you are using a workload controller such as a Deployment, the pod templatetakes care of this for you. If you mix different spread constraints then Kubernetesfollows the API definition of the field; however, the behavior is more likely to becomeconfusing and troubleshooting is less straightforward. You need a mechanism to ensure that all the nodes in a topology domain (such as acloud provider region) are labelled consistently.To avoid you needing to manually label nodes, most clusters automaticallypopulate well-known labels such as topology.kubernetes.io/hostname. Check whetheryour cluster supports this.",153
11.5 - Pod Topology Spread Constraints,Example: one topology spread constraint,"Example: one topology spread constraint Suppose you have a 4-node cluster where 3 Pods labelled foo: bar are located innode1, node2 and node3 respectively: graph BTsubgraph ""zoneB""p3(Pod) --> n3(Node3)n4(Node4)endsubgraph ""zoneA""p1(Pod) --> n1(Node1)p2(Pod) --> n2(Node2)endclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;class n1,n2,n3,n4,p1,p2,p3 k8s;class zoneA,zoneB cluster; JavaScript must be enabled to view this content If you want an incoming Pod to be evenly spread with existing Pods across zones, youcan use a manifest similar to: pods/topology-spread-constraints/one-constraint.yamlkind: PodapiVersion: v1metadata:  name: mypod  labels:    foo: barspec:  topologySpreadConstraints:  - maxSkew: 1    topologyKey: zone    whenUnsatisfiable: DoNotSchedule    labelSelector:      matchLabels:        foo: bar  containers:  - name: pause    image: registry.k8s.io/pause:3.1 From that manifest, topologyKey: zone implies the even distribution will only be appliedto nodes that are labelled zone: <any value> (nodes that don't have a zone labelare skipped). The field whenUnsatisfiable: DoNotSchedule tells the scheduler to let theincoming Pod stay pending if the scheduler can't find a way to satisfy the constraint. If the scheduler placed this incoming Pod into zone A, the distribution of Pods wouldbecome [3, 1]. That means the actual skew is then 2 (calculated as 3 - 1), whichviolates maxSkew: 1. To satisfy the constraints and context for this example, theincoming Pod can only be placed onto a node in zone B: graph BTsubgraph ""zoneB""p3(Pod) --> n3(Node3)p4(mypod) --> n4(Node4)endsubgraph ""zoneA""p1(Pod) --> n1(Node1)p2(Pod) --> n2(Node2)endclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;class n1,n2,n3,n4,p1,p2,p3 k8s;class p4 plain;class zoneA,zoneB cluster; JavaScript must be enabled to view this content OR graph BTsubgraph ""zoneB""p3(Pod) --> n3(Node3)p4(mypod) --> n3n4(Node4)endsubgraph ""zoneA""p1(Pod) --> n1(Node1)p2(Pod) --> n2(Node2)endclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;class n1,n2,n3,n4,p1,p2,p3 k8s;class p4 plain;class zoneA,zoneB cluster; JavaScript must be enabled to view this content You can tweak the Pod spec to meet various kinds of requirements: Change maxSkew to a bigger value - such as 2 - so that the incoming Pod canbe placed into zone A as well.Change topologyKey to node so as to distribute the Pods evenly across nodesinstead of zones. In the above example, if maxSkew remains 1, the incomingPod can only be placed onto the node node4.Change whenUnsatisfiable: DoNotSchedule to whenUnsatisfiable: ScheduleAnywayto ensure the incoming Pod to be always schedulable (suppose other scheduling APIsare satisfied). However, it's preferred to be placed into the topology domain whichhas fewer matching Pods. (Be aware that this preference is jointly normalizedwith other internal scheduling priorities such as resource usage ratio).",1043
11.5 - Pod Topology Spread Constraints,Example: multiple topology spread constraints,"Example: multiple topology spread constraints This builds upon the previous example. Suppose you have a 4-node cluster where 3existing Pods labeled foo: bar are located on node1, node2 and node3 respectively: graph BTsubgraph ""zoneB""p3(Pod) --> n3(Node3)n4(Node4)endsubgraph ""zoneA""p1(Pod) --> n1(Node1)p2(Pod) --> n2(Node2)endclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;class n1,n2,n3,n4,p1,p2,p3 k8s;class p4 plain;class zoneA,zoneB cluster; JavaScript must be enabled to view this content You can combine two topology spread constraints to control the spread of Pods bothby node and by zone: pods/topology-spread-constraints/two-constraints.yamlkind: PodapiVersion: v1metadata:  name: mypod  labels:    foo: barspec:  topologySpreadConstraints:  - maxSkew: 1    topologyKey: zone    whenUnsatisfiable: DoNotSchedule    labelSelector:      matchLabels:        foo: bar  - maxSkew: 1    topologyKey: node    whenUnsatisfiable: DoNotSchedule    labelSelector:      matchLabels:        foo: bar  containers:  - name: pause    image: registry.k8s.io/pause:3.1 In this case, to match the first constraint, the incoming Pod can only be placed ontonodes in zone B; while in terms of the second constraint, the incoming Pod can only bescheduled to the node node4. The scheduler only considers options that satisfy alldefined constraints, so the only valid placement is onto node node4.",466
11.5 - Pod Topology Spread Constraints,Example: conflicting topology spread constraints,"Example: conflicting topology spread constraints Multiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones: graph BTsubgraph ""zoneB""p4(Pod) --> n3(Node3)p5(Pod) --> n3endsubgraph ""zoneA""p1(Pod) --> n1(Node1)p2(Pod) --> n1p3(Pod) --> n2(Node2)endclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;class n1,n2,n3,n4,p1,p2,p3,p4,p5 k8s;class zoneA,zoneB cluster; JavaScript must be enabled to view this content If you were to applytwo-constraints.yaml(the manifest from the previous example)to this cluster, you would see that the Pod mypod stays in the Pending state.This happens because: to satisfy the first constraint, the Pod mypod can onlybe placed into zone B; while in terms of the second constraint, the Pod mypodcan only schedule to node node2. The intersection of the two constraints returnsan empty set, and the scheduler cannot place the Pod. To overcome this situation, you can either increase the value of maxSkew or modifyone of the constraints to use whenUnsatisfiable: ScheduleAnyway. Depending oncircumstances, you might also decide to delete an existing Pod manually - for example,if you are troubleshooting why a bug-fix rollout is not making progress.",387
11.5 - Pod Topology Spread Constraints,Interaction with node affinity and node selectors,Interaction with node affinity and node selectors The scheduler will skip the non-matching nodes from the skew calculations if theincoming Pod has spec.nodeSelector or spec.affinity.nodeAffinity defined.,46
11.5 - Pod Topology Spread Constraints,Example: topology spread constraints with node affinity,"Example: topology spread constraints with node affinity Suppose you have a 5-node cluster ranging across zones A to C: graph BTsubgraph ""zoneB""p3(Pod) --> n3(Node3)n4(Node4)endsubgraph ""zoneA""p1(Pod) --> n1(Node1)p2(Pod) --> n2(Node2)endclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;class n1,n2,n3,n4,p1,p2,p3 k8s;class p4 plain;class zoneA,zoneB cluster; JavaScript must be enabled to view this content graph BTsubgraph ""zoneC""n5(Node5)endclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;class n5 k8s;class zoneC cluster; JavaScript must be enabled to view this content and you know that zone C must be excluded. In this case, you can compose a manifestas below, so that Pod mypod will be placed into zone B instead of zone C.Similarly, Kubernetes also respects spec.nodeSelector. pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yamlkind: PodapiVersion: v1metadata:  name: mypod  labels:    foo: barspec:  topologySpreadConstraints:  - maxSkew: 1    topologyKey: zone    whenUnsatisfiable: DoNotSchedule    labelSelector:      matchLabels:        foo: bar  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:        - matchExpressions:          - key: zone            operator: NotIn            values:            - zoneC  containers:  - name: pause    image: registry.k8s.io/pause:3.1",533
11.5 - Pod Topology Spread Constraints,Implicit conventions,"Implicit conventions There are some implicit conventions worth noting here: Only the Pods holding the same namespace as the incoming Pod can be matching candidates.The scheduler bypasses any nodes that don't have any topologySpreadConstraints[*].topologyKeypresent. This implies that:any Pods located on those bypassed nodes do not impact maxSkew calculation - in theabove example, suppose the node node1 does not have a label ""zone"", then the 2 Pods willbe disregarded, hence the incoming Pod will be scheduled into zone A.the incoming Pod has no chances to be scheduled onto this kind of nodes -in the above example, suppose a node node5 has the mistyped label zone-typo: zoneC(and no zone label set). After node node5 joins the cluster, it will be bypassed andPods for this workload aren't scheduled there.Be aware of what will happen if the incoming Pod'stopologySpreadConstraints[*].labelSelector doesn't match its own labels. In theabove example, if you remove the incoming Pod's labels, it can still be placed ontonodes in zone B, since the constraints are still satisfied. However, after thatplacement, the degree of imbalance of the cluster remains unchanged - it's still zone Ahaving 2 Pods labelled as foo: bar, and zone B having 1 Pod labelled asfoo: bar. If this is not what you expect, update the workload'stopologySpreadConstraints[*].labelSelector to match the labels in the pod template.",325
11.5 - Pod Topology Spread Constraints,Cluster-level default constraints,"Cluster-level default constraints It is possible to set default topology spread constraints for a cluster. Defaulttopology spread constraints are applied to a Pod if, and only if: It doesn't define any constraints in its .spec.topologySpreadConstraints.It belongs to a Service, ReplicaSet, StatefulSet or ReplicationController. Default constraints can be set as part of the PodTopologySpread pluginarguments in a scheduling profile.The constraints are specified with the same API above, except thatlabelSelector must be empty. The selectors are calculated from the Services,ReplicaSets, StatefulSets or ReplicationControllers that the Pod belongs to. An example configuration might look like follows: apiVersion: kubescheduler.config.k8s.io/v1beta3kind: KubeSchedulerConfigurationprofiles:  - schedulerName: default-scheduler    pluginConfig:      - name: PodTopologySpread        args:          defaultConstraints:            - maxSkew: 1              topologyKey: topology.kubernetes.io/zone              whenUnsatisfiable: ScheduleAnyway          defaultingType: List Note: The SelectorSpread pluginis disabled by default. The Kubernetes project recommends using PodTopologySpreadto achieve similar behavior.",280
11.5 - Pod Topology Spread Constraints,Built-in default constraints,"Built-in default constraints FEATURE STATE: Kubernetes v1.24 [stable] If you don't configure any cluster-level default constraints for pod topology spreading,then kube-scheduler acts as if you specified the following default topology constraints: defaultConstraints:  - maxSkew: 3    topologyKey: ""kubernetes.io/hostname""    whenUnsatisfiable: ScheduleAnyway  - maxSkew: 5    topologyKey: ""topology.kubernetes.io/zone""    whenUnsatisfiable: ScheduleAnyway Also, the legacy SelectorSpread plugin, which provides an equivalent behavior,is disabled by default. Note:The PodTopologySpread plugin does not score the nodes that don't havethe topology keys specified in the spreading constraints. This might resultin a different default behavior compared to the legacy SelectorSpread plugin whenusing the default topology constraints.If your nodes are not expected to have both kubernetes.io/hostname andtopology.kubernetes.io/zone labels set, define your own constraintsinstead of using the Kubernetes defaults. If you don't want to use the default Pod spreading constraints for your cluster,you can disable those defaults by setting defaultingType to List and leavingempty defaultConstraints in the PodTopologySpread plugin configuration: apiVersion: kubescheduler.config.k8s.io/v1beta3kind: KubeSchedulerConfigurationprofiles:  - schedulerName: default-scheduler    pluginConfig:      - name: PodTopologySpread        args:          defaultConstraints: []          defaultingType: List",361
11.5 - Pod Topology Spread Constraints,Comparison with podAffinity and podAntiAffinity,"Comparison with podAffinity and podAntiAffinity In Kubernetes, inter-Pod affinity and anti-affinitycontrol how Pods are scheduled in relation to one another - either more packedor more scattered. podAffinityattracts Pods; you can try to pack any number of Pods into qualifyingtopology domain(s).podAntiAffinityrepels Pods. If you set this to requiredDuringSchedulingIgnoredDuringExecution mode thenonly a single Pod can be scheduled into a single topology domain; if you choosepreferredDuringSchedulingIgnoredDuringExecution then you lose the ability to enforce theconstraint. For finer control, you can specify topology spread constraints to distributePods across different topology domains - to achieve either high availability orcost-saving. This can also help on rolling update workloads and scaling outreplicas smoothly. For more context, see theMotivationsection of the enhancement proposal about Pod topology spread constraints.",205
11.5 - Pod Topology Spread Constraints,Known limitations,"Known limitations There's no guarantee that the constraints remain satisfied when Pods are removed. Forexample, scaling down a Deployment may result in imbalanced Pods distribution.You can use a tool such as the Deschedulerto rebalance the Pods distribution.Pods matched on tainted nodes are respected.See Issue 80921.The scheduler doesn't have prior knowledge of all the zones or other topologydomains that a cluster has. They are determined from the existing nodes in thecluster. This could lead to a problem in autoscaled clusters, when a node pool (ornode group) is scaled to zero nodes, and you're expecting the cluster to scale up,because, in this case, those topology domains won't be considered until there isat least one node in them.You can work around this by using an cluster autoscaling tool that is aware ofPod topology spread constraints and is also aware of the overall set of topologydomains. The blog article Introducing PodTopologySpreadexplains maxSkew in some detail, as well as covering some advanced usage examples.Read the scheduling section ofthe API reference for Pod.",244
11.6 - Taints and Tolerations,default,Node affinityis a property of Pods that attracts them toa set of nodes (either as a preference or ahard requirement). Taints are the opposite -- they allow a node to repel a set of pods. Tolerations are applied to pods. Tolerations allow the scheduler to schedule pods with matchingtaints. Tolerations allow scheduling but don't guarantee scheduling: the scheduler alsoevaluates other parametersas part of its function. Taints and tolerations work together to ensure that pods are not scheduledonto inappropriate nodes. One or more taints are applied to a node; thismarks that the node should not accept any pods that do not tolerate the taints.,139
11.6 - Taints and Tolerations,Concepts,"Concepts You add a taint to a node using kubectl taint.For example, kubectl taint nodes node1 key1=value1:NoSchedule places a taint on node node1. The taint has key key1, value value1, and taint effect NoSchedule.This means that no pod will be able to schedule onto node1 unless it has a matching toleration. To remove the taint added by the command above, you can run: kubectl taint nodes node1 key1=value1:NoSchedule- You specify a toleration for a pod in the PodSpec. Both of the following tolerations ""match"" thetaint created by the kubectl taint line above, and thus a pod with either toleration would be ableto schedule onto node1: tolerations:- key: ""key1""  operator: ""Equal""  value: ""value1""  effect: ""NoSchedule"" tolerations:- key: ""key1""  operator: ""Exists""  effect: ""NoSchedule"" Here's an example of a pod that uses tolerations: pods/pod-with-toleration.yamlapiVersion: v1kind: Podmetadata:  name: nginx  labels:    env: testspec:  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent  tolerations:  - key: ""example-key""    operator: ""Exists""    effect: ""NoSchedule"" The default value for operator is Equal. A toleration ""matches"" a taint if the keys are the same and the effects are the same, and: the operator is Exists (in which case no value should be specified), orthe operator is Equal and the values are equal. Note:There are two special cases:An empty key with operator Exists matches all keys, values and effects which means thiswill tolerate everything.An empty effect matches all effects with key key1. The above example used effect of NoSchedule. Alternatively, you can use effect of PreferNoSchedule.This is a ""preference"" or ""soft"" version of NoSchedule -- the system will try to avoid placing apod that does not tolerate the taint on the node, but it is not required. The third kind of effect isNoExecute, described later. You can put multiple taints on the same node and multiple tolerations on the same pod.The way Kubernetes processes multiple taints and tolerations is like a filter: startwith all of a node's taints, then ignore the ones for which the pod has a matching toleration; theremaining un-ignored taints have the indicated effects on the pod. In particular, if there is at least one un-ignored taint with effect NoSchedule then Kubernetes will not schedulethe pod onto that nodeif there is no un-ignored taint with effect NoSchedule but there is at least one un-ignored taint witheffect PreferNoSchedule then Kubernetes will try to not schedule the pod onto the nodeif there is at least one un-ignored taint with effect NoExecute then the pod will be evicted fromthe node (if it is already running on the node), and will not bescheduled onto the node (if it is not yet running on the node). For example, imagine you taint a node like this kubectl taint nodes node1 key1=value1:NoSchedulekubectl taint nodes node1 key1=value1:NoExecutekubectl taint nodes node1 key2=value2:NoSchedule And a pod has two tolerations: tolerations:- key: ""key1""  operator: ""Equal""  value: ""value1""  effect: ""NoSchedule""- key: ""key1""  operator: ""Equal""  value: ""value1""  effect: ""NoExecute"" In this case, the pod will not be able to schedule onto the node, because there is notoleration matching the third taint. But it will be able to continue running if it isalready running on the node when the taint is added, because the third taint is the onlyone of the three that is not tolerated by the pod. Normally, if a taint with effect NoExecute is added to a node, then any pods that donot tolerate the taint will be evicted immediately, and pods that do tolerate thetaint will never be evicted. However, a toleration with NoExecute effect can specifyan optional tolerationSeconds field that dictates how long the pod will stay boundto the node after the taint is added. For example, tolerations:- key: ""key1""  operator: ""Equal""  value: ""value1""  effect: ""NoExecute""  tolerationSeconds: 3600 means that if this pod is running and a matching taint is added to the node, thenthe pod will stay bound to the node for 3600 seconds, and then be evicted. If thetaint is removed before that time, the pod will not be evicted.",1115
11.6 - Taints and Tolerations,Example Use Cases,"Example Use Cases Taints and tolerations are a flexible way to steer pods away from nodes or evictpods that shouldn't be running. A few of the use cases are Dedicated Nodes: If you want to dedicate a set of nodes for exclusive use bya particular set of users, you can add a taint to those nodes (say,kubectl taint nodes nodename dedicated=groupName:NoSchedule) and then add a correspondingtoleration to their pods (this would be done most easily by writing a customadmission controller).The pods with the tolerations will then be allowed to use the tainted (dedicated) nodes aswell as any other nodes in the cluster. If you want to dedicate the nodes to them andensure they only use the dedicated nodes, then you should additionally add a label similarto the taint to the same set of nodes (e.g. dedicated=groupName), and the admissioncontroller should additionally add a node affinity to require that the pods can only scheduleonto nodes labeled with dedicated=groupName.Nodes with Special Hardware: In a cluster where a small subset of nodes have specializedhardware (for example GPUs), it is desirable to keep pods that don't need the specializedhardware off of those nodes, thus leaving room for later-arriving pods that do need thespecialized hardware. This can be done by tainting the nodes that have the specializedhardware (e.g. kubectl taint nodes nodename special=true:NoSchedule orkubectl taint nodes nodename special=true:PreferNoSchedule) and adding a correspondingtoleration to pods that use the special hardware. As in the dedicated nodes use case,it is probably easiest to apply the tolerations using a customadmission controller.For example, it is recommended to use ExtendedResourcesto represent the special hardware, taint your special hardware nodes with theextended resource name and run theExtendedResourceTolerationadmission controller. Now, because the nodes are tainted, no pods without thetoleration will schedule on them. But when you submit a pod that requests theextended resource, the ExtendedResourceToleration admission controller willautomatically add the correct toleration to the pod and that pod will scheduleon the special hardware nodes. This will make sure that these special hardwarenodes are dedicated for pods requesting such hardware and you don't have tomanually add tolerations to your pods.Taint based Evictions: A per-pod-configurable eviction behaviorwhen there are node problems, which is described in the next section.",545
11.6 - Taints and Tolerations,Taint based Evictions,"Taint based Evictions FEATURE STATE: Kubernetes v1.18 [stable] The NoExecute taint effect, mentioned above, affects pods that are alreadyrunning on the node as follows pods that do not tolerate the taint are evicted immediatelypods that tolerate the taint without specifying tolerationSeconds intheir toleration specification remain bound foreverpods that tolerate the taint with a specified tolerationSeconds remainbound for the specified amount of time The node controller automatically taints a Node when certain conditionsare true. The following taints are built in: node.kubernetes.io/not-ready: Node is not ready. This corresponds tothe NodeCondition Ready being ""False"".node.kubernetes.io/unreachable: Node is unreachable from the nodecontroller. This corresponds to the NodeCondition Ready being ""Unknown"".node.kubernetes.io/memory-pressure: Node has memory pressure.node.kubernetes.io/disk-pressure: Node has disk pressure.node.kubernetes.io/pid-pressure: Node has PID pressure.node.kubernetes.io/network-unavailable: Node's network is unavailable.node.kubernetes.io/unschedulable: Node is unschedulable.node.cloudprovider.kubernetes.io/uninitialized: When the kubelet is startedwith ""external"" cloud provider, this taint is set on a node to mark itas unusable. After a controller from the cloud-controller-manager initializesthis node, the kubelet removes this taint. In case a node is to be evicted, the node controller or the kubelet adds relevant taintswith NoExecute effect. If the fault condition returns to normal the kubelet or nodecontroller can remove the relevant taint(s). Note: The control plane limits the rate of adding node new taints to nodes. This rate limitingmanages the number of evictions that are triggered when many nodes become unreachable atonce (for example: if there is a network disruption). You can specify tolerationSeconds for a Pod to define how long that Pod stays boundto a failing or unresponsive Node. For example, you might want to keep an application with a lot of local statebound to node for a long time in the event of network partition, hopingthat the partition will recover and thus the pod eviction can be avoided.The toleration you set for that Pod might look like: tolerations:- key: ""node.kubernetes.io/unreachable""  operator: ""Exists""  effect: ""NoExecute""  tolerationSeconds: 6000 Note:Kubernetes automatically adds a toleration fornode.kubernetes.io/not-ready and node.kubernetes.io/unreachablewith tolerationSeconds=300,unless you, or a controller, set those tolerations explicitly.These automatically-added tolerations mean that Pods remain bound toNodes for 5 minutes after one of these problems is detected. DaemonSet pods are created withNoExecute tolerations for the following taints with no tolerationSeconds: node.kubernetes.io/unreachablenode.kubernetes.io/not-ready This ensures that DaemonSet pods are never evicted due to these problems.",725
11.6 - Taints and Tolerations,Taint Nodes by Condition,"Taint Nodes by Condition The control plane, using the node controller,automatically creates taints with a NoSchedule effect fornode conditions. The scheduler checks taints, not node conditions, when it makes schedulingdecisions. This ensures that node conditions don't directly affect scheduling.For example, if the DiskPressure node condition is active, the control planeadds the node.kubernetes.io/disk-pressure taint and does not schedule new podsonto the affected node. If the MemoryPressure node condition is active, thecontrol plane adds the node.kubernetes.io/memory-pressure taint. You can ignore node conditions for newly created pods by adding the correspondingPod tolerations. The control plane also adds the node.kubernetes.io/memory-pressuretoleration on pods that have a QoS classother than BestEffort. This is because Kubernetes treats pods in the Guaranteedor Burstable QoS classes (even pods with no memory request set) as if they areable to cope with memory pressure, while new BestEffort pods are not scheduledonto the affected node. The DaemonSet controller automatically adds the following NoScheduletolerations to all daemons, to prevent DaemonSets from breaking. node.kubernetes.io/memory-pressurenode.kubernetes.io/disk-pressurenode.kubernetes.io/pid-pressure (1.14 or later)node.kubernetes.io/unschedulable (1.10 or later)node.kubernetes.io/network-unavailable (host network only) Adding these tolerations ensures backward compatibility. You can also addarbitrary tolerations to DaemonSets. Read about Node-pressure Evictionand how you can configure itRead about Pod Priority",393
11.7 - Scheduling Framework,default,"FEATURE STATE: Kubernetes v1.19 [stable] The scheduling framework is a pluggable architecture for the Kubernetes scheduler.It adds a new set of ""plugin"" APIs to the existing scheduler. Plugins are compiled into the scheduler. The APIs allow most scheduling features to be implemented as plugins, while keeping thescheduling ""core"" lightweight and maintainable. Refer to the design proposal of thescheduling framework for more technical information on the design of theframework.",106
Framework workflow,default,"The Scheduling Framework defines a few extension points. Scheduler pluginsregister to be invoked at one or more extension points. Some of these pluginscan change the scheduling decisions and some are informational only. Each attempt to schedule one Pod is split into two phases, the schedulingcycle and the binding cycle.",59
Framework workflow,Scheduling Cycle & Binding Cycle,"Scheduling Cycle & Binding Cycle The scheduling cycle selects a node for the Pod, and the binding cycle appliesthat decision to the cluster. Together, a scheduling cycle and binding cycle arereferred to as a ""scheduling context"". Scheduling cycles are run serially, while binding cycles may run concurrently. A scheduling or binding cycle can be aborted if the Pod is determined tobe unschedulable or if there is an internal error. The Pod will be returned tothe queue and retried.",103
Framework workflow,Extension points,"Extension points The following picture shows the scheduling context of a Pod and the extensionpoints that the scheduling framework exposes. In this picture ""Filter"" isequivalent to ""Predicate"" and ""Scoring"" is equivalent to ""Priority function"". One plugin may register at multiple extension points to perform more complex orstateful tasks. scheduling framework extension points",71
Framework workflow,QueueSort,"QueueSort These plugins are used to sort Pods in the scheduling queue. A queue sort pluginessentially provides a Less(Pod1, Pod2) function. Only one queue sortplugin may be enabled at a time.",45
Framework workflow,PreFilter,"PreFilter These plugins are used to pre-process info about the Pod, or to check certainconditions that the cluster or the Pod must meet. If a PreFilter plugin returnsan error, the scheduling cycle is aborted.",45
Framework workflow,Filter,"Filter These plugins are used to filter out nodes that cannot run the Pod. For eachnode, the scheduler will call filter plugins in their configured order. If anyfilter plugin marks the node as infeasible, the remaining plugins will not becalled for that node. Nodes may be evaluated concurrently.",61
Framework workflow,PostFilter,"PostFilter These plugins are called after Filter phase, but only when no feasible nodeswere found for the pod. Plugins are called in their configured order. Ifany postFilter plugin marks the node as Schedulable, the remaining pluginswill not be called. A typical PostFilter implementation is preemption, whichtries to make the pod schedulable by preempting other Pods.",78
Framework workflow,PreScore,"PreScore These plugins are used to perform ""pre-scoring"" work, which generates a sharablestate for Score plugins to use. If a PreScore plugin returns an error, thescheduling cycle is aborted.",44
Framework workflow,Score,"Score These plugins are used to rank nodes that have passed the filtering phase. Thescheduler will call each scoring plugin for each node. There will be a welldefined range of integers representing the minimum and maximum scores. After theNormalizeScore phase, the scheduler will combine nodescores from all plugins according to the configured plugin weights.",69
Framework workflow,NormalizeScore,"NormalizeScore These plugins are used to modify scores before the scheduler computes a finalranking of Nodes. A plugin that registers for this extension point will becalled with the Score results from the same plugin. This is calledonce per plugin per scheduling cycle. For example, suppose a plugin BlinkingLightScorer ranks Nodes based on howmany blinking lights they have. func ScoreNode(_ *v1.pod, n *v1.Node) (int, error) {    return getBlinkingLightCount(n)} However, the maximum count of blinking lights may be small compared toNodeScoreMax. To fix this, BlinkingLightScorer should also register for thisextension point. func NormalizeScores(scores map[string]int) {    highest := 0    for _, score := range scores {        highest = max(highest, score)    }    for node, score := range scores {        scores[node] = score*NodeScoreMax/highest    }} If any NormalizeScore plugin returns an error, the scheduling cycle isaborted. Note: Plugins wishing to perform ""pre-reserve"" work should use theNormalizeScore extension point.",247
Framework workflow,Reserve,"Reserve A plugin that implements the Reserve extension has two methods, namely Reserveand Unreserve, that back two informational scheduling phases called Reserveand Unreserve, respectively. Plugins which maintain runtime state (aka ""statefulplugins"") should use these phases to be notified by the scheduler when resourceson a node are being reserved and unreserved for a given Pod. The Reserve phase happens before the scheduler actually binds a Pod to itsdesignated node. It exists to prevent race conditions while the scheduler waitsfor the bind to succeed. The Reserve method of each Reserve plugin may succeedor fail; if one Reserve method call fails, subsequent plugins are not executedand the Reserve phase is considered to have failed. If the Reserve method ofall plugins succeed, the Reserve phase is considered to be successful and therest of the scheduling cycle and the binding cycle are executed. The Unreserve phase is triggered if the Reserve phase or a later phase fails.When this happens, the Unreserve method of all Reserve plugins will beexecuted in the reverse order of Reserve method calls. This phase exists toclean up the state associated with the reserved Pod. Caution: The implementation of the Unreserve method in Reserve plugins must beidempotent and may not fail.",256
Framework workflow,Permit,"Permit Permit plugins are invoked at the end of the scheduling cycle for each Pod, toprevent or delay the binding to the candidate node. A permit plugin can do one ofthe three things: approveOnce all Permit plugins approve a Pod, it is sent for binding.denyIf any Permit plugin denies a Pod, it is returned to the scheduling queue.This will trigger the Unreserve phase in Reserve plugins.wait (with a timeout)If a Permit plugin returns ""wait"", then the Pod is kept in an internal ""waiting""Pods list, and the binding cycle of this Pod starts but directly blocks until itgets approved. If a timeout occurs, wait becomes denyand the Pod is returned to the scheduling queue, triggering theUnreserve phase in Reserve plugins. Note: While any plugin can access the list of ""waiting"" Pods and approve them(see FrameworkHandle), we expect only the permitplugins to approve binding of reserved Pods that are in ""waiting"" state. Once a Podis approved, it is sent to the PreBind phase.",224
Framework workflow,PreBind,"PreBind These plugins are used to perform any work required before a Pod is bound. Forexample, a pre-bind plugin may provision a network volume and mount it on thetarget node before allowing the Pod to run there. If any PreBind plugin returns an error, the Pod is rejected andreturned to the scheduling queue.",67
Framework workflow,Bind,"Bind These plugins are used to bind a Pod to a Node. Bind plugins will not be calleduntil all PreBind plugins have completed. Each bind plugin is called in theconfigured order. A bind plugin may choose whether or not to handle the givenPod. If a bind plugin chooses to handle a Pod, the remaining bind plugins areskipped.",70
Framework workflow,PostBind,"PostBind This is an informational extension point. Post-bind plugins are called after aPod is successfully bound. This is the end of a binding cycle, and can be usedto clean up associated resources.",41
Framework workflow,Plugin API,"Plugin API There are two steps to the plugin API. First, plugins must register and getconfigured, then they use the extension point interfaces. Extension pointinterfaces have the following form. type Plugin interface {    Name() string}type QueueSortPlugin interface {    Plugin    Less(*v1.pod, *v1.pod) bool}type PreFilterPlugin interface {    Plugin    PreFilter(context.Context, *framework.CycleState, *v1.pod) error}// ...",105
Framework workflow,Plugin configuration,"Plugin configuration You can enable or disable plugins in the scheduler configuration. If you are usingKubernetes v1.18 or later, most schedulingplugins are in use andenabled by default. In addition to default plugins, you can also implement your own schedulingplugins and get them configured along with default plugins. You can visitscheduler-plugins for more details. If you are using Kubernetes v1.18 or later, you can configure a set of plugins asa scheduler profile and then define multiple profiles to fit various kinds of workload.Learn more at multiple profiles.",119
11.8 - Dynamic Resource Allocation,default,FEATURE STATE: Kubernetes v1.26 [alpha] Dynamic resource allocation is a new API for requesting and sharing resourcesbetween pods and containers inside a pod. It is a generalization of thepersistent volumes API for generic resources. Third-party resource drivers areresponsible for tracking and allocating resources. Different kinds ofresources support arbitrary parameters for defining requirements andinitialization.,78
11.8 - Dynamic Resource Allocation,Before you begin,"Before you begin Kubernetes v1.26 includes cluster-level API support fordynamic resource allocation, but it needs to beenabled explicitly. You also mustinstall a resource driver for specific resources that are meant to be managedusing this API. If you are not running Kubernetes v1.26,check the documentation for that version of Kubernetes.",77
11.8 - Dynamic Resource Allocation,API,"API The new resource.k8s.io/v1alpha1 API group provides four new types: ResourceClassDefines which resource driver handles a certain kind ofresource and provides common parameters for it. ResourceClassesare created by a cluster administrator when installing a resourcedriver.ResourceClaimDefines a particular resource instances that is required by aworkload. Created by a user (lifecycle managed manually, can be sharedbetween different Pods) or for individual Pods by the control plane based ona ResourceClaimTemplate (automatic lifecycle, typically used by just onePod).ResourceClaimTemplateDefines the spec and some meta data for creatingResourceClaims. Created by a user when deploying a workload.PodSchedulingUsed internally by the control plane and resource driversto coordinate pod scheduling when ResourceClaims need to be allocatedfor a Pod. Parameters for ResourceClass and ResourceClaim are stored in separate objects,typically using the type defined by a CRD that was created wheninstalling a resource driver. The core/v1 PodSpec defines ResourceClaims that are needed for a Pod in a newresourceClaims field. Entries in that list reference either a ResourceClaimor a ResourceClaimTemplate. When referencing a ResourceClaim, all Pods usingthis PodSpec (for example, inside a Deployment or StatefulSet) share the sameResourceClaim instance. When referencing a ResourceClaimTemplate, each Pod getsits own instance. The resources.claims list for container resources defines whether a container getsaccess to these resource instances, which makes it possible to share resourcesbetween one or more containers. Here is an example for a fictional resource driver. Two ResourceClaim objectswill get created for this Pod and each container gets access to one of them. apiVersion: resource.k8s.io/v1alpha1kind: ResourceClassname: resource.example.comdriverName: resource-driver.example.com---apiVersion: cats.resource.example.com/v1kind: ClaimParametersname: large-black-cat-claim-parametersspec:  color: black  size: large---apiVersion: resource.k8s.io/v1alpha1kind: ResourceClaimTemplatemetadata:  name: large-black-cat-claim-templatespec:  spec:    resourceClassName: resource.example.com    parametersRef:      apiGroup: cats.resource.example.com      kind: ClaimParameters      name: large-black-cat-claim-parameters–--apiVersion: v1kind: Podmetadata:  name: pod-with-catsspec:  containers:  - name: container0    image: ubuntu:20.04    command: [""sleep"", ""9999""]    resources:      claims:      - name: cat-0  - name: container1    image: ubuntu:20.04    command: [""sleep"", ""9999""]    resources:      claims:      - name: cat-1  resourceClaims:  - name: cat-0    source:      resourceClaimTemplateName: large-black-cat-claim-template  - name: cat-1    source:      resourceClaimTemplateName: large-black-cat-claim-template",677
11.8 - Dynamic Resource Allocation,Scheduling,"Scheduling In contrast to native resources (CPU, RAM) and extended resources (managed by adevice plugin, advertised by kubelet), the scheduler has no knowledge of whatdynamic resources are available in a cluster or how they could be split up tosatisfy the requirements of a specific ResourceClaim. Resource drivers areresponsible for that. They mark ResourceClaims as ""allocated"" once resourcesfor it are reserved. This also then tells the scheduler where in the cluster aResourceClaim is available. ResourceClaims can get allocated as soon as they are created (""immediateallocation""), without considering which Pods will use them. The default is todelay allocation until a Pod gets scheduled which needs the ResourceClaim(i.e. ""wait for first consumer""). In that mode, the scheduler checks all ResourceClaims needed by a Pod andcreates a PodScheduling object where it informs the resource driversresponsible for those ResourceClaims about nodes that the scheduler considerssuitable for the Pod. The resource drivers respond by excluding nodes thatdon't have enough of the driver's resources left. Once the scheduler has thatinformation, it selects one node and stores that choice in the PodSchedulingobject. The resource drivers then allocate their ResourceClaims so that theresources will be available on that node. Once that is complete, the Podgets scheduled. As part of this process, ResourceClaims also get reserved for thePod. Currently ResourceClaims can either be used exclusively by a single Pod oran unlimited number of Pods. One key feature is that Pods do not get scheduled to a node unless all oftheir resources are allocated and reserved. This avoids the scenario where a Podgets scheduled onto one node and then cannot run there, which is bad becausesuch a pending Pod also blocks all other resources like RAM or CPU that wereset aside for it.",388
11.8 - Dynamic Resource Allocation,Limitations,Limitations The scheduler plugin must be involved in scheduling Pods which useResourceClaims. Bypassing the scheduler by setting the nodeName field leadsto Pods that the kubelet refuses to start because the ResourceClaims are notreserved or not even allocated. It may be possible to remove thislimitation in thefuture.,71
11.8 - Dynamic Resource Allocation,Enabling dynamic resource allocation,"Enabling dynamic resource allocation Dynamic resource allocation is an alpha feature and only enabled when theDynamicResourceAllocation featuregate and theresource.k8s.io/v1alpha1 API group are enabled. For details on that, see the--feature-gates and --runtime-config kube-apiserverparameters.kube-scheduler, kube-controller-manager and kubelet also need the feature gate. A quick check whether a Kubernetes cluster supports the feature is to listResourceClass objects with: kubectl get resourceclasses If your cluster supports dynamic resource allocation, the response is either alist of ResourceClass objects or: No resources found If not supported, this error is printed instead: error: the server doesn't have a resource type ""resourceclasses"" The default configuration of kube-scheduler enables the ""DynamicResources""plugin if and only if the feature gate is enabled. Custom configurations mayhave to be modified to include it. In addition to enabling the feature in the cluster, a resource driver also has tobe installed. Please refer to the driver's documentation for details. For more information on the design, see theDynamic Resource Allocation KEP.",249
11.9 - Scheduler Performance Tuning,default,"FEATURE STATE: Kubernetes v1.14 [beta] kube-scheduleris the Kubernetes default scheduler. It is responsible for placement of Podson Nodes in a cluster. Nodes in a cluster that meet the scheduling requirements of a Pod arecalled feasible Nodes for the Pod. The scheduler finds feasible Nodesfor a Pod and then runs a set of functions to score the feasible Nodes,picking a Node with the highest score among the feasible ones to runthe Pod. The scheduler then notifies the API server about this decisionin a process called Binding. This page explains performance tuning optimizations that are relevant forlarge Kubernetes clusters. In large clusters, you can tune the scheduler's behaviour balancingscheduling outcomes between latency (new Pods are placed quickly) andaccuracy (the scheduler rarely makes poor placement decisions). You configure this tuning setting via kube-scheduler settingpercentageOfNodesToScore. This KubeSchedulerConfiguration setting determinesa threshold for scheduling nodes in your cluster.",222
11.9 - Scheduler Performance Tuning,Setting the threshold,"Setting the threshold The percentageOfNodesToScore option accepts whole numeric values between 0and 100. The value 0 is a special number which indicates that the kube-schedulershould use its compiled-in default.If you set percentageOfNodesToScore above 100, kube-scheduler acts as if youhad set a value of 100. To change the value, edit thekube-scheduler configuration fileand then restart the scheduler.In many cases, the configuration file can be found at /etc/kubernetes/config/kube-scheduler.yaml. After you have made this change, you can run kubectl get pods -n kube-system | grep kube-scheduler to verify that the kube-scheduler component is healthy.",173
11.9 - Scheduler Performance Tuning,Node scoring threshold,"Node scoring threshold To improve scheduling performance, the kube-scheduler can stop looking forfeasible nodes once it has found enough of them. In large clusters, this savestime compared to a naive approach that would consider every node. You specify a threshold for how many nodes are enough, as a whole number percentageof all the nodes in your cluster. The kube-scheduler converts this into aninteger number of nodes. During scheduling, if the kube-scheduler has identifiedenough feasible nodes to exceed the configured percentage, the kube-schedulerstops searching for more feasible nodes and moves on to thescoring phase. How the scheduler iterates over Nodesdescribes the process in detail.",149
11.9 - Scheduler Performance Tuning,Default threshold,"Default threshold If you don't specify a threshold, Kubernetes calculates a figure using alinear formula that yields 50% for a 100-node cluster and yields 10%for a 5000-node cluster. The lower bound for the automatic value is 5%. This means that, the kube-scheduler always scores at least 5% of your cluster nomatter how large the cluster is, unless you have explicitly setpercentageOfNodesToScore to be smaller than 5. If you want the scheduler to score all nodes in your cluster, setpercentageOfNodesToScore to 100.",124
11.9 - Scheduler Performance Tuning,Example,Example Below is an example configuration that sets percentageOfNodesToScore to 50%. apiVersion: kubescheduler.config.k8s.io/v1alpha1kind: KubeSchedulerConfigurationalgorithmSource:  provider: DefaultProvider...percentageOfNodesToScore: 50,65
11.9 - Scheduler Performance Tuning,Tuning percentageOfNodesToScore,"Tuning percentageOfNodesToScore percentageOfNodesToScore must be a value between 1 and 100 with the defaultvalue being calculated based on the cluster size. There is also a hardcodedminimum value of 50 nodes. Note:In clusters with less than 50 feasible nodes, the scheduler stillchecks all the nodes because there are not enough feasible nodes to stopthe scheduler's search early.In a small cluster, if you set a low value for percentageOfNodesToScore, yourchange will have no or little effect, for a similar reason.If your cluster has several hundred Nodes or fewer, leave this configuration optionat its default value. Making changes is unlikely to improve thescheduler's performance significantly. An important detail to consider when setting this value is that when a smallernumber of nodes in a cluster are checked for feasibility, some nodes are notsent to be scored for a given Pod. As a result, a Node which could possiblyscore a higher value for running the given Pod might not even be passed to thescoring phase. This would result in a less than ideal placement of the Pod. You should avoid setting percentageOfNodesToScore very low so that kube-schedulerdoes not make frequent, poor Pod placement decisions. Avoid setting thepercentage to anything below 10%, unless the scheduler's throughput is criticalfor your application and the score of nodes is not important. In other words, youprefer to run the Pod on any Node as long as it is feasible.",309
11.9 - Scheduler Performance Tuning,How the scheduler iterates over Nodes,"How the scheduler iterates over Nodes This section is intended for those who want to understand the internal detailsof this feature. In order to give all the Nodes in a cluster a fair chance of being consideredfor running Pods, the scheduler iterates over the nodes in a round robinfashion. You can imagine that Nodes are in an array. The scheduler starts fromthe start of the array and checks feasibility of the nodes until it finds enoughNodes as specified by percentageOfNodesToScore. For the next Pod, thescheduler continues from the point in the Node array that it stopped at whenchecking feasibility of Nodes for the previous Pod. If Nodes are in multiple zones, the scheduler iterates over Nodes in variouszones to ensure that Nodes from different zones are considered in thefeasibility checks. As an example, consider six nodes in two zones: Zone 1: Node 1, Node 2, Node 3, Node 4Zone 2: Node 5, Node 6 The Scheduler evaluates feasibility of the nodes in this order: Node 1, Node 5, Node 2, Node 6, Node 3, Node 4 After going over all the Nodes, it goes back to Node 1. Check the kube-scheduler configuration reference (v1beta3)",269
11.10 - Resource Bin Packing,default,"In the scheduling-plugin NodeResourcesFit of kube-scheduler, there are twoscoring strategies that support the bin packing of resources: MostAllocated and RequestedToCapacityRatio.",42
11.10 - Resource Bin Packing,Enabling bin packing using MostAllocated strategy,"Enabling bin packing using MostAllocated strategy The MostAllocated strategy scores the nodes based on the utilization of resources, favoring the ones with higher allocation.For each resource type, you can set a weight to modify its influence in the node score. To set the MostAllocated strategy for the NodeResourcesFit plugin, use ascheduler configuration similar to the following: apiVersion: kubescheduler.config.k8s.io/v1beta3kind: KubeSchedulerConfigurationprofiles:- pluginConfig:  - args:      scoringStrategy:        resources:        - name: cpu          weight: 1        - name: memory          weight: 1        - name: intel.com/foo          weight: 3        - name: intel.com/bar          weight: 3        type: MostAllocated    name: NodeResourcesFit To learn more about other parameters and their default configuration, see the API documentation forNodeResourcesFitArgs.",197
11.10 - Resource Bin Packing,Enabling bin packing using RequestedToCapacityRatio,"Enabling bin packing using RequestedToCapacityRatio The RequestedToCapacityRatio strategy allows the users to specify the resources along with weights foreach resource to score nodes based on the request to capacity ratio. Thisallows users to bin pack extended resources by using appropriate parametersto improve the utilization of scarce resources in large clusters. It favors nodes according to aconfigured function of the allocated resources. The behavior of the RequestedToCapacityRatio inthe NodeResourcesFit score function can be controlled by thescoringStrategy field.Within the scoringStrategy field, you can configure two parameters: requestedToCapacityRatio andresources. The shape in the requestedToCapacityRatioparameter allows the user to tune the function as least requested or mostrequested based on utilization and score values. The resources parameterconsists of name of the resource to be considered during scoring and weightspecify the weight of each resource. Below is an example configuration that setsthe bin packing behavior for extended resources intel.com/foo and intel.com/barusing the requestedToCapacityRatio field. apiVersion: kubescheduler.config.k8s.io/v1beta3kind: KubeSchedulerConfigurationprofiles:- pluginConfig:  - args:      scoringStrategy:        resources:        - name: intel.com/foo          weight: 3        - name: intel.com/bar          weight: 3        requestedToCapacityRatio:          shape:          - utilization: 0            score: 0          - utilization: 100            score: 10        type: RequestedToCapacityRatio    name: NodeResourcesFit Referencing the KubeSchedulerConfiguration file with the kube-schedulerflag --config=/path/to/config/file will pass the configuration to thescheduler. To learn more about other parameters and their default configuration, see the API documentation forNodeResourcesFitArgs.",406
11.10 - Resource Bin Packing,Tuning the score function,"Tuning the score function shape is used to specify the behavior of the RequestedToCapacityRatio function. shape:  - utilization: 0    score: 0  - utilization: 100    score: 10 The above arguments give the node a score of 0 if utilization is 0% and 10 forutilization 100%, thus enabling bin packing behavior. To enable leastrequested the score value must be reversed as follows. shape:  - utilization: 0    score: 10  - utilization: 100    score: 0 resources is an optional parameter which defaults to: resources:  - name: cpu    weight: 1  - name: memory    weight: 1 It can be used to add extended resources as follows: resources:  - name: intel.com/foo    weight: 5  - name: cpu    weight: 3  - name: memory    weight: 1 The weight parameter is optional and is set to 1 if not specified. Also, theweight cannot be set to a negative value.",204
11.10 - Resource Bin Packing,Node scoring for capacity allocation,"Node scoring for capacity allocation This section is intended for those who want to understand the internal detailsof this feature.Below is an example of how the node score is calculated for a given set of values. Requested resources: intel.com/foo : 2memory: 256MBcpu: 2 Resource weights: intel.com/foo : 5memory: 1cpu: 3 FunctionShapePoint {{0, 0}, {100, 10}} Node 1 spec: Available:  intel.com/foo: 4  memory: 1 GB  cpu: 8Used:  intel.com/foo: 1  memory: 256MB  cpu: 1 Node score: intel.com/foo  = resourceScoringFunction((2+1),4)               = (100 - ((4-3)*100/4)               = (100 - 25)               = 75                       # requested + used = 75% * available               = rawScoringFunction(75)               = 7                        # floor(75/10)memory         = resourceScoringFunction((256+256),1024)               = (100 -((1024-512)*100/1024))               = 50                       # requested + used = 50% * available               = rawScoringFunction(50)               = 5                        # floor(50/10)cpu            = resourceScoringFunction((2+1),8)               = (100 -((8-3)*100/8))               = 37.5                     # requested + used = 37.5% * available               = rawScoringFunction(37.5)               = 3                        # floor(37.5/10)NodeScore   =  (7 * 5) + (5 * 1) + (3 * 3) / (5 + 1 + 3)            =  5 Node 2 spec: Available:  intel.com/foo: 8  memory: 1GB  cpu: 8Used:  intel.com/foo: 2  memory: 512MB  cpu: 6 Node score: intel.com/foo  = resourceScoringFunction((2+2),8)               =  (100 - ((8-4)*100/8)               =  (100 - 50)               =  50               =  rawScoringFunction(50)               = 5memory         = resourceScoringFunction((256+512),1024)               = (100 -((1024-768)*100/1024))               = 75               = rawScoringFunction(75)               = 7cpu            = resourceScoringFunction((2+6),8)               = (100 -((8-8)*100/8))               = 100               = rawScoringFunction(100)               = 10NodeScore   =  (5 * 5) + (7 * 1) + (10 * 3) / (5 + 1 + 3)            =  7 Read more about the scheduling frameworkRead more about scheduler configuration",604
11.11 - Pod Priority and Preemption,default,"FEATURE STATE: Kubernetes v1.14 [stable] Pods can have priority. Priority indicates theimportance of a Pod relative to other Pods. If a Pod cannot be scheduled, thescheduler tries to preempt (evict) lower priority Pods to make scheduling of thepending Pod possible. Warning:In a cluster where not all users are trusted, a malicious user could create Podsat the highest possible priorities, causing other Pods to be evicted/not getscheduled.An administrator can use ResourceQuota to prevent users from creating pods athigh priorities.See limit Priority Class consumption by defaultfor details.",134
11.11 - Pod Priority and Preemption,How to use priority and preemption,How to use priority and preemption To use priority and preemption: Add one or more PriorityClasses.Create Pods withpriorityClassName set to one of the addedPriorityClasses. Of course you do not need to create the Pods directly;normally you would add priorityClassName to the Pod template of acollection object like a Deployment. Keep reading for more information about these steps. Note: Kubernetes already ships with two PriorityClasses:system-cluster-critical and system-node-critical.These are common classes and are used to ensure that critical components are always scheduled first.,128
11.11 - Pod Priority and Preemption,PriorityClass,"PriorityClass A PriorityClass is a non-namespaced object that defines a mapping from apriority class name to the integer value of the priority. The name is specifiedin the name field of the PriorityClass object's metadata. The value isspecified in the required value field. The higher the value, the higher thepriority.The name of a PriorityClass object must be a validDNS subdomain name,and it cannot be prefixed with system-. A PriorityClass object can have any 32-bit integer value smaller than or equalto 1 billion. Larger numbers are reserved for critical system Pods that shouldnot normally be preempted or evicted. A cluster admin should create onePriorityClass object for each such mapping that they want. PriorityClass also has two optional fields: globalDefault and description.The globalDefault field indicates that the value of this PriorityClass shouldbe used for Pods without a priorityClassName. Only one PriorityClass withglobalDefault set to true can exist in the system. If there is noPriorityClass with globalDefault set, the priority of Pods with nopriorityClassName is zero. The description field is an arbitrary string. It is meant to tell users of thecluster when they should use this PriorityClass.",259
11.11 - Pod Priority and Preemption,Notes about PodPriority and existing clusters,"Notes about PodPriority and existing clusters If you upgrade an existing cluster without this feature, the priorityof your existing Pods is effectively zero.Addition of a PriorityClass with globalDefault set to true does notchange the priorities of existing Pods. The value of such a PriorityClass isused only for Pods created after the PriorityClass is added.If you delete a PriorityClass, existing Pods that use the name of thedeleted PriorityClass remain unchanged, but you cannot create more Pods thatuse the name of the deleted PriorityClass.",112
11.11 - Pod Priority and Preemption,Example PriorityClass,"Example PriorityClass apiVersion: scheduling.k8s.io/v1kind: PriorityClassmetadata:  name: high-priorityvalue: 1000000globalDefault: falsedescription: ""This priority class should be used for XYZ service pods only.""",53
11.11 - Pod Priority and Preemption,Non-preempting PriorityClass,"Non-preempting PriorityClass FEATURE STATE: Kubernetes v1.24 [stable] Pods with preemptionPolicy: Never will be placed in the scheduling queueahead of lower-priority pods,but they cannot preempt other pods.A non-preempting pod waiting to be scheduled will stay in the scheduling queue,until sufficient resources are free,and it can be scheduled.Non-preempting pods,like other pods,are subject to scheduler back-off.This means that if the scheduler tries these pods and they cannot be scheduled,they will be retried with lower frequency,allowing other pods with lower priority to be scheduled before them. Non-preempting pods may still be preempted by other,high-priority pods. preemptionPolicy defaults to PreemptLowerPriority,which will allow pods of that PriorityClass to preempt lower-priority pods(as is existing default behavior).If preemptionPolicy is set to Never,pods in that PriorityClass will be non-preempting. An example use case is for data science workloads.A user may submit a job that they want to be prioritized above other workloads,but do not wish to discard existing work by preempting running pods.The high priority job with preemptionPolicy: Never will be scheduledahead of other queued pods,as soon as sufficient cluster resources ""naturally"" become free.",289
11.11 - Pod Priority and Preemption,Example Non-preempting PriorityClass,"Example Non-preempting PriorityClass apiVersion: scheduling.k8s.io/v1kind: PriorityClassmetadata:  name: high-priority-nonpreemptingvalue: 1000000preemptionPolicy: NeverglobalDefault: falsedescription: ""This priority class will not cause other pods to be preempted.""",68
11.11 - Pod Priority and Preemption,Pod priority,"Pod priority After you have one or more PriorityClasses, you can create Pods that specify oneof those PriorityClass names in their specifications. The priority admissioncontroller uses the priorityClassName field and populates the integer value ofthe priority. If the priority class is not found, the Pod is rejected. The following YAML is an example of a Pod configuration that uses thePriorityClass created in the preceding example. The priority admissioncontroller checks the specification and resolves the priority of the Pod to1000000. apiVersion: v1kind: Podmetadata:  name: nginx  labels:    env: testspec:  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent  priorityClassName: high-priority",158
11.11 - Pod Priority and Preemption,Effect of Pod priority on scheduling order,"Effect of Pod priority on scheduling order When Pod priority is enabled, the scheduler orders pending Pods bytheir priority and a pending Pod is placed ahead of other pending Podswith lower priority in the scheduling queue. As a result, the higherpriority Pod may be scheduled sooner than Pods with lower priority ifits scheduling requirements are met. If such Pod cannot be scheduled,scheduler will continue and tries to schedule other lower priority Pods.",90
11.11 - Pod Priority and Preemption,Preemption,"Preemption When Pods are created, they go to a queue and wait to be scheduled. Thescheduler picks a Pod from the queue and tries to schedule it on a Node. If noNode is found that satisfies all the specified requirements of the Pod,preemption logic is triggered for the pending Pod. Let's call the pending Pod P.Preemption logic tries to find a Node where removal of one or more Pods withlower priority than P would enable P to be scheduled on that Node. If such aNode is found, one or more lower priority Pods get evicted from the Node. Afterthe Pods are gone, P can be scheduled on the Node.",138
11.11 - Pod Priority and Preemption,User exposed information,"User exposed information When Pod P preempts one or more Pods on Node N, nominatedNodeName field of PodP's status is set to the name of Node N. This field helps scheduler trackresources reserved for Pod P and also gives users information about preemptionsin their clusters. Please note that Pod P is not necessarily scheduled to the ""nominated Node"".The scheduler always tries the ""nominated Node"" before iterating over any other nodes.After victim Pods are preempted, they get their graceful termination period. Ifanother node becomes available while scheduler is waiting for the victim Pods toterminate, scheduler may use the other node to schedule Pod P. As a resultnominatedNodeName and nodeName of Pod spec are not always the same. Also, ifscheduler preempts Pods on Node N, but then a higher priority Pod than Pod Parrives, scheduler may give Node N to the new higher priority Pod. In such acase, scheduler clears nominatedNodeName of Pod P. By doing this, schedulermakes Pod P eligible to preempt Pods on another Node.",231
11.11 - Pod Priority and Preemption,Graceful termination of preemption victims,"Graceful termination of preemption victims When Pods are preempted, the victims get theirgraceful termination period.They have that much time to finish their work and exit. If they don't, they arekilled. This graceful termination period creates a time gap between the pointthat the scheduler preempts Pods and the time when the pending Pod (P) can bescheduled on the Node (N). In the meantime, the scheduler keeps scheduling otherpending Pods. As victims exit or get terminated, the scheduler tries to schedulePods in the pending queue. Therefore, there is usually a time gap between thepoint that scheduler preempts victims and the time that Pod P is scheduled. Inorder to minimize this gap, one can set graceful termination period of lowerpriority Pods to zero or a small number.",171
11.11 - Pod Priority and Preemption,"PodDisruptionBudget is supported, but not guaranteed","PodDisruptionBudget is supported, but not guaranteed A PodDisruptionBudget (PDB)allows application owners to limit the number of Pods of a replicated applicationthat are down simultaneously from voluntary disruptions. Kubernetes supportsPDB when preempting Pods, but respecting PDB is best effort. The scheduler triesto find victims whose PDB are not violated by preemption, but if no such victimsare found, preemption will still happen, and lower priority Pods will be removeddespite their PDBs being violated.",112
11.11 - Pod Priority and Preemption,Inter-Pod affinity on lower-priority Pods,"Inter-Pod affinity on lower-priority Pods A Node is considered for preemption only when the answer to this question isyes: ""If all the Pods with lower priority than the pending Pod are removed fromthe Node, can the pending Pod be scheduled on the Node?"" Note: Preemption does not necessarily remove all lower-priorityPods. If the pending Pod can be scheduled by removing fewer than alllower-priority Pods, then only a portion of the lower-priority Pods are removed.Even so, the answer to the preceding question must be yes. If the answer is no,the Node is not considered for preemption. If a pending Pod has inter-pod affinityto one or more of the lower-priority Pods on the Node, the inter-Pod affinityrule cannot be satisfied in the absence of those lower-priority Pods. In this case,the scheduler does not preempt any Pods on the Node. Instead, it looks for anotherNode. The scheduler might find a suitable Node or it might not. There is noguarantee that the pending Pod can be scheduled. Our recommended solution for this problem is to create inter-Pod affinity onlytowards equal or higher priority Pods.",252
11.11 - Pod Priority and Preemption,Cross node preemption,"Cross node preemption Suppose a Node N is being considered for preemption so that a pending Pod P canbe scheduled on N. P might become feasible on N only if a Pod on another Node ispreempted. Here's an example: Pod P is being considered for Node N.Pod Q is running on another Node in the same Zone as Node N.Pod P has Zone-wide anti-affinity with Pod Q (topologyKey: topology.kubernetes.io/zone).There are no other cases of anti-affinity between Pod P and other Pods inthe Zone.In order to schedule Pod P on Node N, Pod Q can be preempted, but schedulerdoes not perform cross-node preemption. So, Pod P will be deemedunschedulable on Node N. If Pod Q were removed from its Node, the Pod anti-affinity violation would begone, and Pod P could possibly be scheduled on Node N. We may consider adding cross Node preemption in future versions if there isenough demand and if we find an algorithm with reasonable performance.",226
11.11 - Pod Priority and Preemption,Pods are preempted unnecessarily,"Pods are preempted unnecessarily Preemption removes existing Pods from a cluster under resource pressure to makeroom for higher priority pending Pods. If you give high priorities tocertain Pods by mistake, these unintentionally high priority Pods may causepreemption in your cluster. Pod priority is specified by setting thepriorityClassName field in the Pod's specification. The integer value forpriority is then resolved and populated to the priority field of podSpec. To address the problem, you can change the priorityClassName for those Podsto use lower priority classes, or leave that field empty. An emptypriorityClassName is resolved to zero by default. When a Pod is preempted, there will be events recorded for the preempted Pod.Preemption should happen only when a cluster does not have enough resources fora Pod. In such cases, preemption happens only when the priority of the pendingPod (preemptor) is higher than the victim Pods. Preemption must not happen whenthere is no pending Pod, or when the pending Pods have equal or lower prioritythan the victims. If preemption happens in such scenarios, please file an issue.",233
11.11 - Pod Priority and Preemption,"Pods are preempted, but the preemptor is not scheduled","Pods are preempted, but the preemptor is not scheduled When pods are preempted, they receive their requested graceful terminationperiod, which is by default 30 seconds. If the victim Pods do not terminate withinthis period, they are forcibly terminated. Once all the victims go away, thepreemptor Pod can be scheduled. While the preemptor Pod is waiting for the victims to go away, a higher priorityPod may be created that fits on the same Node. In this case, the scheduler willschedule the higher priority Pod instead of the preemptor. This is expected behavior: the Pod with the higher priority should take the placeof a Pod with a lower priority.",138
11.11 - Pod Priority and Preemption,Higher priority Pods are preempted before lower priority pods,"Higher priority Pods are preempted before lower priority pods The scheduler tries to find nodes that can run a pending Pod. If no node isfound, the scheduler tries to remove Pods with lower priority from an arbitrarynode in order to make room for the pending pod.If a node with low priority Pods is not feasible to run the pending Pod, the schedulermay choose another node with higher priority Pods (compared to the Pods on theother node) for preemption. The victims must still have lower priority than thepreemptor Pod. When there are multiple nodes available for preemption, the scheduler tries tochoose the node with a set of Pods with lowest priority. However, if such Podshave PodDisruptionBudget that would be violated if they are preempted then thescheduler may choose another node with higher priority Pods. When multiple nodes exist for preemption and none of the above scenarios apply,the scheduler chooses a node with the lowest priority.",207
11.11 - Pod Priority and Preemption,Interactions between Pod priority and quality of service,"Interactions between Pod priority and quality of service Pod priority and QoS classare two orthogonal features with few interactions and no default restrictions onsetting the priority of a Pod based on its QoS classes. The scheduler'spreemption logic does not consider QoS when choosing preemption targets.Preemption considers Pod priority and attempts to choose a set of targets withthe lowest priority. Higher-priority Pods are considered for preemption only ifthe removal of the lowest priority Pods is not sufficient to allow the schedulerto schedule the preemptor Pod, or if the lowest priority Pods are protected byPodDisruptionBudget. The kubelet uses Priority to determine pod order for node-pressure eviction.You can use the QoS class to estimate the order in which pods are most likelyto get evicted. The kubelet ranks pods for eviction based on the following factors: Whether the starved resource usage exceeds requestsPod PriorityAmount of resource usage relative to requests See Pod selection for kubelet evictionfor more details. kubelet node-pressure eviction does not evict Pods when theirusage does not exceed their requests. If a Pod with lower priority is notexceeding its requests, it won't be evicted. Another Pod with higher prioritythat exceeds its requests may be evicted. Read about using ResourceQuotas in connection with PriorityClasses: limit Priority Class consumption by defaultLearn about Pod DisruptionLearn about API-initiated EvictionLearn about Node-pressure Eviction",306
11.12 - Node-pressure Eviction,default,"Node-pressure eviction is the process by which the kubelet proactively terminatespods to reclaim resources on nodes. The kubelet monitors resourceslike memory, disk space, and filesystem inodes on your cluster's nodes.When one or more of these resources reach specific consumption levels, thekubelet can proactively fail one or more pods on the node to reclaim resourcesand prevent starvation. During a node-pressure eviction, the kubelet sets the PodPhase for theselected pods to Failed. This terminates the pods. Node-pressure eviction is not the same asAPI-initiated eviction. The kubelet does not respect your configured PodDisruptionBudget or the pod'sterminationGracePeriodSeconds. If you use soft eviction thresholds,the kubelet respects your configured eviction-max-pod-grace-period. If you usehard eviction thresholds, it uses a 0s grace period for termination. If the pods are managed by a workloadresource (such as StatefulSetor Deployment) thatreplaces failed pods, the control plane or kube-controller-manager creates newpods in place of the evicted pods. Note: The kubelet attempts to reclaim node-level resourcesbefore it terminates end-user pods. For example, it removes unused containerimages when disk resources are starved. The kubelet uses various parameters to make eviction decisions, like the following: Eviction signalsEviction thresholdsMonitoring intervals",301
11.12 - Node-pressure Eviction,Eviction signals,"Eviction signals Eviction signals are the current state of a particular resource at a specificpoint in time. Kubelet uses eviction signals to make eviction decisions bycomparing the signals to eviction thresholds, which are the minimum amount ofthe resource that should be available on the node. Kubelet uses the following eviction signals: Eviction SignalDescriptionmemory.availablememory.available := node.status.capacity[memory] - node.stats.memory.workingSetnodefs.availablenodefs.available := node.stats.fs.availablenodefs.inodesFreenodefs.inodesFree := node.stats.fs.inodesFreeimagefs.availableimagefs.available := node.stats.runtime.imagefs.availableimagefs.inodesFreeimagefs.inodesFree := node.stats.runtime.imagefs.inodesFreepid.availablepid.available := node.stats.rlimit.maxpid - node.stats.rlimit.curproc In this table, the Description column shows how kubelet gets the value of thesignal. Each signal supports either a percentage or a literal value. Kubeletcalculates the percentage value relative to the total capacity associated withthe signal. The value for memory.available is derived from the cgroupfs instead of toolslike free -m. This is important because free -m does not work in acontainer, and if users use the node allocatablefeature, out of resource decisionsare made local to the end user Pod part of the cgroup hierarchy as well as theroot node. This scriptreproduces the same set of steps that the kubelet performs to calculatememory.available. The kubelet excludes inactive_file (i.e. # of bytes offile-backed memory on inactive LRU list) from its calculation as it assumes thatmemory is reclaimable under pressure. The kubelet supports the following filesystem partitions: nodefs: The node's main filesystem, used for local disk volumes, emptyDir,log storage, and more. For example, nodefs contains /var/lib/kubelet/.imagefs: An optional filesystem that container runtimes use to store containerimages and container writable layers. Kubelet auto-discovers these filesystems and ignores other filesystems. Kubeletdoes not support other configurations. Some kubelet garbage collection features are deprecated in favor of eviction: Existing FlagNew FlagRationale--image-gc-high-threshold--eviction-hard or --eviction-softexisting eviction signals can trigger image garbage collection--image-gc-low-threshold--eviction-minimum-reclaimeviction reclaims achieve the same behavior--maximum-dead-containers-deprecated once old logs are stored outside of container's context--maximum-dead-containers-per-container-deprecated once old logs are stored outside of container's context--minimum-container-ttl-duration-deprecated once old logs are stored outside of container's context",643
11.12 - Node-pressure Eviction,Eviction thresholds,"Eviction thresholds You can specify custom eviction thresholds for the kubelet to use when it makeseviction decisions. Eviction thresholds have the form [eviction-signal][operator][quantity], where: eviction-signal is the eviction signal to use.operator is the relational operatoryou want, such as < (less than).quantity is the eviction threshold amount, such as 1Gi. The value of quantitymust match the quantity representation used by Kubernetes. You can use eitherliteral values or percentages (%). For example, if a node has 10Gi of total memory and you want trigger eviction ifthe available memory falls below 1Gi, you can define the eviction threshold aseither memory.available<10% or memory.available<1Gi. You cannot use both. You can configure soft and hard eviction thresholds.",173
11.12 - Node-pressure Eviction,Soft eviction thresholds,"Soft eviction thresholds A soft eviction threshold pairs an eviction threshold with a requiredadministrator-specified grace period. The kubelet does not evict pods until thegrace period is exceeded. The kubelet returns an error on startup if there is nospecified grace period. You can specify both a soft eviction threshold grace period and a maximumallowed pod termination grace period for kubelet to use during evictions. If youspecify a maximum allowed grace period and the soft eviction threshold is met,the kubelet uses the lesser of the two grace periods. If you do not specify amaximum allowed grace period, the kubelet kills evicted pods immediately withoutgraceful termination. You can use the following flags to configure soft eviction thresholds: eviction-soft: A set of eviction thresholds like memory.available<1.5Githat can trigger pod eviction if held over the specified grace period.eviction-soft-grace-period: A set of eviction grace periods like memory.available=1m30sthat define how long a soft eviction threshold must hold before triggering a Pod eviction.eviction-max-pod-grace-period: The maximum allowed grace period (in seconds)to use when terminating pods in response to a soft eviction threshold being met.",261
11.12 - Node-pressure Eviction,Hard eviction thresholds,"Hard eviction thresholds A hard eviction threshold has no grace period. When a hard eviction threshold ismet, the kubelet kills pods immediately without graceful termination to reclaimthe starved resource. You can use the eviction-hard flag to configure a set of hard evictionthresholds like memory.available<1Gi. The kubelet has the following default hard eviction thresholds: memory.available<100Minodefs.available<10%imagefs.available<15%nodefs.inodesFree<5% (Linux nodes) These default values of hard eviction thresholds will only be set if noneof the parameters is changed. If you changed the value of any parameter,then the values of other parameters will not be inherited as the defaultvalues and will be set to zero. In order to provide custom values, youshould provide all the thresholds respectively.",174
11.12 - Node-pressure Eviction,Node conditions,"Node conditions The kubelet reports node conditions to reflect that the node is under pressurebecause hard or soft eviction threshold is met, independent of configured graceperiods. The kubelet maps eviction signals to node conditions as follows: Node ConditionEviction SignalDescriptionMemoryPressurememory.availableAvailable memory on the node has satisfied an eviction thresholdDiskPressurenodefs.available, nodefs.inodesFree, imagefs.available, or imagefs.inodesFreeAvailable disk space and inodes on either the node's root filesystem or image filesystem has satisfied an eviction thresholdPIDPressurepid.availableAvailable processes identifiers on the (Linux) node has fallen below an eviction threshold The kubelet updates the node conditions based on the configured--node-status-update-frequency, which defaults to 10s.",167
11.12 - Node-pressure Eviction,Node condition oscillation,"Node condition oscillation In some cases, nodes oscillate above and below soft eviction thresholds withoutholding for the defined grace periods. This causes the reported node conditionto constantly switch between true and false, leading to bad eviction decisions. To protect against oscillation, you can use the eviction-pressure-transition-periodflag, which controls how long the kubelet must wait before transitioning a nodecondition to a different state. The transition period has a default value of 5m.",99
11.12 - Node-pressure Eviction,Reclaiming node level resources,"Reclaiming node level resources The kubelet tries to reclaim node-level resources before it evicts end-user pods. When a DiskPressure node condition is reported, the kubelet reclaims node-levelresources based on the filesystems on the node.",55
11.12 - Node-pressure Eviction,With imagefs,"With imagefs If the node has a dedicated imagefs filesystem for container runtimes to use,the kubelet does the following: If the nodefs filesystem meets the eviction thresholds, the kubelet garbage collectsdead pods and containers.If the imagefs filesystem meets the eviction thresholds, the kubeletdeletes all unused images. With imagefs If nodefs is triggering evictions, the kubelet sorts pods based on nodefsusage (local volumes + logs of all containers). If imagefs is triggering evictions, the kubelet sorts pods based on thewritable layer usage of all containers.",128
11.12 - Node-pressure Eviction,Without imagefs,"Without imagefs If the node only has a nodefs filesystem that meets eviction thresholds,the kubelet frees up disk space in the following order: Garbage collect dead pods and containersDelete unused images Without imagefs If nodefs is triggering evictions, the kubelet sorts pods based on their totaldisk usage (local volumes + logs & writable layer of all containers)",80
11.12 - Node-pressure Eviction,Pod selection for kubelet eviction,"Pod selection for kubelet eviction If the kubelet's attempts to reclaim node-level resources don't bring the evictionsignal below the threshold, the kubelet begins to evict end-user pods. The kubelet uses the following parameters to determine the pod eviction order: Whether the pod's resource usage exceeds requestsPod PriorityThe pod's resource usage relative to requests As a result, kubelet ranks and evicts pods in the following order: BestEffort or Burstable pods where the usage exceeds requests. These podsare evicted based on their Priority and then by how much their usage levelexceeds the request.Guaranteed pods and Burstable pods where the usage is less than requestsare evicted last, based on their Priority. Note: The kubelet does not use the pod's QoS class to determine the eviction order.You can use the QoS class to estimate the most likely pod eviction order whenreclaiming resources like memory. QoS does not apply to EphemeralStorage requests,so the above scenario will not apply if the node is, for example, under DiskPressure. Guaranteed pods are guaranteed only when requests and limits are specified forall the containers and they are equal. These pods will never be evicted becauseof another pod's resource consumption. If a system daemon (such as kubeletand journald) is consuming more resources than were reserved viasystem-reserved or kube-reserved allocations, and the node only hasGuaranteed or Burstable pods using less resources than requests left on it,then the kubelet must choose to evict one of these pods to preserve node stabilityand to limit the impact of resource starvation on other pods. In this case, itwill choose to evict pods of lowest Priority first. When the kubelet evicts pods in response to inode or PID starvation, it usesthe Priority to determine the eviction order, because inodes and PIDs have norequests. The kubelet sorts pods differently based on whether the node has a dedicatedimagefs filesystem:",428
11.12 - Node-pressure Eviction,Minimum eviction reclaim,"Minimum eviction reclaim In some cases, pod eviction only reclaims a small amount of the starved resource.This can lead to the kubelet repeatedly hitting the configured eviction thresholdsand triggering multiple evictions. You can use the --eviction-minimum-reclaim flag or a kubelet config fileto configure a minimum reclaim amount for each resource. When the kubelet noticesthat a resource is starved, it continues to reclaim that resource until itreclaims the quantity you specify. For example, the following configuration sets minimum reclaim amounts: apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationevictionHard:  memory.available: ""500Mi""  nodefs.available: ""1Gi""  imagefs.available: ""100Gi""evictionMinimumReclaim:  memory.available: ""0Mi""  nodefs.available: ""500Mi""  imagefs.available: ""2Gi"" In this example, if the nodefs.available signal meets the eviction threshold,the kubelet reclaims the resource until the signal reaches the threshold of 1Gi,and then continues to reclaim the minimum amount of 500Mi it until the signalreaches 1.5Gi. Similarly, the kubelet reclaims the imagefs resource until the imagefs.availablesignal reaches 102Gi. The default eviction-minimum-reclaim is 0 for all resources.",303
11.12 - Node-pressure Eviction,Node out of memory behavior,"Node out of memory behavior If the node experiences an out of memory (OOM) event prior to the kubeletbeing able to reclaim memory, the node depends on the oom_killerto respond. The kubelet sets an oom_score_adj value for each container based on the QoS for the pod. Quality of Serviceoom_score_adjGuaranteed-997BestEffort1000Burstablemin(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999) Note: The kubelet also sets an oom_score_adj value of -997 for containers in Pods that havesystem-node-critical Priority. If the kubelet can't reclaim memory before a node experiences OOM, theoom_killer calculates an oom_score based on the percentage of memory it'susing on the node, and then adds the oom_score_adj to get an effective oom_scorefor each container. It then kills the container with the highest score. This means that containers in low QoS pods that consume a large amount of memoryrelative to their scheduling requests are killed first. Unlike pod eviction, if a container is OOM killed, the kubelet can restart itbased on its RestartPolicy.",267
11.12 - Node-pressure Eviction,Schedulable resources and eviction policies,"Schedulable resources and eviction policies When you configure the kubelet with an eviction policy, you should make sure thatthe scheduler will not schedule pods if they will trigger eviction because theyimmediately induce memory pressure. Consider the following scenario: Node memory capacity: 10GiOperator wants to reserve 10% of memory capacity for system daemons (kernel, kubelet, etc.)Operator wants to evict Pods at 95% memory utilization to reduce incidence of system OOM. For this to work, the kubelet is launched as follows: --eviction-hard=memory.available<500Mi--system-reserved=memory=1.5Gi In this configuration, the --system-reserved flag reserves 1.5Gi of memoryfor the system, which is 10% of the total memory + the eviction threshold amount. The node can reach the eviction threshold if a pod is using more than its request,or if the system is using more than 1Gi of memory, which makes the memory.availablesignal fall below 500Mi and triggers the threshold.",223
11.12 - Node-pressure Eviction,DaemonSet,"DaemonSet Pod Priority is a major factor in making eviction decisions. If you do not wantthe kubelet to evict pods that belong to a DaemonSet, give those pods a highenough priorityClass in the pod spec. You can also use a lower priorityClassor the default to only allow DaemonSet pods to run when there are enoughresources.",74
11.12 - Node-pressure Eviction,kubelet may not observe memory pressure right away,"kubelet may not observe memory pressure right away By default, the kubelet polls cAdvisor to collect memory usage stats at aregular interval. If memory usage increases within that window rapidly, thekubelet may not observe MemoryPressure fast enough, and the OOMKillerwill still be invoked. You can use the --kernel-memcg-notification flag to enable the memcgnotification API on the kubelet to get notified immediately when a thresholdis crossed. If you are not trying to achieve extreme utilization, but a sensible measure ofovercommit, a viable workaround for this issue is to use the --kube-reservedand --system-reserved flags to allocate memory for the system.",151
11.12 - Node-pressure Eviction,active_file memory is not considered as available memory,"active_file memory is not considered as available memory On Linux, the kernel tracks the number of bytes of file-backed memory on activeLRU list as the active_file statistic. The kubelet treats active_file memoryareas as not reclaimable. For workloads that make intensive use of block-backedlocal storage, including ephemeral local storage, kernel-level caches of fileand block data means that many recently accessed cache pages are likely to becounted as active_file. If enough of these kernel block buffers are on theactive LRU list, the kubelet is liable to observe this as high resource use andtaint the node as experiencing memory pressure - triggering pod eviction. For more details, see https://github.com/kubernetes/kubernetes/issues/43916 You can work around that behavior by setting the memory limit and memory requestthe same for containers likely to perform intensive I/O activity. You will needto estimate or measure an optimal memory limit value for that container. Learn about API-initiated EvictionLearn about Pod Priority and PreemptionLearn about PodDisruptionBudgetsLearn about Quality of Service (QoS)Check out the Eviction API",250
11.13 - API-initiated Eviction,default,"API-initiated eviction is the process by which you use the Eviction APIto create an Eviction object that triggers graceful pod termination. You can request eviction by calling the Eviction API directly, or programmaticallyusing a client of the API server, like the kubectl drain command. Thiscreates an Eviction object, which causes the API server to terminate the Pod. API-initiated evictions respect your configured PodDisruptionBudgetsand terminationGracePeriodSeconds. Using the API to create an Eviction object for a Pod is like performing apolicy-controlled DELETE operationon the Pod.",132
11.13 - API-initiated Eviction,Calling the Eviction API,"Calling the Eviction API You can use a Kubernetes language clientto access the Kubernetes API and create an Eviction object. To do this, youPOST the attempted operation, similar to the following example: policy/v1policy/v1beta1 Note: policy/v1 Eviction is available in v1.22+. Use policy/v1beta1 with prior releases.{  ""apiVersion"": ""policy/v1"",  ""kind"": ""Eviction"",  ""metadata"": {    ""name"": ""quux"",    ""namespace"": ""default""  }}Note: Deprecated in v1.22 in favor of policy/v1{  ""apiVersion"": ""policy/v1beta1"",  ""kind"": ""Eviction"",  ""metadata"": {    ""name"": ""quux"",    ""namespace"": ""default""  }} Alternatively, you can attempt an eviction operation by accessing the API usingcurl or wget, similar to the following example: curl -v -H 'Content-type: application/json' https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/quux/eviction -d @eviction.json",265
11.13 - API-initiated Eviction,How API-initiated eviction works,"How API-initiated eviction works When you request an eviction using the API, the API server performs admissionchecks and responds in one of the following ways: 200 OK: the eviction is allowed, the Eviction subresource is created, andthe Pod is deleted, similar to sending a DELETE request to the Pod URL.429 Too Many Requests: the eviction is not currently allowed because of theconfigured PodDisruptionBudget.You may be able to attempt the eviction again later. You might also see thisresponse because of API rate limiting.500 Internal Server Error: the eviction is not allowed because there is amisconfiguration, like if multiple PodDisruptionBudgets reference the same Pod. If the Pod you want to evict isn't part of a workload that has aPodDisruptionBudget, the API server always returns 200 OK and allows theeviction. If the API server allows the eviction, the Pod is deleted as follows: The Pod resource in the API server is updated with a deletion timestamp,after which the API server considers the Pod resource to be terminated. ThePod resource is also marked with the configured grace period.The kubelet on the node where the local Pod is running notices that the Podresource is marked for termination and starts to gracefully shut down thelocal Pod.While the kubelet is shutting the Pod down, the control plane removes the Podfrom Endpoint andEndpointSliceobjects. As a result, controllers no longer consider the Pod as a valid object.After the grace period for the Pod expires, the kubelet forcefully terminatesthe local Pod.The kubelet tells the API server to remove the Pod resource.The API server deletes the Pod resource.",353
11.13 - API-initiated Eviction,Troubleshooting stuck evictions,"Troubleshooting stuck evictions In some cases, your applications may enter a broken state, where the EvictionAPI will only return 429 or 500 responses until you intervene. This canhappen if, for example, a ReplicaSet creates pods for your application but newpods do not enter a Ready state. You may also notice this behavior in caseswhere the last evicted Pod had a long termination grace period. If you notice stuck evictions, try one of the following solutions: Abort or pause the automated operation causing the issue. Investigate the stuckapplication before you restart the operation.Wait a while, then directly delete the Pod from your cluster control planeinstead of using the Eviction API. Learn how to protect your applications with a Pod Disruption Budget.Learn about Node-pressure Eviction.Learn about Pod Priority and Preemption.",173
12 - Cluster Administration,default,Lower-level detail relevant to creating or administering a Kubernetes cluster. The cluster administration overview is for anyone creating or administering a Kubernetes cluster.It assumes some familiarity with core Kubernetes concepts.,45
12 - Cluster Administration,Planning a cluster,"Planning a cluster See the guides in Setup for examples of how to plan, set up, and configureKubernetes clusters. The solutions listed in this article are called distros. Note: Not all distros are actively maintained. Choose distros which have been tested with a recentversion of Kubernetes. Before choosing a guide, here are some considerations: Do you want to try out Kubernetes on your computer, or do you want to build a high-availability,multi-node cluster? Choose distros best suited for your needs.Will you be using a hosted Kubernetes cluster, such asGoogle Kubernetes Engine, or hosting your own cluster?Will your cluster be on-premises, or in the cloud (IaaS)? Kubernetes does not directlysupport hybrid clusters. Instead, you can set up multiple clusters.If you are configuring Kubernetes on-premises, consider whichnetworking model fits best.Will you be running Kubernetes on ""bare metal"" hardware or on virtual machines (VMs)?Do you want to run a cluster, or do you expect to do active development of Kubernetes project code?If the latter, choose an actively-developed distro. Some distros only use binary releases, butoffer a greater variety of choices.Familiarize yourself with the components needed to run a cluster.",288
12 - Cluster Administration,Securing a cluster,"Securing a cluster Generate Certificates describes the steps togenerate certificates using different tool chains.Kubernetes Container Environment describesthe environment for Kubelet managed containers on a Kubernetes node.Controlling Access to the Kubernetes API describeshow Kubernetes implements access control for its own API.Authenticating explains authentication inKubernetes, including the various authentication options.Authorization is separate fromauthentication, and controls how HTTP calls are handled.Using Admission Controllersexplains plug-ins which intercepts requests to the Kubernetes API server after authenticationand authorization.Using Sysctls in a Kubernetes Clusterdescribes to an administrator how to use the sysctl command-line tool to set kernel parameters.Auditing describes how to interact with Kubernetes'audit logs.",175
12 - Cluster Administration,Optional Cluster Services,Optional Cluster Services DNS Integration describes how to resolvea DNS name directly to a Kubernetes service.Logging and Monitoring Cluster Activityexplains how logging in Kubernetes works and how to implement it.,43
12.2 - Managing Resources,default,"You've deployed your application and exposed it via a service. Now what? Kubernetes provides anumber of tools to help you manage your application deployment, including scaling and updating.Among the features that we will discuss in more depth areconfiguration files andlabels.",55
12.2 - Managing Resources,Organizing resource configurations,"Organizing resource configurations Many applications require multiple resources to be created, such as a Deployment and a Service.Management of multiple resources can be simplified by grouping them together in the same file(separated by --- in YAML). For example: application/nginx-app.yamlapiVersion: v1kind: Servicemetadata:  name: my-nginx-svc  labels:    app: nginxspec:  type: LoadBalancer  ports:  - port: 80  selector:    app: nginx---apiVersion: apps/v1kind: Deploymentmetadata:  name: my-nginx  labels:    app: nginxspec:  replicas: 3  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:1.14.2        ports:        - containerPort: 80 Multiple resources can be created the same way as a single resource: kubectl apply -f https://k8s.io/examples/application/nginx-app.yaml service/my-nginx-svc createddeployment.apps/my-nginx created The resources will be created in the order they appear in the file. Therefore, it's best tospecify the service first, since that will ensure the scheduler can spread the pods associatedwith the service as they are created by the controller(s), such as Deployment. kubectl apply also accepts multiple -f arguments: kubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml \  -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml It is a recommended practice to put resources related to the same microservice or application tierinto the same file, and to group all of the files associated with your application in the samedirectory. If the tiers of your application bind to each other using DNS, you can deploy all ofthe components of your stack together. A URL can also be specified as a configuration source, which is handy for deploying directly fromconfiguration files checked into GitHub: kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml deployment.apps/my-nginx created",528
12.2 - Managing Resources,Bulk operations in kubectl,"Bulk operations in kubectl Resource creation isn't the only operation that kubectl can perform in bulk. It can also extractresource names from configuration files in order to perform other operations, in particular todelete the same resources you created: kubectl delete -f https://k8s.io/examples/application/nginx-app.yaml deployment.apps ""my-nginx"" deletedservice ""my-nginx-svc"" deleted In the case of two resources, you can specify both resources on the command line using theresource/name syntax: kubectl delete deployments/my-nginx services/my-nginx-svc For larger numbers of resources, you'll find it easier to specify the selector (label query)specified using -l or --selector, to filter resources by their labels: kubectl delete deployment,services -l app=nginx deployment.apps ""my-nginx"" deletedservice ""my-nginx-svc"" deleted Because kubectl outputs resource names in the same syntax it accepts, you can chain operationsusing $() or xargs: kubectl get $(kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service)kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service | xargs -i kubectl get {} NAME           TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)      AGEmy-nginx-svc   LoadBalancer   10.0.0.208   <pending>     80/TCP       0s With the above commands, we first create resources under examples/application/nginx/ and printthe resources created with -o name output format (print each resource as resource/name).Then we grep only the ""service"", and then print it with kubectl get. If you happen to organize your resources across several subdirectories within a particulardirectory, you can recursively perform the operations on the subdirectories also, by specifying--recursive or -R alongside the --filename,-f flag. For instance, assume there is a directory project/k8s/development that holds all of themanifests needed for the development environment,organized by resource type: project/k8s/development├── configmap│   └── my-configmap.yaml├── deployment│   └── my-deployment.yaml└── pvc    └── my-pvc.yaml By default, performing a bulk operation on project/k8s/development will stop at the first levelof the directory, not processing any subdirectories. If we had tried to create the resources inthis directory using the following command, we would have encountered an error: kubectl apply -f project/k8s/development error: you must provide one or more resources by argument or filename (.json|.yaml|.yml|stdin) Instead, specify the --recursive or -R flag with the --filename,-f flag as such: kubectl apply -f project/k8s/development --recursive configmap/my-config createddeployment.apps/my-deployment createdpersistentvolumeclaim/my-pvc created The --recursive flag works with any operation that accepts the --filename,-f flag such as:kubectl {create,get,delete,describe,rollout} etc. The --recursive flag also works when multiple -f arguments are provided: kubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive namespace/development creatednamespace/staging createdconfigmap/my-config createddeployment.apps/my-deployment createdpersistentvolumeclaim/my-pvc created If you're interested in learning more about kubectl, go ahead and readCommand line tool (kubectl).",875
12.2 - Managing Resources,Using labels effectively,"Using labels effectively The examples we've used so far apply at most a single label to any resource. There are manyscenarios where multiple labels should be used to distinguish sets from one another. For instance, different applications would use different values for the app label, but amulti-tier application, such as the guestbook example,would additionally need to distinguish each tier. The frontend could carry the following labels: labels:   app: guestbook   tier: frontend while the Redis master and slave would have different tier labels, and perhaps even anadditional role label: labels:   app: guestbook   tier: backend   role: master and labels:   app: guestbook   tier: backend   role: slave The labels allow us to slice and dice our resources along any dimension specified by a label: kubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yamlkubectl get pods -Lapp -Ltier -Lrole NAME                           READY     STATUS    RESTARTS   AGE       APP         TIER       ROLEguestbook-fe-4nlpb             1/1       Running   0          1m        guestbook   frontend   <none>guestbook-fe-ght6d             1/1       Running   0          1m        guestbook   frontend   <none>guestbook-fe-jpy62             1/1       Running   0          1m        guestbook   frontend   <none>guestbook-redis-master-5pg3b   1/1       Running   0          1m        guestbook   backend    masterguestbook-redis-slave-2q2yf    1/1       Running   0          1m        guestbook   backend    slaveguestbook-redis-slave-qgazl    1/1       Running   0          1m        guestbook   backend    slavemy-nginx-divi2                 1/1       Running   0          29m       nginx       <none>     <none>my-nginx-o0ef1                 1/1       Running   0          29m       nginx       <none>     <none> kubectl get pods -lapp=guestbook,role=slave NAME                          READY     STATUS    RESTARTS   AGEguestbook-redis-slave-2q2yf   1/1       Running   0          3mguestbook-redis-slave-qgazl   1/1       Running   0          3m",563
12.2 - Managing Resources,Canary deployments,"Canary deployments Another scenario where multiple labels are needed is to distinguish deployments of differentreleases or configurations of the same component. It is common practice to deploy a canary of anew application release (specified via image tag in the pod template) side by side with theprevious release so that the new release can receive live production traffic before fully rollingit out. For instance, you can use a track label to differentiate different releases. The primary, stable release would have a track label with value as stable: name: frontendreplicas: 3...labels:   app: guestbook   tier: frontend   track: stable...image: gb-frontend:v3 and then you can create a new release of the guestbook frontend that carries the track labelwith different value (i.e. canary), so that two sets of pods would not overlap: name: frontend-canaryreplicas: 1...labels:   app: guestbook   tier: frontend   track: canary...image: gb-frontend:v4 The frontend service would span both sets of replicas by selecting the common subset of theirlabels (i.e. omitting the track label), so that the traffic will be redirected to bothapplications: selector:   app: guestbook   tier: frontend You can tweak the number of replicas of the stable and canary releases to determine the ratio ofeach release that will receive live production traffic (in this case, 3:1).Once you're confident, you can update the stable track to the new application release and removethe canary one. For a more concrete example, check thetutorial of deploying Ghost.",349
12.2 - Managing Resources,Updating labels,"Updating labels Sometimes existing pods and other resources need to be relabeled before creating new resources.This can be done with kubectl label.For example, if you want to label all your nginx pods as frontend tier, run: kubectl label pods -l app=nginx tier=fe pod/my-nginx-2035384211-j5fhi labeledpod/my-nginx-2035384211-u2c7e labeledpod/my-nginx-2035384211-u3t6x labeled This first filters all pods with the label ""app=nginx"", and then labels them with the ""tier=fe"".To see the pods you labeled, run: kubectl get pods -l app=nginx -L tier NAME                        READY     STATUS    RESTARTS   AGE       TIERmy-nginx-2035384211-j5fhi   1/1       Running   0          23m       femy-nginx-2035384211-u2c7e   1/1       Running   0          23m       femy-nginx-2035384211-u3t6x   1/1       Running   0          23m       fe This outputs all ""app=nginx"" pods, with an additional label column of pods' tier (specified with-L or --label-columns). For more information, please see labelsand kubectl label.",320
12.2 - Managing Resources,Updating annotations,"Updating annotations Sometimes you would want to attach annotations to resources. Annotations are arbitrarynon-identifying metadata for retrieval by API clients such as tools, libraries, etc.This can be done with kubectl annotate. For example: kubectl annotate pods my-nginx-v4-9gw19 description='my frontend running nginx'kubectl get pods my-nginx-v4-9gw19 -o yaml apiVersion: v1kind: podmetadata:  annotations:    description: my frontend running nginx... For more information, see annotationsand kubectl annotate document.",137
12.2 - Managing Resources,Scaling your application,"Scaling your application When load on your application grows or shrinks, use kubectl to scale your application.For instance, to decrease the number of nginx replicas from 3 to 1, do: kubectl scale deployment/my-nginx --replicas=1 deployment.apps/my-nginx scaled Now you only have one pod managed by the deployment. kubectl get pods -l app=nginx NAME                        READY     STATUS    RESTARTS   AGEmy-nginx-2035384211-j5fhi   1/1       Running   0          30m To have the system automatically choose the number of nginx replicas as needed,ranging from 1 to 3, do: kubectl autoscale deployment/my-nginx --min=1 --max=3 horizontalpodautoscaler.autoscaling/my-nginx autoscaled Now your nginx replicas will be scaled up and down as needed, automatically. For more information, please see kubectl scale,kubectl autoscale andhorizontal pod autoscaler document.",239
12.2 - Managing Resources,kubectl apply,"kubectl apply It is suggested to maintain a set of configuration files in source control(see configuration as code),so that they can be maintained and versioned along with the code for the resources they configure.Then, you can use kubectl applyto push your configuration changes to the cluster. This command will compare the version of the configuration that you're pushing with the previousversion and apply the changes you've made, without overwriting any automated changes to propertiesyou haven't specified. kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml deployment.apps/my-nginx configured Note that kubectl apply attaches an annotation to the resource in order to determine the changesto the configuration since the previous invocation. When it's invoked, kubectl apply does athree-way diff between the previous configuration, the provided input and the currentconfiguration of the resource, in order to determine how to modify the resource. Currently, resources are created without this annotation, so the first invocation of kubectl apply will fall back to a two-way diff between the provided input and the current configurationof the resource. During this first invocation, it cannot detect the deletion of properties setwhen the resource was created. For this reason, it will not remove them. All subsequent calls to kubectl apply, and other commands that modify the configuration, such askubectl replace and kubectl edit, will update the annotation, allowing subsequent calls tokubectl apply to detect and perform deletions using a three-way diff.",340
12.2 - Managing Resources,kubectl edit,"kubectl edit Alternatively, you may also update resources with kubectl edit: kubectl edit deployment/my-nginx This is equivalent to first get the resource, edit it in text editor, and then apply theresource with the updated version: kubectl get deployment my-nginx -o yaml > /tmp/nginx.yamlvi /tmp/nginx.yaml# do some edit, and then save the filekubectl apply -f /tmp/nginx.yamldeployment.apps/my-nginx configuredrm /tmp/nginx.yaml This allows you to do more significant changes more easily. Note that you can specify the editorwith your EDITOR or KUBE_EDITOR environment variables. For more information, please see kubectl edit document.",178
12.2 - Managing Resources,kubectl patch,"kubectl patch You can use kubectl patch to update API objects in place. This command supports JSON patch,JSON merge patch, and strategic merge patch. SeeUpdate API Objects in Place Using kubectl patchandkubectl patch.",54
12.2 - Managing Resources,Disruptive updates,"Disruptive updates In some cases, you may need to update resource fields that cannot be updated once initialized, oryou may want to make a recursive change immediately, such as to fix broken pods created by aDeployment. To change such fields, use replace --force, which deletes and re-creates theresource. In this case, you can modify your original configuration file: kubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force deployment.apps/my-nginx deleteddeployment.apps/my-nginx replaced",132
12.2 - Managing Resources,Updating your application without a service outage,"Updating your application without a service outage At some point, you'll eventually need to update your deployed application, typically by specifyinga new image or image tag, as in the canary deployment scenario above. kubectl supports severalupdate operations, each of which is applicable to different scenarios. We'll guide you through how to create and update applications with Deployments. Let's say you were running version 1.14.2 of nginx: kubectl create deployment my-nginx --image=nginx:1.14.2 deployment.apps/my-nginx created with 3 replicas (so the old and new revisions can coexist): kubectl scale deployment my-nginx --current-replicas=1 --replicas=3 deployment.apps/my-nginx scaled To update to version 1.16.1, change .spec.template.spec.containers[0].image from nginx:1.14.2to nginx:1.16.1 using the previous kubectl commands. kubectl edit deployment/my-nginx That's it! The Deployment will declaratively update the deployed nginx application progressivelybehind the scene. It ensures that only a certain number of old replicas may be down while they arebeing updated, and only a certain number of new replicas may be created above the desired numberof pods. To learn more details about it, visit Deployment page. Learn about how to use kubectl for application introspection and debugging.See Configuration Best Practices and Tips.",324
12.3 - Cluster Networking,default,"Networking is a central part of Kubernetes, but it can be challenging tounderstand exactly how it is expected to work. There are 4 distinct networkingproblems to address: Highly-coupled container-to-container communications: this is solved byPods and localhost communications.Pod-to-Pod communications: this is the primary focus of this document.Pod-to-Service communications: this is covered by Services.External-to-Service communications: this is also covered by Services. Kubernetes is all about sharing machines between applications. Typically,sharing machines requires ensuring that two applications do not try to use thesame ports. Coordinating ports across multiple developers is very difficult todo at scale and exposes users to cluster-level issues outside of their control. Dynamic port allocation brings a lot of complications to the system - everyapplication has to take ports as flags, the API servers have to know how toinsert dynamic port numbers into configuration blocks, services have to knowhow to find each other, etc. Rather than deal with this, Kubernetes takes adifferent approach. To learn about the Kubernetes networking model, see here.",239
12.3 - Cluster Networking,How to implement the Kubernetes network model,"How to implement the Kubernetes network model The network model is implemented by the container runtime on each node. The most common container runtimes use Container Network Interface (CNI) plugins to manage their network and security capabilities. Many different CNI plugins exist from many different vendors. Some of these provide only basic features of adding and removing network interfaces, while others provide more sophisticated solutions, such as integration with other container orchestration systems, running multiple CNI plugins, advanced IPAM features etc. See this page for a non-exhaustive list of networking addons supported by Kubernetes. The early design of the networking model and its rationale, and some futureplans are described in more detail in thenetworking design document.",150
12.4 - Logging Architecture,default,"Application logs can help you understand what is happening inside your application. Thelogs are particularly useful for debugging problems and monitoring cluster activity. Mostmodern applications have some kind of logging mechanism. Likewise, container enginesare designed to support logging. The easiest and most adopted logging method forcontainerized applications is writing to standard output and standard error streams. However, the native functionality provided by a container engine or runtime is usuallynot enough for a complete logging solution. For example, you may want to access your application's logs if a container crashes,a pod gets evicted, or a node dies. In a cluster, logs should have a separate storage and lifecycle independent of nodes,pods, or containers. This concept is calledcluster-level logging. Cluster-level logging architectures require a separate backend to store, analyze, andquery logs. Kubernetes does not provide a native storage solution for log data. Instead,there are many logging solutions that integrate with Kubernetes. The following sectionsdescribe how to handle and store logs on nodes.",213
12.4 - Logging Architecture,Pod and container logs,"Pod and container logs Kubernetes captures logs from each container in a running Pod. This example uses a manifest for a Pod with a containerthat writes text to the standard output stream, once per second. debug/counter-pod.yamlapiVersion: v1kind: Podmetadata:  name: counterspec:  containers:  - name: count    image: busybox:1.28    args: [/bin/sh, -c,            'i=0; while true; do echo ""$i: $(date)""; i=$((i+1)); sleep 1; done'] To run this pod, use the following command: kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml The output is: pod/counter created To fetch the logs, use the kubectl logs command, as follows: kubectl logs counter The output is similar to: 0: Fri Apr  1 11:42:23 UTC 20221: Fri Apr  1 11:42:24 UTC 20222: Fri Apr  1 11:42:25 UTC 2022 You can use kubectl logs --previous to retrieve logs from a previous instantiation of a container.If your pod has multiple containers, specify which container's logs you want to access byappending a container name to the command, with a -c flag, like so: kubectl logs counter -c count See the kubectl logs documentation for more details.",315
12.4 - Logging Architecture,How nodes handle container logs,"How nodes handle container logs  A container runtime handles and redirects any output generated to a containerized application's stdout and stderr streams.Different container runtimes implement this in different ways; however, the integration with the kubelet is standardizedas the CRI logging format. By default, if a container restarts, the kubelet keeps one terminated container with its logs. If a pod is evicted from the node,all corresponding containers are also evicted, along with their logs. The kubelet makes logs available to clients via a special feature of the Kubernetes API. The usual way to access this isby running kubectl logs.",138
12.4 - Logging Architecture,Log rotation,"Log rotation FEATURE STATE: Kubernetes v1.21 [stable] You can configure the kubelet to rotate logs automatically. If you configure rotation, the kubelet is responsible for rotating container logs and managing the logging directory structure.The kubelet sends this information to the container runtime (using CRI),and the runtime writes the container logs to the given location. You can configure two kubelet configuration settings,containerLogMaxSize and containerLogMaxFiles,using the kubelet configuration file.These settings let you configure the maximum size for each log file and the maximum number of files allowed for each container respectively. When you run kubectl logs as inthe basic logging example, the kubelet on the node handles the request andreads directly from the log file. The kubelet returns the content of the log file. Note:Only the contents of the latest log file are available throughkubectl logs.For example, if a Pod writes 40 MiB of logs and the kubelet rotates logsafter 10 MiB, running kubectl logs returns at most 10MiB of data.",235
12.4 - Logging Architecture,System component logs,"System component logs There are two types of system components: those that typically run in a container,and those components directly involved in running containers. For example: The kubelet and container runtime do not run in containers. The kubelet runsyour containers (grouped together in pods)The Kubernetes scheduler, controller manager, and API server run within pods(usually static Pods).The etcd component runs in the control plane, and most commonly also as a static pod.If your cluster uses kube-proxy, you typically run this as a DaemonSet.",120
12.4 - Logging Architecture,Log locations,"Log locations The way that the kubelet and container runtime write logs depends on the operatingsystem that the node uses: LinuxWindows On Linux nodes that use systemd, the kubelet and container runtime write to journaldby default. You use journalctl to read the systemd journal; for example:journalctl -u kubelet.If systemd is not present, the kubelet and container runtime write to .log files in the/var/log directory. If you want to have logs written elsewhere, you can indirectlyrun the kubelet via a helper tool, kube-log-runner, and use that tool to redirectkubelet logs to a directory that you choose.You can also set a logging directory using the deprecated kubelet command lineargument --log-dir. However, the kubelet always directs your container runtime towrite logs into directories within /var/log/pods.For more information on kube-log-runner, read System Logs.By default, the kubelet writes logs to files within the directory C:\var\logs(notice that this is not C:\var\log).Although C:\var\log is the Kubernetes default location for these logs, severalcluster deployment tools set up Windows nodes to log to C:\var\log\kubelet instead.If you want to have logs written elsewhere, you can indirectlyrun the kubelet via a helper tool, kube-log-runner, and use that tool to redirectkubelet logs to a directory that you choose.However, the kubelet always directs your container runtime to write logs within thedirectory C:\var\log\pods.For more information on kube-log-runner, read System Logs.  For Kubernetes cluster components that run in pods, these write to files insidethe /var/log directory, bypassing the default logging mechanism (the componentsdo not write to the systemd journal). You can use Kubernetes' storage mechanismsto map persistent storage into the container that runs the component. For details about etcd and its logs, view the etcd documentation.Again, you can use Kubernetes' storage mechanisms to map persistent storage intothe container that runs the component. Note:If you deploy Kubernetes cluster components (such as the scheduler) to log toa volume shared from the parent node, you need to consider and ensure that thoselogs are rotated. Kubernetes does not manage that log rotation.Your operating system may automatically implement some log rotation - for example,if you share the directory /var/log into a static Pod for a component, node-levellog rotation treats a file in that directory the same as a file written by any componentoutside Kubernetes.Some deploy tools account for that log rotation and automate it; others leave thisas your responsibility.",601
12.4 - Logging Architecture,Cluster-level logging architectures,"Cluster-level logging architectures While Kubernetes does not provide a native solution for cluster-level logging, there are several common approaches you can consider. Here are some options: Use a node-level logging agent that runs on every node.Include a dedicated sidecar container for logging in an application pod.Push logs directly to a backend from within an application.",75
12.4 - Logging Architecture,Using a node logging agent,"Using a node logging agent  You can implement cluster-level logging by including a node-level logging agent on each node. The logging agent is a dedicated tool that exposes logs or pushes logs to a backend. Commonly, the logging agent is a container that has access to a directory with log files from all of the application containers on that node. Because the logging agent must run on every node, it is recommended to run the agentas a DaemonSet. Node-level logging creates only one agent per node and doesn't require any changes to the applications running on the node. Containers write to stdout and stderr, but with no agreed format. A node-level agent collects these logs and forwards them for aggregation.",148
12.4 - Logging Architecture,Using a sidecar container with the logging agent,"Using a sidecar container with the logging agent You can use a sidecar container in one of the following ways: The sidecar container streams application logs to its own stdout.The sidecar container runs a logging agent, which is configured to pick up logs from an application container.",57
12.4 - Logging Architecture,Streaming sidecar container,"Streaming sidecar container  By having your sidecar containers write to their own stdout and stderrstreams, you can take advantage of the kubelet and the logging agent thatalready run on each node. The sidecar containers read logs from a file, a socket,or journald. Each sidecar container prints a log to its own stdout or stderr stream. This approach allows you to separate several log streams from differentparts of your application, some of which can lack supportfor writing to stdout or stderr. The logic behind redirecting logsis minimal, so it's not a significant overhead. Additionally, becausestdout and stderr are handled by the kubelet, you can use built-in toolslike kubectl logs. For example, a pod runs a single container, and the containerwrites to two different log files using two different formats. Here's amanifest for the Pod: admin/logging/two-files-counter-pod.yamlapiVersion: v1kind: Podmetadata:  name: counterspec:  containers:  - name: count    image: busybox:1.28    args:    - /bin/sh    - -c    - >      i=0;      while true;      do        echo ""$i: $(date)"" >> /var/log/1.log;        echo ""$(date) INFO $i"" >> /var/log/2.log;        i=$((i+1));        sleep 1;      done          volumeMounts:    - name: varlog      mountPath: /var/log  volumes:  - name: varlog    emptyDir: {} It is not recommended to write log entries with different formats to the same logstream, even if you managed to redirect both components to the stdout stream ofthe container. Instead, you can create two sidecar containers. Each sidecarcontainer could tail a particular log file from a shared volume and then redirectthe logs to its own stdout stream. Here's a manifest for a pod that has two sidecar containers: admin/logging/two-files-counter-pod-streaming-sidecar.yamlapiVersion: v1kind: Podmetadata:  name: counterspec:  containers:  - name: count    image: busybox:1.28    args:    - /bin/sh    - -c    - >      i=0;      while true;      do        echo ""$i: $(date)"" >> /var/log/1.log;        echo ""$(date) INFO $i"" >> /var/log/2.log;        i=$((i+1));        sleep 1;      done          volumeMounts:    - name: varlog      mountPath: /var/log  - name: count-log-1    image: busybox:1.28    args: [/bin/sh, -c, 'tail -n+1 -F /var/log/1.log']    volumeMounts:    - name: varlog      mountPath: /var/log  - name: count-log-2    image: busybox:1.28    args: [/bin/sh, -c, 'tail -n+1 -F /var/log/2.log']    volumeMounts:    - name: varlog      mountPath: /var/log  volumes:  - name: varlog    emptyDir: {} Now when you run this pod, you can access each log stream separately byrunning the following commands: kubectl logs counter count-log-1 The output is similar to: 0: Fri Apr  1 11:42:26 UTC 20221: Fri Apr  1 11:42:27 UTC 20222: Fri Apr  1 11:42:28 UTC 2022... kubectl logs counter count-log-2 The output is similar to: Fri Apr  1 11:42:29 UTC 2022 INFO 0Fri Apr  1 11:42:30 UTC 2022 INFO 0Fri Apr  1 11:42:31 UTC 2022 INFO 0... If you installed a node-level agent in your cluster, that agent picks up those logstreams automatically without any further configuration. If you like, you can configurethe agent to parse log lines depending on the source container. Even for Pods that only have low CPU and memory usage (order of a couple of millicoresfor cpu and order of several megabytes for memory), writing logs to a file andthen streaming them to stdout can double how much storage you need on the node.If you have an application that writes to a single file, it's recommended to set/dev/stdout as the destination rather than implement the streaming sidecarcontainer approach. Sidecar containers can also be used to rotate log files that cannot be rotated bythe application itself. An example of this approach is a small container runninglogrotate periodically.However, it's more straightforward to use stdout and stderr directly, andleave rotation and retention policies to the kubelet.",1070
12.4 - Logging Architecture,Sidecar container with a logging agent,"Sidecar container with a logging agent  If the node-level logging agent is not flexible enough for your situation, youcan create a sidecar container with a separate logging agent that you haveconfigured specifically to run with your application. Note: Using a logging agent in a sidecar container can leadto significant resource consumption. Moreover, you won't be able to accessthose logs using kubectl logs because they are not controlledby the kubelet. Here are two example manifests that you can use to implement a sidecar container with a logging agent.The first manifest contains a ConfigMapto configure fluentd. admin/logging/fluentd-sidecar-config.yamlapiVersion: v1kind: ConfigMapmetadata:  name: fluentd-configdata:  fluentd.conf: |    <source>      type tail      format none      path /var/log/1.log      pos_file /var/log/1.log.pos      tag count.format1    </source>    <source>      type tail      format none      path /var/log/2.log      pos_file /var/log/2.log.pos      tag count.format2    </source>    <match **>      type google_cloud    </match> Note: In the sample configurations, you can replace fluentd with any logging agent, readingfrom any source inside an application container. The second manifest describes a pod that has a sidecar container running fluentd.The pod mounts a volume where fluentd can pick up its configuration data. admin/logging/two-files-counter-pod-agent-sidecar.yamlapiVersion: v1kind: Podmetadata:  name: counterspec:  containers:  - name: count    image: busybox:1.28    args:    - /bin/sh    - -c    - >      i=0;      while true;      do        echo ""$i: $(date)"" >> /var/log/1.log;        echo ""$(date) INFO $i"" >> /var/log/2.log;        i=$((i+1));        sleep 1;      done          volumeMounts:    - name: varlog      mountPath: /var/log  - name: count-agent    image: registry.k8s.io/fluentd-gcp:1.30    env:    - name: FLUENTD_ARGS      value: -c /etc/fluentd-config/fluentd.conf    volumeMounts:    - name: varlog      mountPath: /var/log    - name: config-volume      mountPath: /etc/fluentd-config  volumes:  - name: varlog    emptyDir: {}  - name: config-volume    configMap:      name: fluentd-config",612
12.4 - Logging Architecture,Exposing logs directly from the application,Exposing logs directly from the application  Cluster-logging that exposes or pushes logs directly from every application is outside the scope of Kubernetes. Read about Kubernetes system logsLearn about Traces For Kubernetes System ComponentsLearn how to customise the termination message that Kubernetes records when a Pod fails,68
12.5 - Metrics For Kubernetes System Components,default,"System component metrics can give a better look into what is happening inside them. Metrics areparticularly useful for building dashboards and alerts. Kubernetes components emit metrics in Prometheus format.This format is structured plain text, designed so that people and machines can both read it.",56
12.5 - Metrics For Kubernetes System Components,Metrics in Kubernetes,"Metrics in Kubernetes In most cases metrics are available on /metrics endpoint of the HTTP server. For components thatdoesn't expose endpoint by default it can be enabled using --bind-address flag. Examples of those components: kube-controller-managerkube-proxykube-apiserverkube-schedulerkubelet In a production environment you may want to configure Prometheus Serveror some other metrics scraper to periodically gather these metrics and make them available in somekind of time series database. Note that kubelet also exposes metrics in/metrics/cadvisor, /metrics/resource and /metrics/probes endpoints. Those metrics do nothave same lifecycle. If your cluster uses RBAC, reading metrics requiresauthorization via a user, group or ServiceAccount with a ClusterRole that allows accessing/metrics. For example: apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: prometheusrules:  - nonResourceURLs:      - ""/metrics""    verbs:      - get",233
12.5 - Metrics For Kubernetes System Components,Metric lifecycle,"Metric lifecycle Alpha metric → Stable metric → Deprecated metric → Hidden metric → Deleted metric Alpha metrics have no stability guarantees. These metrics can be modified or deleted at any time. Stable metrics are guaranteed to not change. This means: A stable metric without a deprecated signature will not be deleted or renamedA stable metric's type will not be modified Deprecated metrics are slated for deletion, but are still available for use.These metrics include an annotation about the version in which they became deprecated. For example: Before deprecation# HELP some_counter this counts things# TYPE some_counter countersome_counter 0After deprecation# HELP some_counter (Deprecated since 1.15.0) this counts things# TYPE some_counter countersome_counter 0 Hidden metrics are no longer published for scraping, but are still available for use. To use ahidden metric, please refer to the Show hidden metrics section. Deleted metrics are no longer published and cannot be used.",202
12.5 - Metrics For Kubernetes System Components,Show hidden metrics,"Show hidden metrics As described above, admins can enable hidden metrics through a command-line flag on a specificbinary. This intends to be used as an escape hatch for admins if they missed the migration of themetrics deprecated in the last release. The flag show-hidden-metrics-for-version takes a version for which you want to show metricsdeprecated in that release. The version is expressed as x.y, where x is the major version, y isthe minor version. The patch version is not needed even though a metrics can be deprecated in apatch release, the reason for that is the metrics deprecation policy runs against the minor release. The flag can only take the previous minor version as it's value. All metrics hidden in previouswill be emitted if admins set the previous version to show-hidden-metrics-for-version. The tooold version is not allowed because this violates the metrics deprecated policy. Take metric A as an example, here assumed that A is deprecated in 1.n. According to metricsdeprecated policy, we can reach the following conclusion: In release 1.n, the metric is deprecated, and it can be emitted by default.In release 1.n+1, the metric is hidden by default and it can be emitted by command lineshow-hidden-metrics-for-version=1.n.In release 1.n+2, the metric should be removed from the codebase. No escape hatch anymore. If you're upgrading from release 1.12 to 1.13, but still depend on a metric A deprecated in1.12, you should set hidden metrics via command line: --show-hidden-metrics=1.12 and rememberto remove this metric dependency before upgrading to 1.14",360
12.5 - Metrics For Kubernetes System Components,Disable accelerator metrics,"Disable accelerator metrics The kubelet collects accelerator metrics through cAdvisor. To collect these metrics, foraccelerators like NVIDIA GPUs, kubelet held an open handle on the driver. This meant that in orderto perform infrastructure changes (for example, updating the driver), a cluster administratorneeded to stop the kubelet agent. The responsibility for collecting accelerator metrics now belongs to the vendor rather than thekubelet. Vendors must provide a container that collects metrics and exposes them to the metricsservice (for example, Prometheus). The DisableAcceleratorUsageMetrics feature gatedisables metrics collected by the kubelet, with atimeline for enabling this feature by default.",142
12.5 - Metrics For Kubernetes System Components,kube-controller-manager metrics,"kube-controller-manager metrics Controller manager metrics provide important insight into the performance and health of thecontroller manager. These metrics include common Go language runtime metrics such as go_routinecount and controller specific metrics such as etcd request latencies or Cloudprovider (AWS, GCE,OpenStack) API latencies that can be used to gauge the health of a cluster. Starting from Kubernetes 1.7, detailed Cloudprovider metrics are available for storage operationsfor GCE, AWS, Vsphere and OpenStack.These metrics can be used to monitor health of persistent volume operations. For example, for GCE these metrics are called: cloudprovider_gce_api_request_duration_seconds { request = ""instance_list""}cloudprovider_gce_api_request_duration_seconds { request = ""disk_insert""}cloudprovider_gce_api_request_duration_seconds { request = ""disk_delete""}cloudprovider_gce_api_request_duration_seconds { request = ""attach_disk""}cloudprovider_gce_api_request_duration_seconds { request = ""detach_disk""}cloudprovider_gce_api_request_duration_seconds { request = ""list_disk""}",268
12.5 - Metrics For Kubernetes System Components,kube-scheduler metrics,"kube-scheduler metrics FEATURE STATE: Kubernetes v1.21 [beta] The scheduler exposes optional metrics that reports the requested resources and the desired limitsof all running pods. These metrics can be used to build capacity planning dashboards, assesscurrent or historical scheduling limits, quickly identify workloads that cannot schedule due tolack of resources, and compare actual usage to the pod's request. The kube-scheduler identifies the resource requests and limitsconfigured for each Pod; when either a request or limit is non-zero, the kube-scheduler reports ametrics timeseries. The time series is labelled by: namespacepod namethe node where the pod is scheduled or an empty string if not yet scheduledprioritythe assigned scheduler for that podthe name of the resource (for example, cpu)the unit of the resource if known (for example, cores) Once a pod reaches completion (has a restartPolicy of Never or OnFailure and is in theSucceeded or Failed pod phase, or has been deleted and all containers have a terminated state)the series is no longer reported since the scheduler is now free to schedule other pods to run.The two metrics are called kube_pod_resource_request and kube_pod_resource_limit. The metrics are exposed at the HTTP endpoint /metrics/resources and require the sameauthorization as the /metrics endpoint on the scheduler. You must use the--show-hidden-metrics-for-version=1.20 flag to expose these alpha stability metrics.",330
12.5 - Metrics For Kubernetes System Components,Disabling metrics,"Disabling metrics You can explicitly turn off metrics via command line flag --disabled-metrics. This may bedesired if, for example, a metric is causing a performance problem. The input is a list ofdisabled metrics (i.e. --disabled-metrics=metric1,metric2).",64
12.5 - Metrics For Kubernetes System Components,Metric cardinality enforcement,"Metric cardinality enforcement Metrics with unbounded dimensions could cause memory issues in the components they instrument. Tolimit resource use, you can use the --allow-label-value command line option to dynamicallyconfigure an allow-list of label values for a metric. In alpha stage, the flag can only take in a series of mappings as metric label allow-list.Each mapping is of the format <metric_name>,<label_name>=<allowed_labels> where<allowed_labels> is a comma-separated list of acceptable label names. The overall format looks like: --allow-label-value <metric_name>,<label_name>='<allow_value1>, <allow_value2>...', <metric_name2>,<label_name>='<allow_value1>, <allow_value2>...', ... Here is an example: --allow-label-value number_count_metric,odd_number='1,3,5', number_count_metric,even_number='2,4,6', date_gauge_metric,weekend='Saturday,Sunday' Read about the Prometheus text formatfor metricsSee the list of stable Kubernetes metricsRead about the Kubernetes deprecation policy",276
12.6 - System Logs,default,"System component logs record events happening in cluster, which can be very useful for debugging.You can configure log verbosity to see more or less detail.Logs can be as coarse-grained as showing errors within a component, or as fine-grained as showingstep-by-step traces of events (like HTTP access logs, pod state changes, controller actions, orscheduler decisions).",81
12.6 - System Logs,Klog,"Klog klog is the Kubernetes logging library. kloggenerates log messages for the Kubernetes system components. For more information about klog configuration, see the Command line tool reference. Kubernetes is in the process of simplifying logging in its components.The following klog command line flagsare deprecatedstarting with Kubernetes 1.23 and will be removed in a future release: --add-dir-header--alsologtostderr--log-backtrace-at--log-dir--log-file--log-file-max-size--logtostderr--one-output--skip-headers--skip-log-headers--stderrthreshold Output will always be written to stderr, regardless of the output format. Output redirection isexpected to be handled by the component which invokes a Kubernetes component. This can be a POSIXshell or a tool like systemd. In some cases, for example a distroless container or a Windows system service, those options arenot available. Then thekube-log-runnerbinary can be used as wrapper around a Kubernetes component to redirectoutput. A prebuilt binary is included in several Kubernetes base images underits traditional name as /go-runner and as kube-log-runner in server andnode release archives. This table shows how kube-log-runner invocations correspond to shell redirection: UsagePOSIX shell (such as bash)kube-log-runner <options> <cmd>Merge stderr and stdout, write to stdout2>&1kube-log-runner (default behavior)Redirect both into log file1>>/tmp/log 2>&1kube-log-runner -log-file=/tmp/logCopy into log file and to stdout2>&1 | tee -a /tmp/logkube-log-runner -log-file=/tmp/log -also-stdoutRedirect only stdout into log file>/tmp/logkube-log-runner -log-file=/tmp/log -redirect-stderr=false",455
12.6 - System Logs,Klog output,Klog output An example of the traditional klog native format: I1025 00:15:15.525108       1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg: (1.512ms) 200 [pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format 10.56.1.19:51756] The message string may contain line breaks: I1025 00:15:15.525108       1 example.go:79] This is a messagewhich has a line break.,158
12.6 - System Logs,Structured Logging,"Structured Logging FEATURE STATE: Kubernetes v1.23 [beta] Warning:Migration to structured log messages is an ongoing process. Not all log messages are structured inthis version. When parsing log files, you must also handle unstructured log messages.Log formatting and value serialization are subject to change. Structured logging introduces a uniform structure in log messages allowing for programmaticextraction of information. You can store and process structured logs with less effort and cost.The code which generates a log message determines whether it uses the traditional unstructuredklog output or structured logging. The default formatting of structured log messages is as text, with a format that is backwardcompatible with traditional klog: <klog header> ""<message>"" <key1>=""<value1>"" <key2>=""<value2>"" ... Example: I1025 00:15:15.525108       1 controller_utils.go:116] ""Pod status updated"" pod=""kube-system/kubedns"" status=""ready"" Strings are quoted. Other values are formatted with%+v, which may cause log messages tocontinue on the next line depending on the data. I1025 00:15:15.525108       1 example.go:116] ""Example"" data=""This is text with a line break\nand \""quotation marks\""."" someInt=1 someFloat=0.1 someStruct={StringField: First line,second line.}",310
12.6 - System Logs,Contextual Logging,"Contextual Logging FEATURE STATE: Kubernetes v1.24 [alpha] Contextual logging builds on top of structured logging. It is primarily abouthow developers use logging calls: code based on that concept is more flexibleand supports additional use cases as described in the Contextual LoggingKEP. If developers use additional functions like WithValues or WithName intheir components, then log entries contain additional information that getspassed into functions by their caller. Currently this is gated behind the StructuredLogging feature gate anddisabled by default. The infrastructure for this was added in 1.24 withoutmodifying components. Thecomponent-base/logs/examplecommand demonstrates how to use the new logging calls and how a componentbehaves that supports contextual logging. $ cd $GOPATH/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/logs/example/cmd/$ go run . --help...      --feature-gates mapStringBool  A set of key=value pairs that describe feature gates for alpha/experimental features. Options are:                                     AllAlpha=true|false (ALPHA - default=false)                                     AllBeta=true|false (BETA - default=false)                                     ContextualLogging=true|false (ALPHA - default=false)$ go run . --feature-gates ContextualLogging=true...I0404 18:00:02.916429  451895 logger.go:94] ""example/myname: runtime"" foo=""bar"" duration=""1m0s""I0404 18:00:02.916447  451895 logger.go:95] ""example: another runtime"" foo=""bar"" duration=""1m0s"" The example prefix and foo=""bar"" were added by the caller of the functionwhich logs the runtime message and duration=""1m0s"" value, without having tomodify that function. With contextual logging disable, WithValues and WithName do nothing and logcalls go through the global klog logger. Therefore this additional informationis not in the log output anymore: $ go run . --feature-gates ContextualLogging=false...I0404 18:03:31.171945  452150 logger.go:94] ""runtime"" duration=""1m0s""I0404 18:03:31.171962  452150 logger.go:95] ""another runtime"" duration=""1m0s""",538
12.6 - System Logs,JSON log format,"JSON log format FEATURE STATE: Kubernetes v1.19 [alpha] Warning:JSON output does not support many standard klog flags. For list of unsupported klog flags, see theCommand line tool reference.Not all logs are guaranteed to be written in JSON format (for example, during process start).If you intend to parse logs, make sure you can handle log lines that are not JSON as well.Field names and JSON serialization are subject to change. The --logging-format=json flag changes the format of logs from klog native format to JSON format.Example of JSON log format (pretty printed): {   ""ts"": 1580306777.04728,   ""v"": 4,   ""msg"": ""Pod status updated"",   ""pod"":{      ""name"": ""nginx-1"",      ""namespace"": ""default""   },   ""status"": ""ready""} Keys with special meaning: ts - timestamp as Unix time (required, float)v - verbosity (only for info and not for error messages, int)err - error string (optional, string)msg - message (required, string) List of components currently supporting JSON format: kube-controller-managerkube-apiserverkube-schedulerkubelet",268
12.6 - System Logs,Log verbosity level,Log verbosity level The -v flag controls log verbosity. Increasing the value increases the number of logged events.Decreasing the value decreases the number of logged events. Increasing verbosity settings logsincreasingly less severe events. A verbosity setting of 0 logs only critical events.,56
12.6 - System Logs,Log location,"Log location There are two types of system components: those that run in a container and thosethat do not run in a container. For example: The Kubernetes scheduler and kube-proxy run in a container.The kubelet and container runtimedo not run in containers. On machines with systemd, the kubelet and container runtime write to journald.Otherwise, they write to .log files in the /var/log directory.System components inside containers always write to .log files in the /var/log directory,bypassing the default logging mechanism.Similar to the container logs, you should rotate system component logs in the /var/log directory.In Kubernetes clusters created by the kube-up.sh script, log rotation is configured by the logrotate tool.The logrotate tool rotates logs daily, or once the log size is greater than 100MB. Read about the Kubernetes Logging ArchitectureRead about Structured LoggingRead about Contextual LoggingRead about deprecation of klog flagsRead about the Conventions for logging severity",230
12.7 - Traces For Kubernetes System Components,default,FEATURE STATE: Kubernetes v1.22 [alpha] System component traces record the latency of and relationships between operations in the cluster. Kubernetes components emit traces using theOpenTelemetry Protocolwith the gRPC exporter and can be collected and routed to tracing backends using anOpenTelemetry Collector.,67
12.7 - Traces For Kubernetes System Components,Trace Collection,"Trace Collection For a complete guide to collecting traces and using the collector, seeGetting Started with the OpenTelemetry Collector.However, there are a few things to note that are specific to Kubernetes components. By default, Kubernetes components export traces using the grpc exporter for OTLP on theIANA OpenTelemetry port, 4317.As an example, if the collector is running as a sidecar to a Kubernetes component,the following receiver configuration will collect spans and log them to standard output: receivers:  otlp:    protocols:      grpc:exporters:  # Replace this exporter with the exporter for your backend  logging:    logLevel: debugservice:  pipelines:    traces:      receivers: [otlp]      exporters: [logging]",173
12.7 - Traces For Kubernetes System Components,kube-apiserver traces,"kube-apiserver traces The kube-apiserver generates spans for incoming HTTP requests, and for outgoing requeststo webhooks, etcd, and re-entrant requests. It propagates theW3C Trace Context with outgoing requestsbut does not make use of the trace context attached to incoming requests,as the kube-apiserver is often a public endpoint.",81
12.7 - Traces For Kubernetes System Components,Enabling tracing in the kube-apiserver,"Enabling tracing in the kube-apiserver To enable tracing, enable the APIServerTracingfeature gateon the kube-apiserver. Also, provide the kube-apiserver with a tracing configuration filewith --tracing-config-file=<path-to-config>. This is an example config that recordsspans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint: apiVersion: apiserver.config.k8s.io/v1alpha1kind: TracingConfiguration# default value#endpoint: localhost:4317samplingRatePerMillion: 100 For more information about the TracingConfiguration struct, seeAPI server config API (v1alpha1).",154
12.7 - Traces For Kubernetes System Components,kubelet traces,"kubelet traces FEATURE STATE: Kubernetes v1.25 [alpha] The kubelet CRI interface and authenticated http servers are instrumented to generatetrace spans. As with the apiserver, the endpoint and sampling rate are configurable.Trace context propagation is also configured. A parent span's sampling decision is always respected.A provided tracing configuration sampling rate will apply to spans without a parent.Enabled without a configured endpoint, the default OpenTelemetry Collector receiver address of ""localhost:4317"" is set.",112
12.7 - Traces For Kubernetes System Components,Enabling tracing in the kubelet,"Enabling tracing in the kubelet To enable tracing, enable the KubeletTracingfeature gateon the kubelet. Also, provide the kubelet with atracing configuration.This is an example snippet of a kubelet config that records spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint: apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationfeatureGates:  KubeletTracing: truetracing:  # default value  #endpoint: localhost:4317  samplingRatePerMillion: 100",131
12.7 - Traces For Kubernetes System Components,Stability,"Stability Tracing instrumentation is still under active development, and may changein a variety of ways. This includes span names, attached attributes,instrumented endpoints, etc. Until this feature graduates to stable,there are no guarantees of backwards compatibility for tracing instrumentation. Read about Getting Started with the OpenTelemetry Collector",66
12.8 - Proxies in Kubernetes,Proxies,"Proxies There are several different proxies you may encounter when using Kubernetes: The kubectl proxy:runs on a user's desktop or in a podproxies from a localhost address to the Kubernetes apiserverclient to proxy uses HTTPproxy to apiserver uses HTTPSlocates apiserveradds authentication headersThe apiserver proxy:is a bastion built into the apiserverconnects a user outside of the cluster to cluster IPs which otherwise might not be reachableruns in the apiserver processesclient to proxy uses HTTPS (or http if apiserver so configured)proxy to target may use HTTP or HTTPS as chosen by proxy using available informationcan be used to reach a Node, Pod, or Servicedoes load balancing when used to reach a ServiceThe kube proxy:runs on each nodeproxies UDP, TCP and SCTPdoes not understand HTTPprovides load balancingis only used to reach servicesA Proxy/Load-balancer in front of apiserver(s):existence and implementation varies from cluster to cluster (e.g. nginx)sits between all clients and one or more apiserversacts as load balancer if there are several apiservers.Cloud Load Balancers on external services:are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)are created automatically when the Kubernetes service has type LoadBalancerusually supports UDP/TCP onlySCTP support is up to the load balancer implementation of the cloud providerimplementation varies by cloud provider. Kubernetes users will typically not need to worry about anything other than the first two types. The cluster adminwill typically ensure that the latter types are set up correctly.",367
12.9 - API Priority and Fairness,default,"FEATURE STATE: Kubernetes v1.20 [beta] Controlling the behavior of the Kubernetes API server in an overload situationis a key task for cluster administrators. The kube-apiserver has some controls available(i.e. the --max-requests-inflight and --max-mutating-requests-inflightcommand-line flags) to limit the amount of outstanding work that will beaccepted, preventing a flood of inbound requests from overloading andpotentially crashing the API server, but these flags are not enough to ensurethat the most important requests get through in a period of high traffic. The API Priority and Fairness feature (APF) is an alternative that improves uponaforementioned max-inflight limitations. APF classifiesand isolates requests in a more fine-grained way. It also introducesa limited amount of queuing, so that no requests are rejected in casesof very brief bursts. Requests are dispatched from queues using afair queuing technique so that, for example, a poorly-behavedcontroller need notstarve others (even at the same priority level). This feature is designed to work well with standard controllers, whichuse informers and react to failures of API requests with exponentialback-off, and other clients that also work this way. Caution: Some requests classified as ""long-running""—such as remotecommand execution or log tailing—are not subject to the APIPriority and Fairness filter. This is also true for the--max-requests-inflight flag without the API Priority and Fairnessfeature enabled. API Priority and Fairness does apply to watchrequests. When API Priority and Fairness is disabled, watch requestsare not subject to the --max-requests-inflight limit.",376
12.9 - API Priority and Fairness,Enabling/Disabling API Priority and Fairness,"Enabling/Disabling API Priority and Fairness The API Priority and Fairness feature is controlled by a feature gateand is enabled by default. See FeatureGatesfor a general explanation of feature gates and how to enable anddisable them. The name of the feature gate for APF is""APIPriorityAndFairness"". This feature also involves an API Group with: (a) av1alpha1 version and a v1beta1 version, disabled by default, and(b) v1beta2 and v1beta3 versions, enabled by default. You candisable the feature gate and API group beta versions by adding thefollowing command-line flags to your kube-apiserver invocation: kube-apiserver \--feature-gates=APIPriorityAndFairness=false \--runtime-config=flowcontrol.apiserver.k8s.io/v1beta2=false,flowcontrol.apiserver.k8s.io/v1beta3=false \ # …and other flags as usual Alternatively, you can enable the v1alpha1 and v1beta1 versions of the API groupwith --runtime-config=flowcontrol.apiserver.k8s.io/v1alpha1=true,flowcontrol.apiserver.k8s.io/v1beta1=true. The command-line flag --enable-priority-and-fairness=false will disable theAPI Priority and Fairness feature, even if other flags have enabled it.",320
12.9 - API Priority and Fairness,Concepts,"Concepts There are several distinct features involved in the API Priority and Fairnessfeature. Incoming requests are classified by attributes of the request usingFlowSchemas, and assigned to priority levels. Priority levels add a degree ofisolation by maintaining separate concurrency limits, so that requests assignedto different priority levels cannot starve each other. Within a priority level,a fair-queuing algorithm prevents requests from different flows from starvingeach other, and allows for requests to be queued to prevent bursty traffic fromcausing failed requests when the average load is acceptably low.",115
12.9 - API Priority and Fairness,Priority Levels,"Priority Levels Without APF enabled, overall concurrency in the API server is limited by thekube-apiserver flags --max-requests-inflight and--max-mutating-requests-inflight. With APF enabled, the concurrency limitsdefined by these flags are summed and then the sum is divided up among aconfigurable set of priority levels. Each incoming request is assigned to asingle priority level, and each priority level will only dispatch as manyconcurrent requests as its particular limit allows. The default configuration, for example, includes separate priority levels forleader-election requests, requests from built-in controllers, and requests fromPods. This means that an ill-behaved Pod that floods the API server withrequests cannot prevent leader election or actions by the built-in controllersfrom succeeding. The concurrency limits of the priority levels are periodicallyadjusted, allowing under-utilized priority levels to temporarily lendconcurrency to heavily-utilized levels. These limits are based onnominal limits and bounds on how much concurrency a priority level maylend and how much it may borrow, all derived from the configurationobjects mentioned below.",238
12.9 - API Priority and Fairness,Seats Occupied by a Request,"Seats Occupied by a Request The above description of concurrency management is the baseline story.In it, requests have different durations but are counted equally atany given moment when comparing against a priority level's concurrencylimit. In the baseline story, each request occupies one unit ofconcurrency. The word ""seat"" is used to mean one unit of concurrency,inspired by the way each passenger on a train or aircraft takes up oneof the fixed supply of seats. But some requests take up more than one seat. Some of these are listrequests that the server estimates will return a large number ofobjects. These have been found to put an exceptionally heavy burdenon the server, among requests that take a similar amount of time torun. For this reason, the server estimates the number of objects thatwill be returned and considers the request to take a number of seatsthat is proportional to that estimated number.",185
12.9 - API Priority and Fairness,Execution time tweaks for watch requests,"Execution time tweaks for watch requests API Priority and Fairness manages watch requests, but this involves acouple more excursions from the baseline behavior. The first concernshow long a watch request is considered to occupy its seat. Dependingon request parameters, the response to a watch request may or may notbegin with create notifications for all the relevant pre-existingobjects. API Priority and Fairness considers a watch request to bedone with its seat once that initial burst of notifications, if any,is over. The normal notifications are sent in a concurrent burst to allrelevant watch response streams whenever the server is notified of anobject create/update/delete. To account for this work, API Priorityand Fairness considers every write request to spend some additionaltime occupying seats after the actual writing is done. The serverestimates the number of notifications to be sent and adjusts the writerequest's number of seats and seat occupancy time to include thisextra work.",189
12.9 - API Priority and Fairness,Queuing,"Queuing Even within a priority level there may be a large number of distinct sources oftraffic. In an overload situation, it is valuable to prevent one stream ofrequests from starving others (in particular, in the relatively common case of asingle buggy client flooding the kube-apiserver with requests, that buggy clientwould ideally not have much measurable impact on other clients at all). This ishandled by use of a fair-queuing algorithm to process requests that are assignedthe same priority level. Each request is assigned to a flow, identified by thename of the matching FlowSchema plus a flow distinguisher — whichis either the requesting user, the target resource's namespace, or nothing — and thesystem attempts to give approximately equal weight to requests in differentflows of the same priority level.To enable distinct handling of distinct instances, controllers that havemany instances should authenticate with distinct usernames After classifying a request into a flow, the API Priority and Fairnessfeature then may assign the request to a queue. This assignment usesa technique known as shuffle sharding, which makes relatively efficient use ofqueues to insulate low-intensity flows from high-intensity flows. The details of the queuing algorithm are tunable for each priority level, andallow administrators to trade off memory use, fairness (the property thatindependent flows will all make progress when total traffic exceeds capacity),tolerance for bursty traffic, and the added latency induced by queuing.",297
12.9 - API Priority and Fairness,Exempt requests,Exempt requests Some requests are considered sufficiently important that they are not subject toany of the limitations imposed by this feature. These exemptions prevent animproperly-configured flow control configuration from totally disabling an APIserver.,44
12.9 - API Priority and Fairness,Resources,"Resources The flow control API involves two kinds of resources.PriorityLevelConfigurationsdefine the available priority levels, the share of the available concurrencybudget that each can handle, and allow for fine-tuning queuing behavior.FlowSchemasare used to classify individual inbound requests, matching each to asingle PriorityLevelConfiguration. There is also a v1alpha1 versionof the same API group, and it has the same Kinds with the same syntax andsemantics.",97
12.9 - API Priority and Fairness,PriorityLevelConfiguration,"PriorityLevelConfiguration A PriorityLevelConfiguration represents a single priority level. EachPriorityLevelConfiguration has an independent limit on the number of outstandingrequests, and limitations on the number of queued requests. The nominal oncurrency limit for a PriorityLevelConfiguration is notspecified in an absolute number of seats, but rather in ""nominalconcurrency shares."" The total concurrency limit for the API Server isdistributed among the existing PriorityLevelConfigurations inproportion to these shares, to give each level its nominal limit interms of seats. This allows a cluster administrator to scale up ordown the total amount of traffic to a server by restartingkube-apiserver with a different value for --max-requests-inflight(or --max-mutating-requests-inflight), and allPriorityLevelConfigurations will see their maximum allowed concurrencygo up (or down) by the same fraction. Caution: In the versions before v1beta3 the relevantPriorityLevelConfiguration field is named ""assured concurrency shares""rather than ""nominal concurrency shares"". Also, in Kubernetes release1.25 and earlier there were no periodic adjustments: thenominal/assured limits were always applied without adjustment. The bounds on how much concurrency a priority level may lend and howmuch it may borrow are expressed in the PriorityLevelConfiguration aspercentages of the level's nominal limit. These are resolved toabsolute numbers of seats by multiplying with the nominal limit /100.0 and rounding. The dynamically adjusted concurrency limit of apriority level is constrained to lie between (a) a lower bound of itsnominal limit minus its lendable seats and (b) an upper bound of itsnominal limit plus the seats it may borrow. At each adjustment thedynamic limits are derived by each priority level reclaiming any lentseats for which demand recently appeared and then jointly fairlyresponding to the recent seat demand on the priority levels, withinthe bounds just described. Caution: With the Priority and Fairness feature enabled, the total concurrency limit forthe server is set to the sum of --max-requests-inflight and--max-mutating-requests-inflight. There is no longer any distinction madebetween mutating and non-mutating requests; if you want to treat themseparately for a given resource, make separate FlowSchemas that match themutating and non-mutating verbs respectively. When the volume of inbound requests assigned to a singlePriorityLevelConfiguration is more than its permitted concurrency level, thetype field of its specification determines what will happen to extra requests.A type of Reject means that excess traffic will immediately be rejected withan HTTP 429 (Too Many Requests) error. A type of Queue means that requestsabove the threshold will be queued, with the shuffle sharding and fair queuing techniques usedto balance progress between request flows. The queuing configuration allows tuning the fair queuing algorithm for apriority level. Details of the algorithm can be read in theenhancement proposal, but in short: Increasing queues reduces the rate of collisions between different flows, atthe cost of increased memory usage. A value of 1 here effectively disables thefair-queuing logic, but still allows requests to be queued.Increasing queueLengthLimit allows larger bursts of traffic to besustained without dropping any requests, at the cost of increasedlatency and memory usage.Changing handSize allows you to adjust the probability of collisions betweendifferent flows and the overall concurrency available to a single flow in anoverload situation.Note: A larger handSize makes it less likely for two individual flows to collide(and therefore for one to be able to starve the other), but more likely thata small number of flows can dominate the apiserver. A larger handSize alsopotentially increases the amount of latency that a single high-traffic flowcan cause. The maximum number of queued requests possible from asingle flow is handSize * queueLengthLimit. Following is a table showing an interesting collection of shufflesharding configurations, showing for each the probability that agiven mouse (low-intensity flow) is squished by the elephants (high-intensity flows) foran illustrative collection of numbers of elephants. Seehttps://play.golang.org/p/Gi0PLgVHiUg , which computes this table. Example Shuffle Sharding ConfigurationsHandSizeQueues1 elephant4 elephants16 elephants12324.428838398950118e-090.114313488300991440.993508960765602410321.550093439632541e-080.06264798402235450.975310151902755410646.601827268370426e-120.000455713209903707760.499999291500893459643.6310049976037345e-110.000455012123041122730.42823148764548588642.25929199850899e-100.00048866970530404460.3593511468112307681286.994461389026097e-133.4055790161620863e-060.0274617313715506371281.0579122850901972e-116.960839379258192e-060.0240615738634014772567.597695465552631e-146.728547142019406e-080.000670966154253368262562.7134626662687968e-122.9516464018476436e-070.000889565464200034865124.116062922897309e-144.982983350480894e-092.26025764343413e-05610246.337324016514285e-168.09060164312957e-114.517408062903668e-07",1272
12.9 - API Priority and Fairness,FlowSchema,"FlowSchema A FlowSchema matches some inbound requests and assigns them to apriority level. Every inbound request is tested against everyFlowSchema in turn, starting with those with numerically lowest ---which we take to be the logically highest --- matchingPrecedence andworking onward. The first match wins. Caution: Only the first matching FlowSchema for a given request matters. If multipleFlowSchemas match a single inbound request, it will be assigned based on the onewith the highest matchingPrecedence. If multiple FlowSchemas with equalmatchingPrecedence match the same request, the one with lexicographicallysmaller name will win, but it's better not to rely on this, and instead toensure that no two FlowSchemas have the same matchingPrecedence. A FlowSchema matches a given request if at least one of its rulesmatches. A rule matches if at least one of its subjects and at leastone of its resourceRules or nonResourceRules (depending on whether theincoming request is for a resource or non-resource URL) matches the request. For the name field in subjects, and the verbs, apiGroups, resources,namespaces, and nonResourceURLs fields of resource and non-resource rules,the wildcard * may be specified to match all values for the given field,effectively removing it from consideration. A FlowSchema's distinguisherMethod.type determines how requests matching thatschema will be separated into flows. It may beeither ByUser, in which case one requesting user will not be able to starveother users of capacity, or ByNamespace, in which case requests for resourcesin one namespace will not be able to starve requests for resources in othernamespaces of capacity, or it may be blank (or distinguisherMethod may beomitted entirely), in which case all requests matched by this FlowSchema will beconsidered part of a single flow. The correct choice for a given FlowSchemadepends on the resource and your particular environment.",420
12.9 - API Priority and Fairness,Mandatory Configuration Objects,"Mandatory Configuration Objects The four mandatory configuration objects reflect fixed built-inguardrail behavior. This is behavior that the servers have beforethose objects exist, and when those objects exist their specs reflectthis behavior. The four mandatory objects are as follows. The mandatory exempt priority level is used for requests that arenot subject to flow control at all: they will always be dispatchedimmediately. The mandatory exempt FlowSchema classifies allrequests from the system:masters group into this prioritylevel. You may define other FlowSchemas that direct other requeststo this priority level, if appropriate.The mandatory catch-all priority level is used in combination withthe mandatory catch-all FlowSchema to make sure that every requestgets some kind of classification. Typically you should not rely onthis catch-all configuration, and should create your own catch-allFlowSchema and PriorityLevelConfiguration (or use the suggestedglobal-default priority level that is installed by default) asappropriate. Because it is not expected to be used normally, themandatory catch-all priority level has a very small concurrencyshare and does not queue requests.",230
12.9 - API Priority and Fairness,Suggested Configuration Objects,"Suggested Configuration Objects The suggested FlowSchemas and PriorityLevelConfigurations constitute areasonable default configuration. You can modify these and/or createadditional configuration objects if you want. If your cluster islikely to experience heavy load then you should consider whatconfiguration will work best. The suggested configuration groups requests into six priority levels: The node-high priority level is for health updates from nodes.The system priority level is for non-health requests from thesystem:nodes group, i.e. Kubelets, which must be able to contactthe API server in order for workloads to be able to schedule onthem.The leader-election priority level is for leader election requests frombuilt-in controllers (in particular, requests for endpoints, configmaps,or leases coming from the system:kube-controller-manager orsystem:kube-scheduler users and service accounts in the kube-systemnamespace). These are important to isolate from other traffic because failuresin leader election cause their controllers to fail and restart, which in turncauses more expensive traffic as the new controllers sync their informers.The workload-high priority level is for other requests from built-incontrollers.The workload-low priority level is for requests from any other serviceaccount, which will typically include all requests from controllers running inPods.The global-default priority level handles all other traffic, e.g.interactive kubectl commands run by nonprivileged users. The suggested FlowSchemas serve to steer requests into the abovepriority levels, and are not enumerated here.",324
12.9 - API Priority and Fairness,Maintenance of the Mandatory and Suggested Configuration Objects,"Maintenance of the Mandatory and Suggested Configuration Objects Each kube-apiserver independently maintains the mandatory andsuggested configuration objects, using initial and periodic behavior.Thus, in a situation with a mixture of servers of different versionsthere may be thrashing as long as different servers have differentopinions of the proper content of these objects. Each kube-apiserver makes an initial maintenance pass over themandatory and suggested configuration objects, and after that doesperiodic maintenance (once per minute) of those objects. For the mandatory configuration objects, maintenance consists ofensuring that the object exists and, if it does, has the proper spec.The server refuses to allow a creation or update with a spec that isinconsistent with the server's guardrail behavior. Maintenance of suggested configuration objects is designed to allowtheir specs to be overridden. Deletion, on the other hand, is notrespected: maintenance will restore the object. If you do not want asuggested configuration object then you need to keep it around but setits spec to have minimal consequences. Maintenance of suggestedobjects is also designed to support automatic migration when a newversion of the kube-apiserver is rolled out, albeit potentially withthrashing while there is a mixed population of servers. Maintenance of a suggested configuration object consists of creatingit --- with the server's suggested spec --- if the object does notexist. OTOH, if the object already exists, maintenance behaviordepends on whether the kube-apiservers or the users control theobject. In the former case, the server ensures that the object's specis what the server suggests; in the latter case, the spec is leftalone. The question of who controls the object is answered by first lookingfor an annotation with key apf.kubernetes.io/autoupdate-spec. Ifthere is such an annotation and its value is true then thekube-apiservers control the object. If there is such an annotationand its value is false then the users control the object. Ifneither of those condtions holds then the metadata.generation of theobject is consulted. If that is 1 then the kube-apiservers controlthe object. Otherwise the users control the object. These rules wereintroduced in release 1.22 and their consideration ofmetadata.generation is for the sake of migration from the simplerearlier behavior. Users who wish to control a suggested configurationobject should set its apf.kubernetes.io/autoupdate-spec annotationto false. Maintenance of a mandatory or suggested configuration object alsoincludes ensuring that it has an apf.kubernetes.io/autoupdate-specannotation that accurately reflects whether the kube-apiserverscontrol the object. Maintenance also includes deleting objects that are neither mandatorynor suggested but are annotatedapf.kubernetes.io/autoupdate-spec=true.",606
12.9 - API Priority and Fairness,Health check concurrency exemption,"Health check concurrency exemption The suggested configuration gives no special treatment to the healthcheck requests on kube-apiservers from their local kubelets --- whichtend to use the secured port but supply no credentials. With thesuggested config, these requests get assigned to the global-defaultFlowSchema and the corresponding global-default priority level,where other traffic can crowd them out. If you add the following additional FlowSchema, this exempts thoserequests from rate limiting. Caution: Making this change also allows any hostile party to then sendhealth-check requests that match this FlowSchema, at any volume theylike. If you have a web traffic filter or similar external securitymechanism to protect your cluster's API server from general internettraffic, you can configure rules to block any health check requeststhat originate from outside your cluster. priority-and-fairness/health-for-strangers.yamlapiVersion: flowcontrol.apiserver.k8s.io/v1beta2kind: FlowSchemametadata:  name: health-for-strangersspec:  matchingPrecedence: 1000  priorityLevelConfiguration:    name: exempt  rules:    - nonResourceRules:      - nonResourceURLs:          - ""/healthz""          - ""/livez""          - ""/readyz""        verbs:          - ""*""      subjects:        - kind: Group          group:            name: ""system:unauthenticated""",311
12.9 - API Priority and Fairness,Diagnostics,"Diagnostics Every HTTP response from an API server with the priority and fairness featureenabled has two extra headers: X-Kubernetes-PF-FlowSchema-UID andX-Kubernetes-PF-PriorityLevel-UID, noting the flow schema that matched the requestand the priority level to which it was assigned, respectively. The API objects'names are not included in these headers in case the requesting user does nothave permission to view them, so when debugging you can use a command like kubectl get flowschemas -o custom-columns=""uid:{metadata.uid},name:{metadata.name}""kubectl get prioritylevelconfigurations -o custom-columns=""uid:{metadata.uid},name:{metadata.name}"" to get a mapping of UIDs to names for both FlowSchemas andPriorityLevelConfigurations.",182
12.9 - API Priority and Fairness,Metrics,"Metrics Note: In versions of Kubernetes before v1.20, the labels flow_schema andpriority_level were inconsistently named flowSchema and priorityLevel,respectively. If you're running Kubernetes versions v1.19 and earlier, youshould refer to the documentation for your version. When you enable the API Priority and Fairness feature, the kube-apiserverexports additional metrics. Monitoring these can help you determine whether yourconfiguration is inappropriately throttling important traffic, or findpoorly-behaved workloads that may be harming system health. apiserver_flowcontrol_rejected_requests_total is a counter vector(cumulative since server start) of requests that were rejected,broken down by the labels flow_schema (indicating the one thatmatched the request), priority_level (indicating the one to whichthe request was assigned), and reason. The reason label will behave one of the following values:queue-full, indicating that too many requests were alreadyqueued,concurrency-limit, indicating that thePriorityLevelConfiguration is configured to reject rather thanqueue excess requests, ortime-out, indicating that the request was still in the queuewhen its queuing time limit expired.apiserver_flowcontrol_dispatched_requests_total is a countervector (cumulative since server start) of requests that beganexecuting, broken down by the labels flow_schema (indicating theone that matched the request) and priority_level (indicating theone to which the request was assigned).apiserver_current_inqueue_requests is a gauge vector of recenthigh water marks of the number of queued requests, grouped by alabel named request_kind whose value is mutating or readOnly.These high water marks describe the largest number seen in the onesecond window most recently completed. These complement the olderapiserver_current_inflight_requests gauge vector that holds thelast window's high water mark of number of requests actively beingserved.apiserver_flowcontrol_read_vs_write_current_requests is ahistogram vector of observations, made at the end of everynanosecond, of the number of requests broken down by the labelsphase (which takes on the values waiting and executing) andrequest_kind (which takes on the values mutating andreadOnly). Each observed value is a ratio, between 0 and 1, of anumber of requests divided by the corresponding limit on the numberof requests (queue volume limit for waiting and concurrency limitfor executing).apiserver_flowcontrol_current_inqueue_requests is a gauge vectorholding the instantaneous number of queued (not executing) requests,broken down by the labels priority_level and flow_schema.apiserver_flowcontrol_current_executing_requests is a gauge vectorholding the instantaneous number of executing (not waiting in aqueue) requests, broken down by the labels priority_level andflow_schema.apiserver_flowcontrol_request_concurrency_in_use is a gauge vectorholding the instantaneous number of occupied seats, broken down bythe labels priority_level and flow_schema.apiserver_flowcontrol_priority_level_request_utilization is ahistogram vector of observations, made at the end of eachnanosecond, of the number of requests broken down by the labelsphase (which takes on the values waiting and executing) andpriority_level. Each observed value is a ratio, between 0 and 1,of a number of requests divided by the corresponding limit on thenumber of requests (queue volume limit for waiting and concurrencylimit for executing).apiserver_flowcontrol_priority_level_seat_utilization is ahistogram vector of observations, made at the end of eachnanosecond, of the utilization of a priority level's concurrencylimit, broken down by priority_level. This utilization is thefraction (number of seats occupied) / (concurrency limit). Thismetric considers all stages of execution (both normal and the extradelay at the end of a write to cover for the correspondingnotification work) of all requests except WATCHes; for those itconsiders only the initial stage that delivers notifications ofpre-existing objects. Each histogram in the vector is also labeledwith phase: executing (there is no seat limit for the waitingphase).apiserver_flowcontrol_request_queue_length_after_enqueue is ahistogram vector of queue lengths for the queues, broken down bythe labels priority_level and flow_schema, as sampled by theenqueued requests. Each request that gets queued contributes onesample to its histogram, reporting the length of the queue immediatelyafter the request was added. Note that this produces differentstatistics than an unbiased survey would.Note: An outlier value in a histogram here means it is likely that a single flow(i.e., requests by one user or for one namespace, depending onconfiguration) is flooding the API server, and being throttled. By contrast,if one priority level's histogram shows that all queues for that prioritylevel are longer than those for other priority levels, it may be appropriateto increase that PriorityLevelConfiguration's concurrency shares.apiserver_flowcontrol_request_concurrency_limit is the same asapiserver_flowcontrol_nominal_limit_seats. Before theintroduction of concurrency borrowing between priority levels, thiswas always equal to apiserver_flowcontrol_current_limit_seats(which did not exist as a distinct metric).apiserver_flowcontrol_nominal_limit_seats is a gauge vectorholding each priority level's nominal concurrency limit, computedfrom the API server's total concurrency limit and the prioritylevel's configured nominal concurrency shares.apiserver_flowcontrol_lower_limit_seats is a gauge vector holdingthe lower bound on each priority level's dynamic concurrency limit.apiserver_flowcontrol_upper_limit_seats is a gauge vector holdingthe upper bound on each priority level's dynamic concurrency limit.apiserver_flowcontrol_demand_seats is a histogram vector countingobservations, at the end of every nanosecond, of each prioritylevel's ratio of (seat demand) / (nominal concurrency limit). Apriority level's seat demand is the sum, over both queued requestsand those in the initial phase of execution, of the maximum of thenumber of seats occupied in the request's initial and finalexecution phases.apiserver_flowcontrol_demand_seats_high_watermark is a gauge vectorholding, for each priority level, the maximum seat demand seenduring the last concurrency borrowing adjustment period.apiserver_flowcontrol_demand_seats_average is a gauge vectorholding, for each priority level, the time-weighted average seatdemand seen during the last concurrency borrowing adjustment period.apiserver_flowcontrol_demand_seats_stdev is a gauge vectorholding, for each priority level, the time-weighted populationstandard deviation of seat demand seen during the last concurrencyborrowing adjustment period.apiserver_flowcontrol_demand_seats_smoothed is a gauge vectorholding, for each priority level, the smoothed enveloped seat demanddetermined at the last concurrency adjustment.apiserver_flowcontrol_target_seats is a gauge vector holding, foreach priority level, the concurrency target going into the borrowingallocation problem.apiserver_flowcontrol_seat_fair_frac is a gauge holding the fairallocation fraction determined in the last borrowing adjustment.apiserver_flowcontrol_current_limit_seats is a gauge vectorholding, for each priority level, the dynamic concurrency limitderived in the last adjustment.apiserver_flowcontrol_request_wait_duration_seconds is a histogramvector of how long requests spent queued, broken down by the labelsflow_schema (indicating which one matched the request),priority_level (indicating the one to which the request wasassigned), and execute (indicating whether the request startedexecuting).Note: Since each FlowSchema always assigns requests to a singlePriorityLevelConfiguration, you can add the histograms for all theFlowSchemas for one priority level to get the effective histogram forrequests assigned to that priority level.apiserver_flowcontrol_request_execution_seconds is a histogramvector of how long requests took to actually execute, broken down bythe labels flow_schema (indicating which one matched the request)and priority_level (indicating the one to which the request wasassigned).apiserver_flowcontrol_watch_count_samples is a histogram vector ofthe number of active WATCH requests relevant to a given write,broken down by flow_schema and priority_level.apiserver_flowcontrol_work_estimated_seats is a histogram vectorof the number of estimated seats (maximum of initial and final stageof execution) associated with requests, broken down by flow_schemaand priority_level.apiserver_flowcontrol_request_dispatch_no_accommodation_total is acounter vec of the number of events that in principle could have ledto a request being dispatched but did not, due to lack of availableconcurrency, broken down by flow_schema and priority_level. Therelevant sorts of events are arrival of a request and completion ofa request.",2011
12.9 - API Priority and Fairness,Debug endpoints,"Debug endpoints When you enable the API Priority and Fairness feature, the kube-apiserverserves the following additional paths at its HTTP[S] ports. /debug/api_priority_and_fairness/dump_priority_levels - a listing ofall the priority levels and the current state of each. You can fetch like this:kubectl get --raw /debug/api_priority_and_fairness/dump_priority_levelsThe output is similar to this:PriorityLevelName, ActiveQueues, IsIdle, IsQuiescing, WaitingRequests, ExecutingRequests,workload-low,      0,            true,   false,       0,               0,global-default,    0,            true,   false,       0,               0,exempt,            <none>,       <none>, <none>,      <none>,          <none>,catch-all,         0,            true,   false,       0,               0,system,            0,            true,   false,       0,               0,leader-election,   0,            true,   false,       0,               0,workload-high,     0,            true,   false,       0,               0,/debug/api_priority_and_fairness/dump_queues - a listing of all thequeues and their current state. You can fetch like this:kubectl get --raw /debug/api_priority_and_fairness/dump_queuesThe output is similar to this:PriorityLevelName, Index,  PendingRequests, ExecutingRequests, VirtualStart,workload-high,     0,      0,               0,                 0.0000,workload-high,     1,      0,               0,                 0.0000,workload-high,     2,      0,               0,                 0.0000,...leader-election,   14,     0,               0,                 0.0000,leader-election,   15,     0,               0,                 0.0000,/debug/api_priority_and_fairness/dump_requests - a listing of all the requeststhat are currently waiting in a queue. You can fetch like this:kubectl get --raw /debug/api_priority_and_fairness/dump_requestsThe output is similar to this:PriorityLevelName, FlowSchemaName, QueueIndex, RequestIndexInQueue, FlowDistingsher,       ArriveTime,exempt,            <none>,         <none>,     <none>,              <none>,                <none>,system,            system-nodes,   12,         0,                   system:node:127.0.0.1, 2020-07-23T15:26:57.179170694Z,In addition to the queued requests, the output includes one phantom linefor each priority level that is exempt from limitation.You can get a more detailed listing with a command like this:kubectl get --raw '/debug/api_priority_and_fairness/dump_requests?includeRequestDetails=1'The output is similar to this:PriorityLevelName, FlowSchemaName, QueueIndex, RequestIndexInQueue, FlowDistingsher,       ArriveTime,                     UserName,              Verb,   APIPath,                                                     Namespace, Name,   APIVersion, Resource, SubResource,system,            system-nodes,   12,         0,                   system:node:127.0.0.1, 2020-07-23T15:31:03.583823404Z, system:node:127.0.0.1, create, /api/v1/namespaces/scaletest/configmaps,system,            system-nodes,   12,         1,                   system:node:127.0.0.1, 2020-07-23T15:31:03.594555947Z, system:node:127.0.0.1, create, /api/v1/namespaces/scaletest/configmaps,",893
12.9 - API Priority and Fairness,Debug logging,"Debug logging At -v=3 or more verbose the server outputs an httplog line for everyrequest, and it includes the following attributes. apf_fs: the name of the flow schema to which the request was classified.apf_pl: the name of the priority level for that flow schema.apf_iseats: the number of seats determined for the initial(normal) stage of execution of the request.apf_fseats: the number of seats determined for the final stage ofexecution (accounting for the associated WATCH notifications) of therequest.apf_additionalLatency: the duration of the final stage ofexecution of the request. At higher levels of verbosity there will be log lines exposing detailsof how APF handled the request, primarily for debug purposes.",168
12.9 - API Priority and Fairness,Response headers,"Response headers APF adds the following two headers to each HTTP response message. X-Kubernetes-PF-FlowSchema-UID holds the UID of the FlowSchemaobject to which the corresponding request was classified.X-Kubernetes-PF-PriorityLevel-UID holds the UID of thePriorityLevelConfiguration object associated with that FlowSchema. For background information on design details for API priority and fairness, seethe enhancement proposal.You can make suggestions and feature requests via SIG API Machineryor the feature's slack channel.",113
12.10 - Installing Addons,default,"Note:This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information. Add-ons extend the functionality of Kubernetes. This page lists some of the available add-ons and links to their respective installation instructions. The list does not try to be exhaustive.",98
12.10 - Installing Addons,Networking and Network Policy,"Networking and Network Policy ACI provides integrated container networking and network security with Cisco ACI.Antrea operates at Layer 3/4 to provide networking and security services for Kubernetes, leveraging Open vSwitch as the networking data plane. Antrea is a CNCF project at the Sandbox level.Calico is a networking and network policy provider. Calico supports a flexible set of networking options so you can choose the most efficient option for your situation, including non-overlay and overlay networks, with or without BGP. Calico uses the same engine to enforce network policy for hosts, pods, and (if using Istio & Envoy) applications at the service mesh layer.Canal unites Flannel and Calico, providing networking and network policy.Cilium is a networking, observability, and security solution with an eBPF-based data plane. Cilium provides a simple flat Layer 3 network with the ability to span multiple clusters in either a native routing or overlay/encapsulation mode, and can enforce network policies on L3-L7 using an identity-based security model that is decoupled from network addressing. Cilium can act as a replacement for kube-proxy; it also offers additional, opt-in observability and security features. Cilium is a CNCF project at the Incubation level.CNI-Genie enables Kubernetes to seamlessly connect to a choice of CNI plugins, such as Calico, Canal, Flannel, or Weave. CNI-Genie is a CNCF project at the Sandbox level.Contiv provides configurable networking (native L3 using BGP, overlay using vxlan, classic L2, and Cisco-SDN/ACI) for various use cases and a rich policy framework. Contiv project is fully open sourced. The installer provides both kubeadm and non-kubeadm based installation options.Contrail, based on Tungsten Fabric, is an open source, multi-cloud network virtualization and policy management platform. Contrail and Tungsten Fabric are integrated with orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide isolation modes for virtual machines, containers/pods and bare metal workloads.Flannel is an overlay network provider that can be used with Kubernetes.Knitter is a plugin to support multiple network interfaces in a Kubernetes pod.Multus is a Multi plugin for multiple network support in Kubernetes to support all CNI plugins (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK and VPP based workloads in Kubernetes.OVN-Kubernetes is a networking provider for Kubernetes based on OVN (Open Virtual Network), a virtual networking implementation that came out of the Open vSwitch (OVS) project. OVN-Kubernetes provides an overlay based networking implementation for Kubernetes, including an OVS based implementation of load balancing and network policy.Nodus is an OVN based CNI controller plugin to provide cloud native based Service function chaining(SFC).NSX-T Container Plug-in (NCP) provides integration between VMware NSX-T and container orchestrators such as Kubernetes, as well as integration between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service (PKS) and OpenShift.Nuage is an SDN platform that provides policy-based networking between Kubernetes Pods and non-Kubernetes environments with visibility and security monitoring.Romana is a Layer 3 networking solution for pod networks that also supports the NetworkPolicy API.Weave Net provides networking and network policy, will carry on working on both sides of a network partition, and does not require an external database.",832
12.10 - Installing Addons,Visualization & Control,"Visualization & Control Dashboard is a dashboard web interface for Kubernetes.Weave Scope is a tool for graphically visualizing your containers, pods, services etc. Use it in conjunction with a Weave Cloud account or host the UI yourself.",52
12.10 - Installing Addons,Infrastructure,Infrastructure KubeVirt is an add-on to run virtual machines on Kubernetes. Usually run on bare-metal clusters.Thenode problem detectorruns on Linux nodes and reports system issues as eitherEvents orNode conditions.,48
13 - Extending Kubernetes,default,"Different ways to change the behavior of your Kubernetes cluster. Kubernetes is highly configurable and extensible. As a result, there is rarely a need to fork orsubmit patches to the Kubernetes project code. This guide describes the options for customizing a Kubernetes cluster. It is aimed atcluster operators who want to understandhow to adapt their Kubernetes cluster to the needs of their work environment. Developers who areprospective Platform Developers orKubernetes Project Contributors will alsofind it useful as an introduction to what extension points and patterns exist, and theirtrade-offs and limitations. Customization approaches can be broadly divided into configuration, which onlyinvolves changing command line arguments, local configuration files, or API resources; and extensions,which involve running additional programs, additional network services, or both.This document is primarily about extensions.",181
13 - Extending Kubernetes,Configuration,"Configuration Configuration files and command arguments are documented in the Reference section of the onlinedocumentation, with a page for each binary: kube-apiserverkube-controller-managerkube-schedulerkubeletkube-proxy Command arguments and configuration files may not always be changeable in a hosted Kubernetes service or adistribution with managed installation. When they are changeable, they are usually only changeableby the cluster operator. Also, they are subject to change in future Kubernetes versions, andsetting them may require restarting processes. For those reasons, they should be used only whenthere are no other options. Built-in policy APIs, such as ResourceQuota,NetworkPolicy and Role-based Access Control(RBAC), are built-in Kubernetes APIs that provide declaratively configured policy settings.APIs are typically usable even with hosted Kubernetes services and with managed Kubernetes installations.The built-in policy APIs follow the same conventions as other Kubernetes resources such as Pods.When you use a policy APIs that is stable, you benefit from adefined support policy like other Kubernetes APIs.For these reasons, policy APIs are recommended over configuration files and command arguments where suitable.",264
13 - Extending Kubernetes,Extensions,"Extensions Extensions are software components that extend and deeply integrate with Kubernetes.They adapt it to support new types and new kinds of hardware. Many cluster administrators use a hosted or distribution instance of Kubernetes.These clusters come with extensions pre-installed. As a result, most Kubernetesusers will not need to install extensions and even fewer users will need to author new ones.",81
13 - Extending Kubernetes,Extension patterns,"Extension patterns Kubernetes is designed to be automated by writing client programs. Anyprogram that reads and/or writes to the Kubernetes API can provide usefulautomation. Automation can run on the cluster or off it. By followingthe guidance in this doc you can write highly available and robust automation.Automation generally works with any Kubernetes cluster, including hostedclusters and managed installations. There is a specific pattern for writing client programs that work well withKubernetes called the controllerpattern. Controllers typically read an object's .spec, possibly do things, and thenupdate the object's .status. A controller is a client of the Kubernetes API. When Kubernetes is the client and callsout to a remote service, Kubernetes calls this a webhook. The remote service is calleda webhook backend. As with custom controllers, webhooks do add a point of failure. Note: Outside of Kubernetes, the term “webhook” typically refers to a mechanism for asynchronousnotifications, where the webhook call serves as a one-way notification to another system orcomponent. In the Kubernetes ecosystem, even synchronous HTTP callouts are oftendescribed as “webhooks”. In the webhook model, Kubernetes makes a network request to a remote service.With the alternative binary Plugin model, Kubernetes executes a binary (program).Binary plugins are used by the kubelet (for example, CSI storage pluginsand CNI network plugins),and by kubectl (see Extend kubectl with plugins).",343
13 - Extending Kubernetes,Key to the figure,"Key to the figure Users often interact with the Kubernetes API using kubectl. Pluginscustomise the behaviour of clients. There are generic extensions that can apply to different clients,as well as specific ways to extend kubectl.The API server handles all requests. Several types of extension points in the API server allowauthenticating requests, or blocking them based on their content, editing content, and handlingdeletion. These are described in the API Access Extensions section.The API server serves various kinds of resources. Built-in resource kinds, such aspods, are defined by the Kubernetes project and can't be changed.Read API extensions to learn about extending the Kubernetes API.The Kubernetes scheduler decideswhich nodes to place pods on. There are several ways to extend scheduling, which aredescribed in the Scheduling extensions section.Much of the behavior of Kubernetes is implemented by programs calledcontrollers, that areclients of the API server. Controllers are often used in conjunction with custom resources.Read combining new APIs with automation andChanging built-in resources to learn more.The kubelet runs on servers (nodes), and helps pods appear like virtual servers with their own IPs onthe cluster network. Network Plugins allow for different implementations ofpod networking.You can use Device Plugins to integrate custom hardware or other specialnode-local facilities, and make these available to Pods running in your cluster. The kubeletincludes support for working with device plugins.The kubelet also mounts and unmountsvolume for pods and their containers.You can use Storage Plugins to add support for new kindsof storage and other volume types.",353
13 - Extending Kubernetes,Client extensions,"Client extensions Plugins for kubectl are separate binaries that add or replace the behavior of specific subcommands.The kubectl tool can also integrate with credential pluginsThese extensions only affect a individual user's local environment, and so cannot enforce site-wide policies. If you want to extend the kubectl tool, read Extend kubectl with plugins.",77
13 - Extending Kubernetes,Custom resource definitions,"Custom resource definitions Consider adding a Custom Resource to Kubernetes if you want to define new controllers, applicationconfiguration objects or other declarative APIs, and to manage them using Kubernetes tools, suchas kubectl. For more about Custom Resources, see theCustom Resources concept guide.",63
13 - Extending Kubernetes,Combining new APIs with automation,"Combining new APIs with automation A combination of a custom resource API and a control loop is called thecontrollers pattern. If your controller takesthe place of a human operator deploying infrastructure based on a desired state, then the controllermay also be following the operator pattern.The Operator pattern is used to manage specific applications; usually, these are applications thatmaintain state and require care in how they are managed. You can also make your own custom APIs and control loops that manage other resources, such as storage,or to define policies (such as an access control restriction).",115
13 - Extending Kubernetes,Changing built-in resources,"Changing built-in resources When you extend the Kubernetes API by adding custom resources, the added resources always fallinto a new API Groups. You cannot replace or change existing API groups.Adding an API does not directly let you affect the behavior of existing APIs (such as Pods), whereasAPI Access Extensions do.",65
13 - Extending Kubernetes,API access extensions,"API access extensions When a request reaches the Kubernetes API Server, it is first authenticated, then authorized,and is then subject to various types of admission control (some requests are in fact notauthenticated, and get special treatment). SeeControlling Access to the Kubernetes APIfor more on this flow. Each of the steps in the Kubernetes authentication / authorization flow offers extension points.",84
13 - Extending Kubernetes,Authentication,"Authentication Authentication maps headers or certificatesin all requests to a username for the client making the request. Kubernetes has several built-in authentication methods that it supports. It can also sit behind anauthenticating proxy, and it can send a token from an Authorization: header to a remote service forverification (an authentication webhook)if those don't meet your needs.",77
13 - Extending Kubernetes,Authorization,"Authorization Authorization determines whether specificusers can read, write, and do other operations on API resources. It works at the level of wholeresources -- it doesn't discriminate based on arbitrary object fields. If the built-in authorization options don't meet your needs, anauthorization webhookallows calling out to custom code that makes an authorization decision.",69
13 - Extending Kubernetes,Dynamic admission control,"Dynamic admission control After a request is authorized, if it is a write operation, it also goes throughAdmission Control steps.In addition to the built-in steps, there are several extensions: The Image Policy webhookrestricts what images can be run in containers.To make arbitrary admission control decisions, a generalAdmission webhookcan be used. Admission webhooks can reject creations or updates.Some admission webhooks modify the incoming request data before it is handled further by Kubernetes.",103
13 - Extending Kubernetes,Storage plugins,"Storage plugins Container Storage Interface (CSI) plugins providea way to extend Kubernetes with supports for new kinds of volumes. The volumes can be backed bydurable external storage, or provide ephemeral storage, or they might offer a read-only interfaceto information using a filesystem paradigm. Kubernetes also includes support for FlexVolume plugins,which are deprecated since Kubernetes v1.23 (in favour of CSI). FlexVolume plugins allow users to mount volume types that aren't natively supported by Kubernetes. Whenyou run a Pod that relies on FlexVolume storage, the kubelet calls a binary plugin to mount the volume.The archived FlexVolumedesign proposal has more detail on this approach. The Kubernetes Volume Plugin FAQ for Storage Vendorsincludes general information on storage plugins.",172
13 - Extending Kubernetes,Network plugins,Network plugins Your Kubernetes cluster needs a network plugin in order to have a working Pod networkand to support other aspects of the Kubernetes network model. Network Pluginsallow Kubernetes to work with different networking topologies and technologies.,52
13 - Extending Kubernetes,Scheduling extensions,"Scheduling extensions The scheduler is a special type of controller that watches pods, and assignspods to nodes. The default scheduler can be replaced entirely, whilecontinuing to use other Kubernetes components, ormultiple schedulerscan run at the same time. This is a significant undertaking, and almost all Kubernetes users find theydo not need to modify the scheduler. You can control which scheduling pluginsare active, or associate sets of plugins with different named scheduler profiles.You can also write your own plugin that integrates with one or more of the kube-scheduler'sextension points. Finally, the built in kube-scheduler component supports awebhookthat permits a remote HTTP backend (scheduler extension) to filter and / or prioritizethe nodes that the kube-scheduler chooses for a pod. Note: You can only affect node filteringand node prioritization with a scheduler extender webhook; other extension points arenot available through the webhook integration. Learn more about infrastructure extensionsDevice PluginsNetwork PluginsCSI storage pluginsLearn about kubectl pluginsLearn more about Custom ResourcesLearn more about Extension API ServersLearn about Dynamic admission controlLearn about the Operator pattern",256
"13.1 - Compute, Storage, and Networking Extensions",default,"This section covers extensions to your cluster that do not come as part as Kubernetes itself.You can use these extensions to enhance the nodes in your cluster, or to provide the networkfabric that links Pods together. CSI and FlexVolume storage pluginsContainer Storage Interface (CSI) pluginsprovide a way to extend Kubernetes with supports for new kinds of volumes. The volumes canbe backed by durable external storage, or provide ephemeral storage, or they might offer aread-only interface to information using a filesystem paradigm.Kubernetes also includes support for FlexVolumeplugins, which are deprecated since Kubernetes v1.23 (in favour of CSI).FlexVolume plugins allow users to mount volume types that aren't nativelysupported by Kubernetes. When you run a Pod that relies on FlexVolumestorage, the kubelet calls a binary plugin to mount the volume. The archivedFlexVolumedesign proposal has more detail on this approach.The Kubernetes Volume Plugin FAQ for Storage Vendorsincludes general information on storage plugins.Device pluginsDevice plugins allow a node to discover new Node facilities (in addition to thebuilt-in node resources such as cpu and memory), and provide these custom node-localfacilities to Pods that request them.Network pluginsA network plugin allow Kubernetes to work with different networking topologies and technologies.Your Kubernetes cluster needs a network plugin in order to have a working Pod networkand to support other aspects of the Kubernetes network model.Kubernetes 1.26 is compatible with CNInetwork plugins.",338
13.1.1 - Network Plugins,default,Kubernetes 1.26 supports Container Network Interface(CNI) plugins for cluster networking. You must use a CNI plugin that is compatible with yourcluster and that suits your needs. Different plugins are available (both open- and closed- source)in the wider Kubernetes ecosystem. A CNI plugin is required to implement theKubernetes network model. You must use a CNI plugin that is compatible with thev0.4.0 or laterreleases of the CNI specification. The Kubernetes project recommends using a plugin that iscompatible with the v1.0.0CNI specification (plugins can be compatible with multiple spec versions).,140
13.1.1 - Network Plugins,Installation,"Installation A Container Runtime, in the networking context, is a daemon on a node configured to provide CRIServices for kubelet. In particular, the Container Runtime must be configured to load the CNIplugins required to implement the Kubernetes network model. Note:Prior to Kubernetes 1.24, the CNI plugins could also be managed by the kubelet using thecni-bin-dir and network-plugin command-line parameters.These command-line parameters were removed in Kubernetes 1.24, with management of the CNI nolonger in scope for kubelet.See Troubleshooting CNI plugin-related errorsif you are facing issues following the removal of dockershim. For specific information about how a Container Runtime manages the CNI plugins, see thedocumentation for that Container Runtime, for example: containerdCRI-O For specific information about how to install and manage a CNI plugin, see the documentation forthat plugin or networking provider.",208
13.1.1 - Network Plugins,Network Plugin Requirements,"Network Plugin Requirements For plugin developers and users who regularly build or deploy Kubernetes, the plugin may also needspecific configuration to support kube-proxy. The iptables proxy depends on iptables, and theplugin may need to ensure that container traffic is made available to iptables. For example, ifthe plugin connects containers to a Linux bridge, the plugin must set thenet/bridge/bridge-nf-call-iptables sysctl to 1 to ensure that the iptables proxy functionscorrectly. If the plugin does not use a Linux bridge, but uses something like Open vSwitch orsome other mechanism instead, it should ensure container traffic is appropriately routed for theproxy. By default, if no kubelet network plugin is specified, the noop plugin is used, which setsnet/bridge/bridge-nf-call-iptables=1 to ensure simple configurations (like Docker with a bridge)work correctly with the iptables proxy.",201
13.1.1 - Network Plugins,Loopback CNI,"Loopback CNI In addition to the CNI plugin installed on the nodes for implementing the Kubernetes networkmodel, Kubernetes also requires the container runtimes to provide a loopback interface lo, whichis used for each sandbox (pod sandboxes, vm sandboxes, ...).Implementing the loopback interface can be accomplished by re-using theCNI loopback plugin.or by developing your own code to achieve this (seethis example from CRI-O).",101
13.1.1 - Network Plugins,Support hostPort,"Support hostPort The CNI networking plugin supports hostPort. You can use the officialportmapplugin offered by the CNI plugin team or use your own plugin with portMapping functionality. If you want to enable hostPort support, you must specify portMappings capability in yourcni-conf-dir. For example: {  ""name"": ""k8s-pod-network"",  ""cniVersion"": ""0.4.0"",  ""plugins"": [    {      ""type"": ""calico"",      ""log_level"": ""info"",      ""datastore_type"": ""kubernetes"",      ""nodename"": ""127.0.0.1"",      ""ipam"": {        ""type"": ""host-local"",        ""subnet"": ""usePodCidr""      },      ""policy"": {        ""type"": ""k8s""      },      ""kubernetes"": {        ""kubeconfig"": ""/etc/cni/net.d/calico-kubeconfig""      }    },    {      ""type"": ""portmap"",      ""capabilities"": {""portMappings"": true},      ""externalSetMarkChain"": ""KUBE-MARK-MASQ""    }  ]}",278
13.1.1 - Network Plugins,Support traffic shaping,"Support traffic shaping Experimental Feature The CNI networking plugin also supports pod ingress and egress traffic shaping. You can use theofficial bandwidthplugin offered by the CNI plugin team or use your own plugin with bandwidth control functionality. If you want to enable traffic shaping support, you must add the bandwidth plugin to your CNIconfiguration file (default /etc/cni/net.d) and ensure that the binary is included in your CNIbin dir (default /opt/cni/bin). {  ""name"": ""k8s-pod-network"",  ""cniVersion"": ""0.4.0"",  ""plugins"": [    {      ""type"": ""calico"",      ""log_level"": ""info"",      ""datastore_type"": ""kubernetes"",      ""nodename"": ""127.0.0.1"",      ""ipam"": {        ""type"": ""host-local"",        ""subnet"": ""usePodCidr""      },      ""policy"": {        ""type"": ""k8s""      },      ""kubernetes"": {        ""kubeconfig"": ""/etc/cni/net.d/calico-kubeconfig""      }    },    {      ""type"": ""bandwidth"",      ""capabilities"": {""bandwidth"": true}    }  ]} Now you can add the kubernetes.io/ingress-bandwidth and kubernetes.io/egress-bandwidthannotations to your Pod. For example: apiVersion: v1kind: Podmetadata:  annotations:    kubernetes.io/ingress-bandwidth: 1M    kubernetes.io/egress-bandwidth: 1M...",377
13.1.2 - Device Plugins,default,"Device plugins let you configure your cluster with support for devices or resources that require vendor-specific setup, such as GPUs, NICs, FPGAs, or non-volatile main memory. FEATURE STATE: Kubernetes v1.26 [stable] Kubernetes provides a device plugin frameworkthat you can use to advertise system hardware resources to theKubelet. Instead of customizing the code for Kubernetes itself, vendors can implement adevice plugin that you deploy either manually or as a DaemonSet.The targeted devices include GPUs, high-performance NICs, FPGAs, InfiniBand adapters,and other similar computing resources that may require vendor specific initializationand setup.",145
13.1.2 - Device Plugins,Device plugin registration,"Device plugin registration The kubelet exports a Registration gRPC service: service Registration {	rpc Register(RegisterRequest) returns (Empty) {}} A device plugin can register itself with the kubelet through this gRPC service.During the registration, the device plugin needs to send: The name of its Unix socket.The Device Plugin API version against which it was built.The ResourceName it wants to advertise. Here ResourceName needs to follow theextended resource naming schemeas vendor-domain/resourcetype.(For example, an NVIDIA GPU is advertised as nvidia.com/gpu.) Following a successful registration, the device plugin sends the kubelet thelist of devices it manages, and the kubelet is then in charge of advertising thoseresources to the API server as part of the kubelet node status update.For example, after a device plugin registers hardware-vendor.example/foo with the kubeletand reports two healthy devices on a node, the node status is updatedto advertise that the node has 2 ""Foo"" devices installed and available. Then, users can request devices as part of a Pod specification(see container).Requesting extended resources is similar to how you manage requests and limits forother resources, with the following differences: Extended resources are only supported as integer resources and cannot be overcommitted.Devices cannot be shared between containers.",286
13.1.2 - Device Plugins,Example,"Example Suppose a Kubernetes cluster is running a device plugin that advertises resource hardware-vendor.example/fooon certain nodes. Here is an example of a pod requesting this resource to run a demo workload: ---apiVersion: v1kind: Podmetadata:  name: demo-podspec:  containers:    - name: demo-container-1      image: registry.k8s.io/pause:2.0      resources:        limits:          hardware-vendor.example/foo: 2## This Pod needs 2 of the hardware-vendor.example/foo devices# and can only schedule onto a Node that's able to satisfy# that need.## If the Node has more than 2 of those devices available, the# remainder would be available for other Pods to use.",168
13.1.2 - Device Plugins,Device plugin implementation,"Device plugin implementation The general workflow of a device plugin includes the following steps: Initialization. During this phase, the device plugin performs vendor-specificinitialization and setup to make sure the devices are in a ready state.The plugin starts a gRPC service, with a Unix socket under the host path/var/lib/kubelet/device-plugins/, that implements the following interfaces:service DevicePlugin {      // GetDevicePluginOptions returns options to be communicated with Device Manager.      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}      // ListAndWatch returns a stream of List of Devices      // Whenever a Device state change or a Device disappears, ListAndWatch      // returns the new list      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}      // Allocate is called during container creation so that the Device      // Plugin can run device specific operations and instruct Kubelet      // of the steps to make the Device available in the container      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}      // GetPreferredAllocation returns a preferred set of devices to allocate      // from a list of available ones. The resulting preferred allocation is not      // guaranteed to be the allocation ultimately performed by the      // devicemanager. It is only designed to help the devicemanager make a more      // informed allocation decision when possible.      rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {}      // PreStartContainer is called, if indicated by Device Plugin during registeration phase,      // before each container start. Device plugin can run device specific operations      // such as resetting the device before making devices available to the container.      rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}}Note: Plugins are not required to provide useful implementations forGetPreferredAllocation() or PreStartContainer(). Flags indicatingthe availability of these calls, if any, should be set in the DevicePluginOptionsmessage sent back by a call to GetDevicePluginOptions(). The kubelet willalways call GetDevicePluginOptions() to see which optional functions areavailable, before calling any of them directly.The plugin registers itself with the kubelet through the Unix socket at hostpath /var/lib/kubelet/device-plugins/kubelet.sock.Note: The ordering of the workflow is important. A plugin MUST start serving gRPCservice before registering itself with kubelet for successful registration.After successfully registering itself, the device plugin runs in serving mode, during which it keepsmonitoring device health and reports back to the kubelet upon any device state changes.It is also responsible for serving Allocate gRPC requests. During Allocate, the device plugin maydo device-specific preparation; for example, GPU cleanup or QRNG initialization.If the operations succeed, the device plugin returns an AllocateResponse that contains containerruntime configurations for accessing the allocated devices. The kubelet passes this informationto the container runtime.",641
13.1.2 - Device Plugins,Handling kubelet restarts,Handling kubelet restarts A device plugin is expected to detect kubelet restarts and re-register itself with the newkubelet instance. A new kubelet instance deletes all the existing Unix sockets under/var/lib/kubelet/device-plugins when it starts. A device plugin can monitor the deletionof its Unix socket and re-register itself upon such an event.,83
13.1.2 - Device Plugins,Device plugin deployment,"Device plugin deployment You can deploy a device plugin as a DaemonSet, as a package for your node's operating system,or manually. The canonical directory /var/lib/kubelet/device-plugins requires privileged access,so a device plugin must run in a privileged security context.If you're deploying a device plugin as a DaemonSet, /var/lib/kubelet/device-pluginsmust be mounted as a Volumein the plugin's PodSpec. If you choose the DaemonSet approach you can rely on Kubernetes to: place the device plugin'sPod onto Nodes, to restart the daemon Pod after failure, and to help automate upgrades.",139
13.1.2 - Device Plugins,API compatibility,"API compatibility Previously, the versioning scheme required the Device Plugin's API version to matchexactly the Kubelet's version. Since the graduation of this feature to Beta in v1.12this is no longer a hard requirement. The API is versioned and has been stable sinceBeta graduation of this feature. Because of this, kubelet upgrades should be seamlessbut there still may be changes in the API before stabilization making upgrades notguaranteed to be non-breaking. Note: Although the Device Manager component of Kubernetes is a generally available feature,the device plugin API is not stable. For information on the device plugin API andversion compatibility, read Device Plugin API versions. As a project, Kubernetes recommends that device plugin developers: Watch for Device Plugin API changes in the future releases.Support multiple versions of the device plugin API for backward/forward compatibility. To run device plugins on nodes that need to be upgraded to a Kubernetes release witha newer device plugin API version, upgrade your device plugins to support both versionsbefore upgrading these nodes. Taking that approach will ensure the continuous functioningof the device allocations during the upgrade.",235
13.1.2 - Device Plugins,Monitoring device plugin resources,"Monitoring device plugin resources FEATURE STATE: Kubernetes v1.15 [beta] In order to monitor resources provided by device plugins, monitoring agents need to be able todiscover the set of devices that are in-use on the node and obtain metadata to describe whichcontainer the metric should be associated with. Prometheus metricsexposed by device monitoring agents should follow theKubernetes Instrumentation Guidelines,identifying containers using pod, namespace, and container prometheus labels. The kubelet provides a gRPC service to enable discovery of in-use devices, and to provide metadatafor these devices: // PodResourcesLister is a service provided by the kubelet that provides information about the// node resources consumed by pods and containers on the nodeservice PodResourcesLister {    rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}    rpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {}}",205
13.1.2 - Device Plugins,List gRPC endpoint,"List gRPC endpoint The List endpoint provides information on resources of running pods, with details such as theid of exclusively allocated CPUs, device id as it was reported by device plugins and id ofthe NUMA node where these devices are allocated. Also, for NUMA-based machines, it contains theinformation about memory and hugepages reserved for a container. // ListPodResourcesResponse is the response returned by List functionmessage ListPodResourcesResponse {    repeated PodResources pod_resources = 1;}// PodResources contains information about the node resources assigned to a podmessage PodResources {    string name = 1;    string namespace = 2;    repeated ContainerResources containers = 3;}// ContainerResources contains information about the resources assigned to a containermessage ContainerResources {    string name = 1;    repeated ContainerDevices devices = 2;    repeated int64 cpu_ids = 3;    repeated ContainerMemory memory = 4;}// ContainerMemory contains information about memory and hugepages assigned to a containermessage ContainerMemory {    string memory_type = 1;    uint64 size = 2;    TopologyInfo topology = 3;}// Topology describes hardware topology of the resourcemessage TopologyInfo {        repeated NUMANode nodes = 1;}// NUMA representation of NUMA nodemessage NUMANode {        int64 ID = 1;}// ContainerDevices contains information about the devices assigned to a containermessage ContainerDevices {    string resource_name = 1;    repeated string device_ids = 2;    TopologyInfo topology = 3;} Note:cpu_ids in the ContainerResources in the List endpoint correspond to exclusive CPUs allocatedto a particular container. If the goal is to evaluate CPUs that belong to the shared pool, the Listendpoint needs to be used in conjunction with the GetAllocatableResources endpoint as explainedbelow:Call GetAllocatableResources to get a list of all the allocatable CPUsCall GetCpuIds on all ContainerResources in the systemSubtract out all of the CPUs from the GetCpuIds calls from the GetAllocatableResources call",436
13.1.2 - Device Plugins,GetAllocatableResources gRPC endpoint,"GetAllocatableResources gRPC endpoint FEATURE STATE: Kubernetes v1.23 [beta] GetAllocatableResources provides information on resources initially available on the worker node.It provides more information than kubelet exports to APIServer. Note:GetAllocatableResources should only be used to evaluate allocatableresources on a node. If the goal is to evaluate free/unallocated resources it should be used inconjunction with the List() endpoint. The result obtained by GetAllocatableResources would remainthe same unless the underlying resources exposed to kubelet change. This happens rarely but whenit does (for example: hotplug/hotunplug, device health changes), client is expected to callGetAlloctableResources endpoint.However, calling GetAllocatableResources endpoint is not sufficient in case of cpu and/or memoryupdate and Kubelet needs to be restarted to reflect the correct resource capacity and allocatable. // AllocatableResourcesResponses contains informations about all the devices known by the kubeletmessage AllocatableResourcesResponse {    repeated ContainerDevices devices = 1;    repeated int64 cpu_ids = 2;    repeated ContainerMemory memory = 3;} Starting from Kubernetes v1.23, the GetAllocatableResources is enabled by default.You can disable it by turning off the KubeletPodResourcesGetAllocatablefeature gate. Preceding Kubernetes v1.23, to enable this feature kubelet must be started with the following flag: --feature-gates=KubeletPodResourcesGetAllocatable=true ContainerDevices do expose the topology information declaring to which NUMA cells the device isaffine. The NUMA cells are identified using a opaque integer ID, which value is consistent towhat device plugins reportwhen they register themselves to the kubelet. The gRPC service is served over a unix socket at /var/lib/kubelet/pod-resources/kubelet.sock.Monitoring agents for device plugin resources can be deployed as a daemon, or as a DaemonSet.The canonical directory /var/lib/kubelet/pod-resources requires privileged access, so monitoringagents must run in a privileged security context. If a device monitoring agent is running as aDaemonSet, /var/lib/kubelet/pod-resources must be mounted as aVolume in the device monitoring agent'sPodSpec. Support for the PodResourcesLister service requires KubeletPodResourcesfeature gate to be enabled.It is enabled by default starting with Kubernetes 1.15 and is v1 since Kubernetes 1.20.",563
13.1.2 - Device Plugins,Device plugin integration with the Topology Manager,"Device plugin integration with the Topology Manager FEATURE STATE: Kubernetes v1.18 [beta] The Topology Manager is a Kubelet component that allows resources to be co-ordinated in a Topologyaligned manner. In order to do this, the Device Plugin API was extended to include aTopologyInfo struct. message TopologyInfo {    repeated NUMANode nodes = 1;}message NUMANode {    int64 ID = 1;} Device Plugins that wish to leverage the Topology Manager can send back a populated TopologyInfostruct as part of the device registration, along with the device IDs and the health of the device.The device manager will then use this information to consult with the Topology Manager and makeresource assignment decisions. TopologyInfo supports setting a nodes field to either nil or a list of NUMA nodes. Thisallows the Device Plugin to advertise a device that spans multiple NUMA nodes. Setting TopologyInfo to nil or providing an empty list of NUMA nodes for a given deviceindicates that the Device Plugin does not have a NUMA affinity preference for that device. An example TopologyInfo struct populated for a device by a Device Plugin: pluginapi.Device{ID: ""25102017"", Health: pluginapi.Healthy, Topology:&pluginapi.TopologyInfo{Nodes: []*pluginapi.NUMANode{&pluginapi.NUMANode{ID: 0,},}}}",304
13.1.2 - Device Plugins,Device plugin examples,"Device plugin examples Note:This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information. Here are some examples of device plugin implementations: The AMD GPU device pluginThe Intel device plugins forIntel GPU, FPGA, QAT, VPU, SGX, DSA, DLB and IAA devicesThe KubeVirt device plugins forhardware-assisted virtualizationThe NVIDIA GPU device plugin for Container-Optimized OSThe RDMA device pluginThe SocketCAN device pluginThe Solarflare device pluginThe SR-IOV Network device pluginThe Xilinx FPGA device plugins for Xilinx FPGA devices Learn about scheduling GPU resources using devicepluginsLearn about advertising extended resourceson a nodeLearn about the Topology ManagerRead about using hardware acceleration for TLS ingresswith Kubernetes",209
13.2.1 - Custom Resources,default,Custom resources are extensions of the Kubernetes API. This page discusses when to add a customresource to your Kubernetes cluster and when to use a standalone service. It describes the twomethods for adding custom resources and how to choose between them.,54
13.2.1 - Custom Resources,Custom resources,"Custom resources A resource is an endpoint in the Kubernetes API thatstores a collection of API objectsof a certain kind; for example, the built-in pods resource contains a collection of Pod objects. A custom resource is an extension of the Kubernetes API that is not necessarily available in a defaultKubernetes installation. It represents a customization of a particular Kubernetes installation. However,many core Kubernetes functions are now built using custom resources, making Kubernetes more modular. Custom resources can appear and disappear in a running cluster through dynamic registration,and cluster admins can update custom resources independently of the cluster itself.Once a custom resource is installed, users can create and access its objects usingkubectl, just as they do for built-in resourceslike Pods.",167
13.2.1 - Custom Resources,Custom controllers,"Custom controllers On their own, custom resources let you store and retrieve structured data.When you combine a custom resource with a custom controller, custom resourcesprovide a true declarative API. The Kubernetes declarative APIenforces a separation of responsibilities. You declare the desired state ofyour resource. The Kubernetes controller keeps the current state of Kubernetesobjects in sync with your declared desired state. This is in contrast to animperative API, where you instruct a server what to do. You can deploy and update a custom controller on a running cluster, independentlyof the cluster's lifecycle. Custom controllers can work with any kind of resource,but they are especially effective when combined with custom resources. TheOperator pattern combines customresources and custom controllers. You can use custom controllers to encode domain knowledgefor specific applications into an extension of the Kubernetes API.",182
13.2.1 - Custom Resources,Should I add a custom resource to my Kubernetes cluster?,"Should I add a custom resource to my Kubernetes cluster? When creating a new API, consider whether toaggregate your API with the Kubernetes cluster APIsor let your API stand alone. Consider API aggregation if:Prefer a stand-alone API if:Your API is Declarative.Your API does not fit the Declarative model.You want your new types to be readable and writable using kubectl.kubectl support is not requiredYou want to view your new types in a Kubernetes UI, such as dashboard, alongside built-in types.Kubernetes UI support is not required.You are developing a new API.You already have a program that serves your API and works well.You are willing to accept the format restriction that Kubernetes puts on REST resource paths, such as API Groups and Namespaces. (See the API Overview.)You need to have specific REST paths to be compatible with an already defined REST API.Your resources are naturally scoped to a cluster or namespaces of a cluster.Cluster or namespace scoped resources are a poor fit; you need control over the specifics of resource paths.You want to reuse Kubernetes API support features.You don't need those features.",261
13.2.1 - Custom Resources,Declarative APIs,"Declarative APIs In a Declarative API, typically: Your API consists of a relatively small number of relatively small objects (resources).The objects define configuration of applications or infrastructure.The objects are updated relatively infrequently.Humans often need to read and write the objects.The main operations on the objects are CRUD-y (creating, reading, updating and deleting).Transactions across objects are not required: the API represents a desired state, not an exact state. Imperative APIs are not declarative.Signs that your API might not be declarative include: The client says ""do this"", and then gets a synchronous response back when it is done.The client says ""do this"", and then gets an operation ID back, and has to check a separateOperation object to determine completion of the request.You talk about Remote Procedure Calls (RPCs).Directly storing large amounts of data; for example, > a few kB per object, or > 1000s of objects.High bandwidth access (10s of requests per second sustained) needed.Store end-user data (such as images, PII, etc.) or other large-scale data processed by applications.The natural operations on the objects are not CRUD-y.The API is not easily modeled as objects.You chose to represent pending operations with an operation ID or an operation object.",282
13.2.1 - Custom Resources,Should I use a ConfigMap or a custom resource?,"Should I use a ConfigMap or a custom resource? Use a ConfigMap if any of the following apply: There is an existing, well-documented configuration file format, such as a mysql.cnf orpom.xml.You want to put the entire configuration into one key of a ConfigMap.The main use of the configuration file is for a program running in a Pod on your cluster toconsume the file to configure itself.Consumers of the file prefer to consume via file in a Pod or environment variable in a pod,rather than the Kubernetes API.You want to perform rolling updates via Deployment, etc., when the file is updated. Note: Use a Secret for sensitive data, which is similarto a ConfigMap but more secure. Use a custom resource (CRD or Aggregated API) if most of the following apply: You want to use Kubernetes client libraries and CLIs to create and update the new resource.You want top-level support from kubectl; for example, kubectl get my-object object-name.You want to build new automation that watches for updates on the new object, and then CRUD otherobjects, or vice versa.You want to write automation that handles updates to the object.You want to use Kubernetes API conventions like .spec, .status, and .metadata.You want the object to be an abstraction over a collection of controlled resources, or asummarization of other resources.",312
13.2.1 - Custom Resources,Adding custom resources,"Adding custom resources Kubernetes provides two ways to add custom resources to your cluster: CRDs are simple and can be created without any programming.API Aggregationrequires programming, but allows more control over API behaviors like how data is stored andconversion between API versions. Kubernetes provides these two options to meet the needs of different users, so that neither easeof use nor flexibility is compromised. Aggregated APIs are subordinate API servers that sit behind the primary API server, which acts asa proxy. This arrangement is called API Aggregation(AA).To users, the Kubernetes API appears extended. CRDs allow users to create new types of resources without adding another API server. You do notneed to understand API Aggregation to use CRDs. Regardless of how they are installed, the new resources are referred to as Custom Resources todistinguish them from built-in Kubernetes resources (like pods). Note:Avoid using a Custom Resource as data storage for application, end user, or monitoring data:architecture designs that store application data within the Kubernetes API typically representa design that is too closely coupled.Architecturally, cloud native application architecturesfavor loose coupling between components. If part of your workload requires a backing service forits routine operation, run that backing service as a component or consume it as an external service.This way, your workload does not rely on the Kubernetes API for its normal operation.",299
13.2.1 - Custom Resources,CustomResourceDefinitions,"CustomResourceDefinitions The CustomResourceDefinitionAPI resource allows you to define custom resources.Defining a CRD object creates a new custom resource with a name and schema that you specify.The Kubernetes API serves and handles the storage of your custom resource.The name of a CRD object must be a validDNS subdomain name. This frees you from writing your own API server to handle the custom resource,but the generic nature of the implementation means you have less flexibility than withAPI server aggregation. Refer to the custom controller examplefor an example of how to register a new custom resource, work with instances of your new resource type,and use a controller to handle events.",139
13.2.1 - Custom Resources,API server aggregation,"API server aggregation Usually, each resource in the Kubernetes API requires code that handles REST requests and managespersistent storage of objects. The main Kubernetes API server handles built-in resources likepods and services, and can also generically handle custom resources throughCRDs. The aggregation layerallows you to provide specialized implementations for your custom resources by writing anddeploying your own API server.The main API server delegates requests to your API server for the custom resources that you handle,making them available to all of its clients.",110
13.2.1 - Custom Resources,Choosing a method for adding custom resources,"Choosing a method for adding custom resources CRDs are easier to use. Aggregated APIs are more flexible. Choose the method that best meets your needs. Typically, CRDs are a good fit if: You have a handful of fieldsYou are using the resource within your company, or as part of a small open-source project (asopposed to a commercial product)",76
13.2.1 - Custom Resources,Comparing ease of use,"Comparing ease of use CRDs are easier to create than Aggregated APIs. CRDsAggregated APIDo not require programming. Users can choose any language for a CRD controller.Requires programming and building binary and image.No additional service to run; CRDs are handled by API server.An additional service to create and that could fail.No ongoing support once the CRD is created. Any bug fixes are picked up as part of normal Kubernetes Master upgrades.May need to periodically pickup bug fixes from upstream and rebuild and update the Aggregated API server.No need to handle multiple versions of your API; for example, when you control the client for this resource, you can upgrade it in sync with the API.You need to handle multiple versions of your API; for example, when developing an extension to share with the world.",174
13.2.1 - Custom Resources,Advanced features and flexibility,"Advanced features and flexibility Aggregated APIs offer more advanced API features and customization of other features; for example, the storage layer. FeatureDescriptionCRDsAggregated APIValidationHelp users prevent errors and allow you to evolve your API independently of your clients. These features are most useful when there are many clients who can't all update at the same time.Yes. Most validation can be specified in the CRD using OpenAPI v3.0 validation. Any other validations supported by addition of a Validating Webhook.Yes, arbitrary validation checksDefaultingSee aboveYes, either via OpenAPI v3.0 validation default keyword (GA in 1.17), or via a Mutating Webhook (though this will not be run when reading from etcd for old objects).YesMulti-versioningAllows serving the same object through two API versions. Can help ease API changes like renaming fields. Less important if you control your client versions.YesYesCustom StorageIf you need storage with a different performance mode (for example, a time-series database instead of key-value store) or isolation for security (for example, encryption of sensitive information, etc.)NoYesCustom Business LogicPerform arbitrary checks or actions when creating, reading, updating or deleting an objectYes, using Webhooks.YesScale SubresourceAllows systems like HorizontalPodAutoscaler and PodDisruptionBudget interact with your new resourceYesYesStatus SubresourceAllows fine-grained access control where user writes the spec section and the controller writes the status section. Allows incrementing object Generation on custom resource data mutation (requires separate spec and status sections in the resource)YesYesOther SubresourcesAdd operations other than CRUD, such as ""logs"" or ""exec"".NoYesstrategic-merge-patchThe new endpoints support PATCH with Content-Type: application/strategic-merge-patch+json. Useful for updating objects that may be modified both locally, and by the server. For more information, see ""Update API Objects in Place Using kubectl patch""NoYesProtocol BuffersThe new resource supports clients that want to use Protocol BuffersNoYesOpenAPI SchemaIs there an OpenAPI (swagger) schema for the types that can be dynamically fetched from the server? Is the user protected from misspelling field names by ensuring only allowed fields are set? Are types enforced (in other words, don't put an int in a string field?)Yes, based on the OpenAPI v3.0 validation schema (GA in 1.16).Yes",529
13.2.1 - Custom Resources,Common Features,"Common Features When you create a custom resource, either via a CRD or an AA, you get many features for your API,compared to implementing it outside the Kubernetes platform: FeatureWhat it doesCRUDThe new endpoints support CRUD basic operations via HTTP and kubectlWatchThe new endpoints support Kubernetes Watch operations via HTTPDiscoveryClients like kubectl and dashboard automatically offer list, display, and field edit operations on your resourcesjson-patchThe new endpoints support PATCH with Content-Type: application/json-patch+jsonmerge-patchThe new endpoints support PATCH with Content-Type: application/merge-patch+jsonHTTPSThe new endpoints uses HTTPSBuilt-in AuthenticationAccess to the extension uses the core API server (aggregation layer) for authenticationBuilt-in AuthorizationAccess to the extension can reuse the authorization used by the core API server; for example, RBAC.FinalizersBlock deletion of extension resources until external cleanup happens.Admission WebhooksSet default values and validate extension resources during any create/update/delete operation.UI/CLI DisplayKubectl, dashboard can display extension resources.Unset versus EmptyClients can distinguish unset fields from zero-valued fields.Client Libraries GenerationKubernetes provides generic client libraries, as well as tools to generate type-specific client libraries.Labels and annotationsCommon metadata across objects that tools know how to edit for core and custom resources.",309
13.2.1 - Custom Resources,Third party code and new points of failure,"Third party code and new points of failure While creating a CRD does not automatically add any new points of failure (for example, by causingthird party code to run on your API server), packages (for example, Charts) or other installationbundles often include CRDs as well as a Deployment of third-party code that implements thebusiness logic for a new custom resource. Installing an Aggregated API server always involves running a new Deployment.",94
13.2.1 - Custom Resources,Storage,"Storage Custom resources consume storage space in the same way that ConfigMaps do. Creating too manycustom resources may overload your API server's storage space. Aggregated API servers may use the same storage as the main API server, in which case the samewarning applies.",54
13.2.1 - Custom Resources,"Authentication, authorization, and auditing","Authentication, authorization, and auditing CRDs always use the same authentication, authorization, and audit logging as the built-inresources of your API server. If you use RBAC for authorization, most RBAC roles will not grant access to the new resources(except the cluster-admin role or any role created with wildcard rules). You'll need to explicitlygrant access to the new resources. CRDs and Aggregated APIs often come bundled with new roledefinitions for the types they add. Aggregated API servers may or may not use the same authentication, authorization, and auditing asthe primary API server.",128
13.2.1 - Custom Resources,Accessing a custom resource,"Accessing a custom resource Kubernetes client libraries can be used to accesscustom resources. Not all client libraries support custom resources. The Go and Python clientlibraries do. When you add a custom resource, you can access it using: kubectlThe Kubernetes dynamic client.A REST client that you write.A client generated using Kubernetes client generation tools(generating one is an advanced undertaking, but some projects may provide a client along withthe CRD or AA). Learn how to Extend the Kubernetes API with the aggregation layer.Learn how to Extend the Kubernetes API with CustomResourceDefinition.",133
13.2.2 - Kubernetes API Aggregation Layer,default,"The aggregation layer allows Kubernetes to be extended with additional APIs, beyond what isoffered by the core Kubernetes APIs.The additional APIs can either be ready-made solutions such as ametrics server, or APIs that you develop yourself. The aggregation layer is different fromCustom Resources,which are a way to make the kube-apiserverrecognise new kinds of object.",82
13.2.2 - Kubernetes API Aggregation Layer,Aggregation layer,"Aggregation layer The aggregation layer runs in-process with the kube-apiserver. Until an extension resource isregistered, the aggregation layer will do nothing. To register an API, you add an APIServiceobject, which ""claims"" the URL path in the Kubernetes API. At that point, the aggregation layerwill proxy anything sent to that API path (e.g. /apis/myextension.mycompany.io/v1/…) to theregistered APIService. The most common way to implement the APIService is to run an extension API server in Pod(s) thatrun in your cluster. If you're using the extension API server to manage resources in your cluster,the extension API server (also written as ""extension-apiserver"") is typically paired with one ormore controllers. The apiserver-builderlibrary provides a skeleton for both extension API servers and the associated controller(s).",200
13.2.2 - Kubernetes API Aggregation Layer,Response latency,"Response latency Extension API servers should have low latency networking to and from the kube-apiserver.Discovery requests are required to round-trip from the kube-apiserver in five seconds or less. If your extension API server cannot achieve that latency requirement, consider making changes thatlet you meet it. To get the aggregator working in your environment, configure the aggregation layer.Then, setup an extension api-server to work with the aggregation layer.Read about APIService in the API reference Alternatively: learn how toextend the Kubernetes API using Custom Resource Definitions.",122
13.3 - Operator pattern,Motivation,"Motivation The operator pattern aims to capture the key aim of a human operator whois managing a service or set of services. Human operators who look afterspecific applications and services have deep knowledge of how the systemought to behave, how to deploy it, and how to react if there are problems. People who run workloads on Kubernetes often like to use automation to takecare of repeatable tasks. The operator pattern captures how you can writecode to automate a task beyond what Kubernetes itself provides.",104
13.3 - Operator pattern,Operators in Kubernetes,"Operators in Kubernetes Kubernetes is designed for automation. Out of the box, you get lots ofbuilt-in automation from the core of Kubernetes. You can use Kubernetesto automate deploying and running workloads, and you can automate howKubernetes does that. Kubernetes' operator patternconcept lets you extend the cluster's behaviour without modifying the code of Kubernetesitself by linking controllers toone or more custom resources. Operators are clients of the Kubernetes API that act ascontrollers for a Custom Resource.",122
13.3 - Operator pattern,An example operator,"An example operator Some of the things that you can use an operator to automate include: deploying an application on demandtaking and restoring backups of that application's statehandling upgrades of the application code alongside related changes suchas database schemas or extra configuration settingspublishing a Service to applications that don't support Kubernetes APIs todiscover themsimulating failure in all or part of your cluster to test its resiliencechoosing a leader for a distributed application without an internalmember election process What might an operator look like in more detail? Here's an example: A custom resource named SampleDB, that you can configure into the cluster.A Deployment that makes sure a Pod is running that contains thecontroller part of the operator.A container image of the operator code.Controller code that queries the control plane to find out what SampleDBresources are configured.The core of the operator is code to tell the API server how to makereality match the configured resources.If you add a new SampleDB, the operator sets up PersistentVolumeClaimsto provide durable database storage, a StatefulSet to run SampleDB anda Job to handle initial configuration.If you delete it, the operator takes a snapshot, then makes sure thatthe StatefulSet and Volumes are also removed.The operator also manages regular database backups. For each SampleDBresource, the operator determines when to create a Pod that can connectto the database and take backups. These Pods would rely on a ConfigMapand / or a Secret that has database connection details and credentials.Because the operator aims to provide robust automation for the resourceit manages, there would be additional supporting code. For this example,code checks to see if the database is running an old version and, if so,creates Job objects that upgrade it for you.",363
13.3 - Operator pattern,Deploying operators,"Deploying operators The most common way to deploy an operator is to add theCustom Resource Definition and its associated Controller to your cluster.The Controller will normally run outside of thecontrol plane,much as you would run any containerized application.For example, you can run the controller in your cluster as a Deployment.",63
13.3 - Operator pattern,Using an operator,"Using an operator Once you have an operator deployed, you'd use it by adding, modifying ordeleting the kind of resource that the operator uses. Following the aboveexample, you would set up a Deployment for the operator itself, and then: kubectl get SampleDB                   # find configured databaseskubectl edit SampleDB/example-database # manually change some settings …and that's it! The operator will take care of applying the changesas well as keeping the existing service in good shape.",104
13.3 - Operator pattern,Writing your own operator,"Writing your own operator If there isn't an operator in the ecosystem that implements the behavior youwant, you can code your own. You also implement an operator (that is, a Controller) using any language / runtimethat can act as a client for the Kubernetes API. Following are a few libraries and tools you can use to write your own cloud nativeoperator. Note:This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information. Charmed Operator FrameworkJava Operator SDKKopf (Kubernetes Operator Pythonic Framework)kube-rs (Rust)kubebuilderKubeOps (.NET operator SDK)KUDO (Kubernetes Universal Declarative Operator)MastMetacontroller along with WebHooks thatyou implement yourselfOperator Frameworkshell-operator Read the CNCFOperator White Paper.Learn more about Custom ResourcesFind ready-made operators on OperatorHub.io to suit your use casePublish your operator for other people to useRead CoreOS' original articlethat introduced the operator pattern (this is an archived version of the original article).Read an articlefrom Google Cloud about best practices for building operators",285
